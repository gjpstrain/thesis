\documentclass[../main.tex]{subfiles}
\begin{document}


\section{Introduction}\label{introduction}

In this chapter we describe our research methodologies. Chapters 4, 5,
and 6 share most aspects of experimental method, while the experiment
described in chapter 7 differs substantially. Throughout this chapter,
the reader should assume that we are referring to the entire body of
experimental work this thesis describes. Methods that differ regarding
the final experiment in chapter 7 are detailed along the way. In this
chapter, we discuss our experimental designs, the tools we use to build
and run our experiments, our approach to statistical analyses, and the
computational methods and practices we employed particularly with
regards to reproducibility and open science.

\section{Experimental Methods}\label{experimental-methods}

It is important to acknowledge that the way in which we conduct
experiments influences what we find and the conclusions that we may draw
from those findings. The decisions that lead us to designing experiments
in certain ways must be based not only on theory, but also on the
practical constraints imposed by external factors on the research team.
Concerns such as time, convenience, and cost must be addressed, and a
compromise between research that is \emph{valuable} and research that is
\emph{doable} must be reached. We focused on pragmatism and impact
throughout the course of this research project; happily, the research
journey we embarked on resulted in methodologies that satisfied both
principles. It is for this reason that we consider the framework we
present to be a key contribution of this thesis.

\subsection{Experimental Design}\label{experimental-design}

All but our final experiment utilised within-participants designs. Each
participant saw all experimental stimuli and provided a judgement of
correlation using a sliding scale between 0 and 1 (see
Figure~\ref{fig-slider}). Experiments 1 to 3 featured featured a single
experimental factor of design, all with 4 levels corresponding to
scatterplots with different design features. Experiment 4 employed a
factorial 2 \(\times\) 2 design. Experiment 5 is a departure from the
shared experimental paradigm of the previous experiments, and features a
1 factor, 2 level between-participants design.

\begin{figure}

\centering{

\includegraphics{../supplied_graphics/example_slider.png}

}

\caption{\label{fig-slider}An example of the slider participants used to
estimate correlation in experiments 1-4.}

\end{figure}%

\subsection{Tools for Testing}\label{tools-for-testing}

Whatever the design of our experiments, software plays a crucial role in
allowing us to carry them out. Fortunately, at the time of writing,
there is a wealth of tools available to facilitate the testing of
visualisations both in traditional lab-based tests and in online
experiments. As we adhere to the principles of open and reproducible
research \cite{ayris_2018}, we discount closed-source software, such as
Gorilla \cite{anwyl_2020} or E-prime \cite{eprime_2020}, as these rely
on paid licenses and do not allow us to share code with future
researchers. We settled on using PsychoPy \cite{peirce_2019} due to its
open-source status, flexibility regarding graphical and code-based
experimental design, and high level of timings accuracy
\cite{bridges_2020}. Using such a open-source tool not only facilitated
our own learning with regard to experiment building, but also enables to
contribute further examples of visualisation studies by hosting the
resulting experiments online for use and modification by future
researchers.

We elected to pursue online testing throughout this thesis. Doing so is
much quicker than carrying out in-person lab-based testing, meaning we
can collect data from a much larger number of participants. This reduces
the chances of detecting false positives during analysis and ensures
adequate levels of power despite the potential for small effects sizes.
Online testing also affords us access to diverse groups of participants
across our populations of interest, especially when compared to the
relatively homogeneous student populations usually accessed by doctoral
researchers. Research has identified online experimentation as producing
reliable results that closely match those found in traditional lab-based
experiments \cite{arechar_2018, hirao_2021, prisse_2022}, especially
with large sample sizes. Due to its integration with PsychoPy, we chose
to use Pavlovia (pavlovia.org) to host all the experiments described in
this thesis. Section \ref{experimental-resources} contains links to all
experiments publicly hosted on Pavlovia's GitLab instance.

\subsection{Recruitment \& Participants}\label{recruitment-participants}

Recruitment of participants online is possible through a range of
service providers, each with advantages and disadvantages. Research
evaluating a number of these providers recently found that Prolific
\cite{prolific_2024} and CloudResearch provide the highest quality data
for the lowest cost \cite{douglas_2023}; we elected to use the former
due to familiarity with the system. Despite these findings, there has
also been evidence of low data quality and skewed demographics affecting
even high quality platforms tailored towards academic research. On the
24th of July, 2021, the Prolific.co platform went viral on social media
\cite{chara_2021}, leading to a participant pool heavily skewed towards
young people identifying as female. At the time, Prolific did not
manually balance the participants recruited for a study. We addressed
this in our pilot study (see Section \ref{pilot}) by preventing
participants who joined after this data from participating, in addition
to manually requesting a 1:1 ratio of male to female participants. The
demographic issues settled quickly, however we maintained our screened
1:1 ratio for the remainder of the experiments.

The first experiment we conducted was a pilot study (see Section
\ref{pilot} for full details) investigating a very early iteration of
the point opacity manipulation in combination with exploratory work
around plot size and correlation estimation. At the time, the author was
relatively naive to the intricacies of recruiting research participants
online, and thus experienced issues with regards to participant
engagement. Each experiment, including the pilot, included attention
check questions in which participants were instructed to ignore the
stimulus and provide a specific answer. We stated in the advert for each
experiment that failure of more than 2 attention check items would
result in a submission being rejected. This pilot study suffered from a
rejection rate of 57.5\%, indicating that we were experiencing low
levels of participant engagement. For our following studies, we
therefore followed published guidelines \cite{peer_2021} to address
these issues; specifically, we required that participants:

\begin{itemize}
\tightlist
\item
  Had previously completed at least 100 studies on Prolific.
\item
  Had an acceptance rate of at least 99\% for those studies. \footnote{this
    is a more strict rate than the 95\% recommended by Peer et
    al.~\cite{peer_2021}.}
\end{itemize}

Following implementation of these pre-screen criteria, the rejection
rate for our next experiment fell to \textasciitilde5\%. Rejection rates
were similar for the remainder of our experiments. Exact numbers of
accepted and rejected participants can be found in the
\textbf{Participants} sections of each experiment.

\subsection{Creating Stimuli}\label{creating-stimuli}

All our stimuli were created using \texttt{ggplot2} in R. Specific
versions are cited separately with regard to the specific visualisations
produced for each experiment. We followed identical principles regarding
data visualisation design for each experiment bar the last, which is
discussed \textit{in situ}.

We designed with the intention of isolating and addressing a perceptual
effect; the underestimation of correlation in positively correlated
scatterplots. For this reason, we sought to remove the potential for
other design factors to have effects on correlation estimation. To this
end, we removed most of the conventionally present visual features of
scatterplots, including axis labels, tick labels, grid lines, and
titles. We elected to preserve the axis ticks themselves.
Figure~\ref{fig-scatter-example} demonstrates the basic design of the
scatterplots used in experiments 1 to 4.

\begin{figure}

\centering{

\includegraphics{3_general_methodology_files/figure-latex/fig-scatter-example-1.pdf}

}

\caption{\label{fig-scatter-example}The basic design of scatterplots in
experiments 1 to 4.}

\end{figure}%

\section{Analytical Methods}\label{analytical-methods}

\subsection{Linear Mixed-Effects
Models}\label{linear-mixed-effects-models}

To investigate whether the experimental manipulations we test have
actual effects on the interpretations participants provide, we must
employ appropriate statistical testing. This involves taking into
account the variability in responses that can be attributed to an
experimental experimental against the backdrop of other variability
inherent in the dataset. To accomplish this, we utilise linear
mixed-effects modelling, a broadly applicable and reliable approach that
is also resistant to a variety of distributional assumption violations
\cite{schielzeth_2020}.

In a mixed-effects modelling paradigm, a distinction is made between
variability that is thought to arise as a result of an experimental
manipulation (fixed effects), and that which arises due to differences
between, for example, participants or particular experimental items
(random effects). When a variable is manipulated by a researcher in an
experiment, each level of that variable is present, meaning it is
appropriate to be modelled as a fixed effect. When only a \emph{subset}
of levels of a variable is present, such as a sample of all possible
participants or experimental items, then this variable is appropriate
for modelling as a random effect. Typically, mixed-effects models
require the specification of \emph{intercepts}; these are different
baselines for each participant or item that reflect random deviations
from the mean of the dependent variable. Mixed-effects models may also
specify random \emph{slopes}; these are differences in the magnitude of
the difference between levels of the independent variable for each
participant or experimental item \cite{brown_2021}.
Figure~\ref{fig-mixed-effects-demo} visualises these concepts.

\begin{figure}

\centering{

\includegraphics{3_general_methodology_files/figure-latex/fig-mixed-effects-demo-1.pdf}

}

\caption{\label{fig-mixed-effects-demo}Visualising random intercepts and
slopes for a theoretical experiment with 4 participants. The grand mean
of the dependent variable is shown as a solid line, while each separate
random intercept is drawn with dashed lines. Each line has a different
gradient, representing different random slopes for each participant.
This graphic was inspired by those featured in Brown, 2021
\cite{brown_2021}.}

\end{figure}%

Throughout the course of this thesis we attempt to model both random
intercepts and slopes in order to capture the maximum amount of
variability present in our datasets.

\subsection{Advantages Over Aggregate-Level Statistical
Tests}\label{advantages-over-aggregate-level-statistical-tests}

Traditional analysis of the data that we collect throughout this thesis
would involve the use of repeated measures analyses of variance
(ANOVAs). This technique assesses whether there are significant
differences in means of dependent variables between conditions. While
these techniques are commonplace, they do not allow for comparisons of
differences across the full range of participant responses, nor do they
allow for simultaneous consideration of by-item and by-participant
variance. It is for these reasons that we employ linear mixed-effects
models throughout this thesis; doing so simply allows us to appreciate
the data story in a broader and more detailed fashion.

\subsection{Ordinal Modelling}\label{ordinal-modelling}

In experiment 5, participants used Likert scales to provide responses.
These scales capture whether one rating is higher or lower than another,
however they do not quantify the magnitude of the difference between
levels of rating. Metric modelling, such as linear regression, treats
the response options to a Likert scale as if they were numeric. Doing so
assumes equal levels of difference between ratings, when in reality
there is no theoretical reason to assume so. Metric modelling is
therefore considered inappropriate for modelling responses to Likert
scales \cite{liddell_2018}. In light of these issues, we use
\texttt{ordinal} package \cite{ordinal} in R to build cumulative link
mixed effects models for the analysis of Likert scale data. This allows
us to treat our Likert responses as an ordered factor as opposed to a
continuous response scale.

\subsection{Model Construction}\label{model-construction}

Choices are inherent in every type of statistical analysis, and can play
a large role in the conclusions that are drawn from them. In linear
mixed-effects modelling, deciding what is a fixed and what is a random
effect is straightforward; deciding how to specify random effects is a
more complicated matter. Barr et al.~\cite{barr_2013} argue that for
fully repeated measures designs, we should prefer a maximal model; one
with random intercepts and slopes for each participant and experimental
item. More recently, Bates et al.~\cite{bates_2018} have argued that
attempting to specify maximal models for insufficiently rich datasets
may lead to overfitting and unreliable conclusions. In light of this we
sought a more systematic approach to selecting the random effects
structure of a given model.

In an attempt to balance simplicity, explanatory power, and model
convergence (whether or not a solution can be found), we chose to use
the \texttt{buildmer} package \cite{buildmer} in R to automate the
selection of our model specifications. Having been provided with a
maximal model, \texttt{buildmer} uses stepwise regression to select the
most complex model structure that successfully converges. Following
this, random effects terms that fail to explain a significant amount of
variance in the dataset are dropped. This results in a model that
captures the maximal amount of feasible variability while minimising
redundancy. Note that we do not rely on \texttt{buildmer} as a modelling
\emph{panacea}; models are still based on theoretical underpinnings and
are evaluated critically.

\subsection{Effects Sizes}\label{effects-sizes}

Our approach to effects sizes evolved throughout the course of the
research project due to reviewer feedback and an increasing appreciation
of complexities of effect sizes when discussing linear mixed-effects
models. Experiments 1, 2, and 3 used the \texttt{EMAtools} package
\cite{ematools} to calculate equivalent Cohen's \emph{d} effect sizes.
Experiment 4 did not feature a theoretically sound baseline condition,
meaning Cohen's \emph{d} was inappropriate. We therefore used the
\texttt{r2glmm} package \cite{r2glmm} to calculate semi-partial
R\textsuperscript{2}. We use this in lieu of a traditional measure of
effect size to demonstrate the unique variance in our dependent variable
explained by each level of our independent \cite{nakagawa_2013}.
Experiment 5 features a much simpler modelling situation, and returns to
providing equivalent Cohen's \emph{d} values, this time calculated by
converting odds ratios using the \texttt{effectsize} package
\cite{effectsize}. More details on specific calculations, measures, and
conclusions can be found \emph{in situ}.

\subsection{Reporting Analyses}\label{reporting-analyses}

Throughout this thesis, we take a broad approach to the reporting of
statistical analyses; while we consider our analytical methods and
conclusions to be sound, we also present a range of statistics to allow
the reader to draw their own conclusions should they wish. Statistics
are visualised where appropriate, and where visualisation aids
understanding and interpretation. In addition, we include details about
model structures and the issues we tackled when modelling, for
transparency \cite{meteyard_2020}.

\section{Computational Methods}\label{computational-methods}

We took an approach to computational methods that sought to marry
convenience, simplicity, and reproducibility. Often, this meant that
what would otherwise be a makeshift script followed by copy-pasting of
results into overleaf ended up being an involved exercise in functional
programming and code wrangling. This involved effort and time,
particularly in the early stages of the project, however has yielded a
number of benefits. Many of the techniques developed early in the
project proved to be instrumental later on, meaning time was, on the
whole saved. Additionally, we share these techniques, principles, and
practices to enable future researchers to learn, where we had to
struggle. In this section, we detail our approaches to computational
methods, including how we utilised the idea of \textbf{executable
papers}, and how we used containerised environments to capture a
\emph{freeze-frame} of our analyses.

\subsection{Executable Reporting}\label{executable-reporting}

Each paper published throughout this project, each chapter in this
thesis, and the thesis itself, has been written to be executable. Doing
so allows us to package our research such that a lay person can follow
simple instructions to recreate our work, while also facilitating
literate programming, or the close alignment of documentation and
underlying code \cite{piccolo_2016}.

The use of a literate programming paradigm to generate reports (usually
using LaTeX) has a rich history. Here, we focus on that history
specifically with regards to the language used throughout the course of
this project, R. \texttt{Sweave} \cite{leisch_2002}, written in 2002,
allowed R code to be integrated into LaTeX documents. This was followed
by Yihui Xie's \texttt{knitr} \cite{xie_2015}, which expanded
\texttt{Sweave} functionality and improved integration with tools such
as \texttt{pandoc} \cite{pandoc}. \texttt{knitr} uses \texttt{Rmarkdown}
\cite{xie_2020} to mix markdown-flavoured text with code chunks into a
document that can easily be rendered into an appropriately-formatted
conference or journal pdf; this workflow was used for the paper
associated with experiments 1 and 2. \texttt{Quarto}
\cite{allaire_2024}, released in 2022, further expands on
\texttt{Rmarkdown} functionality, and removes reliance on R or Rstudio.
We used \texttt{Quarto} for the remainder of the papers associated with
this project, and for the present thesis.

Writing executable or dynamic documents allows results to be
re-generated whenever the document is rendered. This includes any
associated data visualisations and statistical modelling. Structuring
documents like this effectively ``open up'' research by allowing others
to view the code that performed the analysis and generated the data
visualisations, in addition to guarding against accusations of
questionable research practices through high levels of transparency
\cite{holmes_2021}. This paradigm also allows for the caching of
computationally expensive statistical models.

\subsection{Containerised
Environments}\label{containerised-environments}

Providing the code associated with a project, even when that code is
integrated into a literately programmed executable paper, is necessary,
but not sufficient, for enabling adequate reproducibility. Previous work
has found many instances where publicly-accessible code could not
reproduce the results included in the corresponding document or failed
to run \cite{collberg_2016, trisovic_2022, samuel_2024}. Poor
programming practices accounted for a significant portion of these
issues, highlighting the issue of researchers without technical
backgrounds being expected to produce high quality technical
documentation. Elsewhere, differences in computational environment,
package versions, and operating systems have been identified as
responsible for the non-replication of results. Large research projects,
such as this, can include hundreds of functions from scores of packages,
meaning that small changes can critically break code.

To address these issues, we elected to use containers, specifically,
those created by \texttt{Docker} \cite{merkel_2014, boettiger_2015}.
1979 saw the development of \texttt{chroot}, which is able to isolate an
application's file access. The following 50 years has seen rapid
development and uptake of containerisation software, mostly within the
software security and development communities. Docker, released in 2014,
is a popular, lightweight containerisation tool that enables a precise
recreation of computational environments. By recording software versions
and dependencies, we avoid the potential for broken code in the future,
and by publicly hosting papers as GitHub repositories that build into
Docker containers, we ensure that future researchers can interact with
our code and data in the same computational environment we did when
carrying out the research.

\section{Reproducibility In This
Thesis}\label{reproducibility-in-this-thesis}

gold standard of reproducibility \cite{peng_2011}

\subsection{Sharing Data and Code}\label{sharing-data-and-code}

\subsection{Executable Papers and Docker
Containers}\label{executable-papers-and-docker-containers}

\subsection{Experimental Resources}\label{experimental-resources}

Everything needed to run each experiment is included in the
corresponding GitLab repository. Links to these repositories are also
provided in the sections concerning each experiment.

\textbf{Chapter 4}

Experiment 1:
https://gitlab.pavlovia.org/Strain/exp\_uniform\_adjustments

Experiment 2:
https://gitlab.pavlovia.org/Strain/exp\_spatially\_dependent

\textbf{Chapter 5}

Experiment 3: https://gitlab.pavlovia.org/Strain/exp\_size\_only

\textbf{Chapter 6}

Experiment 4:
https://gitlab.pavlovia.org/Strain/size\_and\_opacity\_additive\_exp

\textbf{Chapter 7}

Experiment 5 Pre-Study:
https://gitlab.pavlovia.org/Strain/beliefs\_scatterplots\_pretest

Experiment 5 Main Study (Group A):
https://gitlab.pavlovia.org/Strain/atypical\_scatterplots\_main\_a

Experiment 5 Main Study (Group B):
https://gitlab.pavlovia.org/Strain/atypical\_scatterplots\_main\_t

\section{Conclusion}\label{conclusion}




\end{document}
