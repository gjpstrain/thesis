---
title: "general_methodology"
output:
  format: 
    latex:
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

# Introduction

In this chapter we describe our research methodologies. Chapters 4, 5, and 6 share
most aspects of experimental method, while the experiment described in chapter 7 differs
substantially. Throughout this chapter, the reader should assume that we are 
referring to the entire body of experimental work this thesis describes. Methods
that differ regarding the final experiment in chapter 7 are detailed along the way.
In this chapter, we discuss our experimental designs, the tools we use to build
and run our experiments, our approach to statistical analyses, and the computational
methods and practices we employed particularly with regards to reproducibility
and open science.

# Experimental Methods

It is important to acknowledge that the way in which we conduct experiments influences
what we find and the conclusions that we may draw from those findings. The decisions
that lead us to designing experiments in certain ways must be based not only
on theory, but also on the practical constraints imposed by external factors on
the research team. Concerns such as time, convenience, and cost must be addressed,
and a compromise between research that is *valuable* and research that is *doable*
must be reached. We focused on pragmatism and impact throughout the course of this
research project; happily, the research journey we embarked on resulted in methodologies
that satisfied both principles. It is for this reason that we consider the framework
we present to be a key contribution of this thesis.

## Experimental Design

All but our final experiment utilised within-participants designs. Each
participant saw all experimental stimuli and provided a judgement 
of correlation using a sliding scale between 0 and 1 (see @fig-slider). Experiments 1 to 3 featured
featured a single experimental factor of design, all with 4 levels corresponding
to scatterplots with different design features. Experiment 4 employed a factorial
2 $\times$ 2 design. Experiment 5 is a departure from the shared experimental
paradigm of the previous experiments, and features a 1 factor, 2 level 
between-participants design.

```{r}
#| label: fig-slider
#| include: true
#| fig-cap: An example of the slider participants used to estimate correlation in experiments 1-4.
#| out-width: NA
#| out-height: NA

# find slider png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example_slider.png", dpi = NA)
```

## Tools for Testing

Whatever the design of our experiments, software plays a crucial role in allowing 
us to carry them out. Fortunately, at the time of writing, there is a wealth of 
tools available to facilitate the testing of visualisations both in traditional
lab-based tests and in online experiments. As we adhere to the principles of 
open and reproducible research \cite{ayris_2018}, we discount closed-source
software, such as Gorilla \cite{anwyl_2020} or E-prime \cite{eprime_2020}, as these
rely on paid licenses and do not allow us to share code with future researchers.
We settled on using PsychoPy \cite{peirce_2019} due to its open-source status,
flexibility regarding graphical and code-based experimental design, and high 
level of timings accuracy \cite{bridges_2020}. Using such a open-source tool not only
facilitated our own learning with regard to experiment building, but also enables
to contribute further examples of visualisation studies by hosting the resulting
experiments online for use and modification by future researchers.

We elected to pursue online testing throughout this thesis. Doing so is 
much quicker than carrying out in-person lab-based testing, meaning we can collect
data from a much larger number of participants. This reduces the chances of detecting
false positives during analysis and ensures adequate levels of power despite the
potential for small effects sizes. Online testing also affords us access to 
diverse groups of participants across our populations of interest, especially
when compared to the relatively homogeneous student populations usually accessed
by doctoral researchers. Research has identified online experimentation as producing
reliable results that closely match those found in traditional lab-based 
experiments \cite{arechar_2018, hirao_2021, prisse_2022}, especially with large
sample sizes. Due to its integration with PsychoPy, we chose to use
Pavlovia (pavlovia.org) to host all the experiments described in this thesis.
Section \ref{experimental-resources} contains links to all experiments publicly hosted
on Pavlovia's GitLab instance.

## Recruitment \& Participants

Recruitment of participants online is possible through a range of service providers,
each with advantages and disadvantages. Research evaluating a number of these
providers recently found that Prolific \cite{prolific_2024} and CloudResearch 
provide the highest quality data for the lowest cost \cite{douglas_2023}; we elected
to use the former due to familiarity with the system. Despite these findings,
there has also been evidence of low data quality and skewed demographics affecting
even high quality platforms tailored towards academic research. On the 24th of July, 2021,
the Prolific.co platform went viral on social media \cite{chara_2021}, leading to a participant
pool heavily skewed towards young people identifying as female. At the time,
Prolific did not manually balance the participants recruited for a study. We addressed
this in our pilot study (see Section \ref{pilot}) by preventing participants
who joined after this data from participating, in addition to manually requesting
a 1:1 ratio of male to female participants. The demographic issues settled quickly,
however we maintained our screened 1:1 ratio for the remainder of the experiments.

The first experiment we conducted was a pilot study (see Section \ref{pilot} for full details)
investigating a very early iteration of the point opacity manipulation

## Creating Stimuli

# Analytical Methods

## Linear Mixed-Effects Models

## Advantages Over Aggregate-Level Statistical Tests

## Model Construction

## Effects Sizes

## Reporting Analyses

# Computational Methods

## Executable Reporting

## Containerised Environments

# Reproducibility In This Thesis

## Sharing Data and Code

## Executable Papers and Docker Containers

## Experimental Resources {#experimental-resources}

Everything needed to run each experiment is included in the corresponding
GitLab repository. Links to these repositories are also provided in the sections
concerning each experiment.

\textbf{Chapter 4}

Experiment 1: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments

Experiment 2: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent

\textbf{Chapter 5}

Experiment 3: https://gitlab.pavlovia.org/Strain/exp_size_only

\textbf{Chapter 6}

Experiment 4: https://gitlab.pavlovia.org/Strain/size_and_opacity_additive_exp

\textbf{Chapter 7}

Experiment 5 Pre-Study: https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest

Experiment 5 Main Study (Group A): https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a

Experiment 5 Main Study (Group B): https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t


# Conclusion
