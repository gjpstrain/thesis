---
title: "general_methodology"
output:
  format: 
    latex:
      
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

```{r}
#| label: setup

# load required packages

library(tidyverse)
library(MASS)
library(ggdist)
library(geomtextpath)
library(ggpubr)
library(bbplot)
library(conflicted)

# load shared functions

source("../shared_functions.R")

# use prepare_slopes function to create df for plotting example plots

slopes_0.2 <- prepare_slopes(0.2)
slopes_0.6 <- prepare_slopes(0.6)
slopes_0.99 <- prepare_slopes(0.99)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
```

# Introduction

In this chapter I describe my research methodologies. The experiments described
in \chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and
\chap{chap:interactions_opacity_size} share most aspects of experimental method,
and are therefore described in full in this chapter. \chap{chap:belief_change}
features a different methodology, and is described therein. This chapter
discusses experimental designs, the tools used to build and run the experiments,
the approach to statistical analyses, and the computational methods and
practices employed, particularly with regards to reproducibility and open
science.

# Experimental Methods

It is important to acknowledge that the way in which we conduct experiments
influences what research questions we can ask and the conclusions that we may
draw. The decisions that lead us to designing experiments in certain ways must
be based not only on theory, but also on the external constraints imposed on
(and by) the research team. Concerns such as time, practicality, and cost must
be addressed, and a compromise between research that is *valuable* and research
that is *doable* must be reached.

## Experimental Design

```{r}
#| label: fig-exp-illustrate
#| include: true
#| fig-cap: An example of an experimental trial in Experiments 1-4, including the scatterplot stimulus (A), the slider that participants were instructed to use to estimate correlation (B), and the trial counter (C).
#| out-width: NA
#| out-height: NA
# find exp example png in supplied graphics folder

# include pre-rendered graphics

knitr::include_graphics(path = "../supplied_graphics/example_exp.png", dpi = NA)
```

All but the final experiment utilised within-participants designs. In such a
design, each participant is exposed to each level of the intervention. This is
in contrast with between-participants designs, where separate groups are exposed
to only a single level of the intervention each. Where possible,
within-participants designs are preferred. These designs do not rely on random
allocation, and as each participant is able to provide as many data points as
there are experimental items in levels \cite{charness_2012}, offer a significant
boost in statistical power over between-participant designs where each
participant may only provide data points for a portion of the total experimental
items. In Experiments 1 to 4, each participant saw all experimental stimuli and
provided a judgement of correlation using a sliding scale between 0 and 1 (see
@fig-exp-illustrate). Experiment 1 featured a single factor of global scatterplot point
opacity with 4 levels (see @fig-exp1-examples). Experiment 2 featured a single
factor of scatterplot point design regarding opacity with 4 levels (see
@fig-exp2-examples). Experiment 3 featured a single factor of scatterplot point
design regarding size with 4 levels (see @fig-exp3-examples). Experiment 4
featured a factorial 2 $\times$ 2 design; IV~1~ was the scatterplot point
opacity design used with 2 levels, and IV~2~ was the scatterplot point size
design used with 2 levels (see @fig-exp4-examples). Experiment 5 is a departure
from the shared experimental paradigm of the previous experiments, and features
a 1 factor, 2 level between-participants design; group A saw scatterplots
designed to elicit greater levels of belief change compared to typical
scatterplots, which were shown to group B (see @fig-exp5-examples).

```{r}
#| label: fig-exp1-examples
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 1, demonstrated with \textit{r} values of 0.2, 0.6, and 0.99. Here, "opacity" refers to the alpha value used by `ggplot2`.
#| fig-asp: 0.8

# create example plots, arrange with ggarrange()

row_1 <- ggarrange(plot_example_function(slopes_0.2, "Opacity = 0.25",
                                         0.25, 0.2, 8),
                   plot_example_function(slopes_0.2, "Opacity = 0.5",
                                         0.5, 0.2, 8),
                   plot_example_function(slopes_0.2, "Opacity = 0.75",
                                         0.75, 0.2, 8),
                   plot_example_function(slopes_0.2, "Opacity = 1",
                                         1, 0.2, 8),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_2 <- ggarrange(plot_example_function(slopes_0.6, NULL,
                                        0.25, 0.2, 6.5),
                  plot_example_function(slopes_0.6, NULL,
                                        0.5, 0.2, 6.5),
                  plot_example_function(slopes_0.6, NULL,
                                        0.75, 0.2, 6.5),
                  plot_example_function(slopes_0.6, NULL,
                                        1, 0.2, 6.5),
                  nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_3 <- ggarrange(plot_example_function(slopes_0.99, NULL,
                                        0.25, 0.2, 6.5),
                  plot_example_function(slopes_0.99, NULL,
                                        0.5, 0.2, 6.5),
                  plot_example_function(slopes_0.99, NULL,
                                        0.75, 0.2, 6.5),
                  plot_example_function(slopes_0.99, NULL,
                                        1, 0.2, 6.5),
                  nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.99", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

ggarrange(row_1, row_2, row_3, ncol = 1, heights = c(1.2, 1, 1))
```

```{r}
#| label: fig-exp2-examples
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 2, demonstrated with \textit{r} values of 0.2, 0.6, and 0.99.
#| fig-asp: 0.8

# example plots for experiment 2, arranged with ggarrange()

row_1 <- ggarrange(plot_example_function(slopes_0.2, "Full Opacity",
                                         1, 0.2, 8),
                   plot_example_function(slopes_0.2, "Linear Decay",
                                         (1-slopes_0.2$slope_linear), 0.2, 8),
                   plot_example_function(slopes_0.2, "Non-linear Decay",
                                         (1-slopes_0.2$slope_0.25), 0.2, 8),
                   plot_example_function(slopes_0.2, "Inverted Non-linear Decay",
                                         (1-slopes_0.2$slope_inverted), 0.2, 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_2 <- ggarrange(plot_example_function(slopes_0.6, NULL,
                                         1, 0.2, 6.5),
                   plot_example_function(slopes_0.6, NULL,
                                         (1-slopes_0.6$slope_linear), 0.2, 6.5),
                   plot_example_function(slopes_0.6, NULL,
                                         (1-slopes_0.6$slope_0.25), 0.2, 6.5),
                   plot_example_function(slopes_0.6, NULL,
                                         (1-slopes_0.6$slope_inverted), 0.2, 6.5),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_3 <- ggarrange(plot_example_function(slopes_0.99, NULL,
                                         1, 0.2, 6.5),
                   plot_example_function(slopes_0.99, NULL,
                                         (1-slopes_0.99$slope_linear), 0.2, 6.5),
                   plot_example_function(slopes_0.99, NULL,
                                         (1-slopes_0.99$slope_0.25), 0.2, 6.5),
                   plot_example_function(slopes_0.99, NULL,
                                         (1-slopes_0.99$slope_inverted), 0.2, 6.5),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.99", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

ggarrange(row_1, row_2, row_3, ncol = 1, heights = c(1.2, 1, 1))
```

```{r}
#| label: fig-exp3-examples
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 3, demonstrated with \textit{r} values of 0.2, 0.6, and 0.99.
#| fig-asp: 0.8

# example plots for experiment 3, arranged with ggarrange()

row_1 <- ggarrange(plot_example_function(slopes_0.2, "Standard Size",
                                         1, 0.2, 8),
                   plot_example_function(slopes_0.2, "Linear Decay",
                                         1, (1-slopes_0.2$slope_linear), 8),
                   plot_example_function(slopes_0.2, "Non-linear Decay",
                                         1, (1-slopes_0.2$slope_0.25), 8),
                   plot_example_function(slopes_0.2, "Inverted Non-linear Decay",
                                         1, (1-slopes_0.2$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_2 <- ggarrange(plot_example_function(slopes_0.6, NULL,
                                         1, 0.2, 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_linear), 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_0.25), 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_3 <- ggarrange(plot_example_function(slopes_0.99, NULL,
                                         1, 0.2, 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_linear), 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_0.25), 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.99", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

ggarrange(row_1, row_2, row_3, ncol = 1, heights = c(1.2, 1, 1))
```

```{r}
#| label: fig-exp4-examples
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 4, demonstrated with \textit{r} values of 0.2, 0.6, and 0.99.
#| fig-asp: 0.8

########################### 0.2 plots ###########################

example_plots_congruent_02 <- ggarrange(plot_example_function(slopes_0.2,
                                                           "Typical Orientation Size\nTypical Orientation Opacity",
                                                           (1-slopes_0.2$slope_0.25),
                                                           (1-slopes_0.2$slope_0.25), 7),
                                     plot_example_function(slopes_0.2,
                                                           "Inverted Orientation Size\nInverted Orientation Opacity",
                                                           (1-slopes_0.2$slope_inverted),
                                                           (1-slopes_0.2$slope_inverted), 7), nrow = 1)
                   
example_plots_incongruent_02 <- ggarrange(plot_example_function(slopes_0.2,
                                                             "Typical Orientation Size\nInverted Orientation Opacity",
                                                             (1-slopes_0.2$slope_inverted),
                                                             (1-slopes_0.2$slope_0.25), 7),
                                       plot_example_function(slopes_0.2,
                                                             "Inverted Orientation Size\nTypical Orientation Opacity",
                                                             (1-slopes_0.2$slope_0.25),
                                                             (1-slopes_0.2$slope_inverted), 7), nrow = 1)

plots_02 <- ggarrange(example_plots_congruent_02,
                      example_plots_incongruent_02,
                      nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

########################### 0.6 plots ###########################

example_plots_congruent_06 <- ggarrange(plot_example_function(slopes_0.6,
                                                           NULL,
                                                           (1-slopes_0.6$slope_0.25),
                                                           (1-slopes_0.6$slope_0.25), 8),
                                     plot_example_function(slopes_0.6,
                                                           NULL,
                                                           (1-slopes_0.6$slope_inverted),
                                                           (1-slopes_0.6$slope_inverted), 8), nrow = 1)
                   
example_plots_incongruent_06 <- ggarrange(plot_example_function(slopes_0.6,
                                                             NULL,
                                                             (1-slopes_0.6$slope_inverted),
                                                             (1-slopes_0.6$slope_0.25), 8),
                                       plot_example_function(slopes_0.6,
                                                             NULL,
                                                             (1-slopes_0.6$slope_0.25),
                                                             (1-slopes_0.6$slope_inverted), 8), nrow = 1)

plots_06 <- ggarrange(example_plots_congruent_06,
                      example_plots_incongruent_06,
                      nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

########################### 0.99 plots ###########################

example_plots_congruent_99 <- ggarrange(plot_example_function(slopes_0.99,
                                                           NULL,
                                                           (1-slopes_0.99$slope_0.25),
                                                           (1-slopes_0.99$slope_0.25), 8),
                                     plot_example_function(slopes_0.99,
                                                           NULL,
                                                           (1-slopes_0.99$slope_inverted),
                                                           (1-slopes_0.99$slope_inverted), 8), nrow = 1) +
  annotate(geom = "text",
           label = "Congruent Plots",
           x = 0.5,
           y = 0.05, size = 3)
                   
example_plots_incongruent_99 <- ggarrange(plot_example_function(slopes_0.99,
                                                             NULL,
                                                             (1-slopes_0.99$slope_inverted),
                                                             (1-slopes_0.99$slope_0.25), 8),
                                       plot_example_function(slopes_0.99,
                                                             NULL,
                                                             (1-slopes_0.99$slope_0.25),
                                                             (1-slopes_0.99$slope_inverted), 8), nrow = 1) +
  annotate(geom = "text",
           label = "Incongruent Plots",
           x = 0.5,
           y = 0.05, size = 3)

plots_99 <- ggarrange(example_plots_congruent_99,
                      example_plots_incongruent_99,
                      nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.99", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

########################### COMBINE ALL ###########################

ggarrange(plots_02, plots_06, plots_99, ncol = 1, heights = c(1.2, 1, 1))
```

```{r}
#| label: fig-exp5-examples
#| include: true
#| fig-cap: Examples of the stimuli for Experiment 5 using an \textit{r} value of 0.6. Group A saw the alternative scatterplot presented on the left, while group B saw the typical design on the right. 
#| fig-asp: 1.5

# create example plots for experiment 5

########################### 0.2 plots ###########################

plots_02 <- ggarrange(example_plot_function_exp5(slopes_exp5_02,
                                                 slopeI_02,
                                                 slopeI_floored_02,
                                                 bbc_style()),
                      example_plot_function_exp5(slopes_exp5_02,
                                                 typical_02,
                                                 standard_alpha_02,
                                                 bbc_style()),
                      nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

########################### 0.6 plots ###########################

plots_06 <- ggarrange(example_plot_function_exp5(slopes_exp5_02,
                                                 slopeI_06,
                                                 slopeI_floored_06,
                                                 bbc_style()),
                      example_plot_function_exp5(slopes_exp5_06,
                                                 typical_06,
                                                 standard_alpha_06,
                                                 bbc_style()),
                      nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

########################### 0.99 plots ###########################

plots_99 <- annotate_figure(
  ggarrange(
    example_plot_function_exp5(slopes_exp5_99,
                               slopeI_99,
                               slopeI_floored_99,
                               bbc_style()),
    example_plot_function_exp5(slopes_exp5_99,
                               typical_99,
                               standard_alpha_99,
                               bbc_style()),
    nrow = 1
  ) +
    annotate("text",
             label = "Typical Scatterplot",
             x = 0.75,
             y = 0.1,
             size = 3) +
    annotate("text",
             label = "Atypical Scatterplot",
             x = 0.25,
             y = 0.1,
             size = 3),
  left = gridtext::richtext_grob(
    "<i>r</i> = 0.99",
    rot = 90,
    gp = grid::gpar(fontsize = 10)
  )
)

########################### COMBINE ALL ###########################

ggarrange(plots_02, plots_06, plots_99, ncol = 1)
```

## Tools for Testing

However we design experiments, software plays a crucial role in allowing us to
carry them out. Fortunately, there is a wealth of tools available to facilitate
the testing of visualisations both in traditional lab-based tests and in online
experiments. Following the principles of open and reproducible research
\cite{ayris_2018}, closed-source software, such as Gorilla \cite{anwyl_2020} or
E-prime \cite{eprime_2020} was discounted, as these rely on paid licences and do
not allow for the sharing of code with future researchers. I settled on using
PsychoPy \cite{peirce_2019} due to its open-source status, flexibility regarding
graphical and code-based experimental design, and high level of timings accuracy
\cite{bridges_2020}. Using such an open-source tool not only facilitated my own
learning with regard to experiment building, but also enables the contribution
of further examples of visualisation studies by hosting the resulting
experiments online for use and modification by future researchers.

I elected to pursue online testing throughout this thesis. Doing so is much
quicker than carrying out in-person lab-based testing, facilitating the
collection of data from a much larger number of participants. This reduces the
chances of detecting false positives during analysis and ensures adequate levels
of power despite the potential for small effects sizes (see Section
\ref{recruitment}). Online testing also affords access to diverse groups of
participants across our populations of interest, especially when compared to the
relatively homogeneous student populations usually accessed in the lab by
doctoral researchers. Research has identified online experimentation as
producing reliable results that closely match those found in traditional
lab-based experiments \cite{arechar_2018, hirao_2021, prisse_2022}, especially
with large sample sizes. Due to its integration with PsychoPy,
[Pavlovia](pavlovia.org) was used to host all the experiments described in this
thesis. Section \ref{experimental-resources} contains links to all experiments
publicly hosted on Pavlovia's GitLab instance; these links are also provided as
experiments are described in each chapter.

## Recruitment & Participants {#recruitment}

Recruitment of participants online is possible through a range of service
providers, each with advantages and disadvantages. Research evaluating a number
of these providers recently found that Prolific \cite{prolific} and
CloudResearch provide the highest quality data for the lowest cost
\cite{douglas_2023}; I elected to use the former due to my familiarity with the
system. Despite these findings, there has been evidence of low data quality and
skewed demographics affecting both general crowdsourcing platforms, such as
Amazon's MTurk, and those tailored specifically towards academic research. On
the 24th of July, 2021, the Prolific.co platform went viral on social media
\cite{charalambides_2021}, leading to a participant pool heavily skewed towards
young people identifying as female. At the time, Prolific did not manually
balance the participants recruited for a study. This was addressed in the pilot
study (see Section \ref{pilot-study}) by preventing participants who joined
after this date from participating, in addition to manually requesting a 1:1
ratio of male to female participants. The demographic issues settled quickly,
however the screened 1:1 ratio was maintained for the remainder of the
experiments. Participants were only permitted to complete any of the experiments
on desktop or laptop computers, not on phones or tablets.

Chronologically, the first experiment conducted was a pilot study (see Section \ref{pilot-study}
for full details) investigating a very early iteration of the point opacity
manipulation in combination with exploratory work around plot size and
correlation estimation. At the time, I was relatively naive to the intricacies
of recruiting research participants online, and thus experienced issues
regarding participant engagement. Each experiment included attention check
questions in which participants were instructed to ignore the stimulus and
provide a specific answer. The advert for each experiment stated that failure of
more than 2 attention check items would result in a submission being rejected.
This pilot study suffered from a rejection rate of 57.5%, indicating very low
levels of participant engagement. For the following studies, published
guidelines \cite{peer_2021} were followed to address these issues; specifically,
it was required that participants:

-   Had previously completed at least 100 studies on Prolific.
-   Had an acceptance rate of at least 99% for those studies.[^1]

Following implementation of these pre-screen criteria, the rejection rate for
the next experiment fell to \~5%. Rejection rates were similar for the remainder
of experiments. Exact numbers of accepted and rejected participants can be found
in the \textbf{Participants} sections of each experiment.

Each full experiment recruited until 150 participants had completed
successfully. Due to the novelty of this work, it was difficult to get a sense
of the effect sizes that would be seen. I assumed a small effect size (Cohen’s
*d* \~ 0.2), and aimed to recruit enough participants to adequately power the
experiments \cite{brysbaert_2019}. NB: I did not conduct an *a priori* power
analysis. A post-hoc power analysis of the first experiment revealed a power of
0.54. Effect sizes were larger in the subsequent experiments, however to
facilitate comparison, it was decided that *N* = 150 would remain the target
recruitment rate.

[^1]: this is a more strict rate than the 95% recommended by Peer et al. \cite{peer_2021}.

## Creating Stimuli {#creating-stimuli}

All stimuli were created using `ggplot2` \cite{wickham_2016} in R. Specific
versions numbers are provided with regard to the specific visualisations
produced for each experiment. Identical principles were followed regarding data
visualisation design for each experiment bar the last, which is discussed
\textit{in situ}.

Experiments were designed with the intention of isolating and addressing a
perceptual effect; the underestimation of correlation in positively correlated
scatterplots. To achieve this, confounding extraneous design factors were
removed, including axis labels, tick labels, grid lines, and titles. The axis
ticks themselves were preserved. Scatterplots always featured 128 data points and
points had a hard size minimum of 12 pixels. @fig-scatter-example demonstrates 
the basic design of the scatterplots used in Experiments 1 to 4.

```{r}
#| label: fig-scatter-example
#| include: true
#| fig-cap: The basic design of scatterplots in Experiments 1 to 4.
#| out-width: NA
#| out-height: NA
#| fig-asp: 1

# create basic design of experimental items scatterplot
  
plot_example_function(slopes_0.6, t = "", o = 1, s = 0.4, 2)  
```

A single random seed was used to generate scatterplot datasets throughout this
thesis. 45 *r* values were uniformly generated between 0.2 and 0.99, as there is
evidence that little correlation is perceived below *r* = 0.2
\cite{strahan_1978, bobko_1979, cleveland_1982}. The data that forms the
scatterplots was randomly generated from bivariate normal distributions with
standard deviations of 1 in each direction. Scatterplots always had a 1:1 aspect
ratio, and were configured such that they occupied the same proportion of the
experimental window regardless of the size or resolution of a participant's
monitor. From \chap{chap:adjusting_size} onwards, a measure of dot pitch is
included, which facilitates the approximation of the physical size of
scatterplot points on-screen; where available, this is included in discussions
and analyses.

# Analytical Methods

All analyses in this thesis were conducted using R (version 4.5.1
\cite{r_core}). To investigate whether the experimental manipulations have
actual effects on the interpretations participants provide, appropriate
statistical testing must be employed. This involves taking into account the
variability in responses that can be attributed to an experimental manipulation
against the backdrop of other variability inherent in the dataset. Traditional
analysis of the data collected throughout this thesis would involve the use of
repeated measures analysis of variance (ANOVA). This technique assesses whether
there are significant differences in the means of the dependent variables
between conditions. While these techniques are commonplace, they do not allow
for comparisons of differences across the full range of individual participant
responses, nor do they allow for simultaneous consideration of by-item and
by-participant variance. It is for these reasons that linear mixed-effects
models were used throughout. Linear mixed-effects modelling is a reliable
approach that is resistant to a variety of distributional assumption violations
\cite{schielzeth_2020}, and facilitates the appreciation of the data story in a
broader and more detailed fashion.

## Linear Mixed-Effects Models

In a mixed-effects modelling paradigm, a distinction is made between variability
that is thought to arise as a result of an experimental manipulation (fixed
effects), and that which arises due to differences between, for example,
participants or particular experimental items (random effects). When a variable
is manipulated by a researcher in an experiment, each level of that variable is
present, meaning it is appropriate to be modelled as a fixed effect. When only a
*subset* of levels of a variable is present, such as a sample of all possible
participants or experimental items, then this variable is appropriate for
modelling as a random effect. Typically, mixed-effects models require the
specification of *intercepts*; these are different baselines for each
participant or item that reflect random deviations from the mean of the
dependent variable. Mixed-effects models may also specify random *slopes*; these
are differences in the magnitude of the difference between levels of the
independent variable for each random effect \cite{brown_2021}.
@fig-mixed-effects-demo visualises these concepts.

```{r}
#| label: fig-mixed-effects-demo
#| include: true
#| fig-cap: Visualising random intercepts and slopes for a theoretical experiment with 4 participants. The grand mean of the dependent variable is shown as a solid line, while each separate random intercept is drawn with dashed lines. Each line has a different gradient, representing different random slopes for each participant. This graphic was inspired by those featured in Brown, 2021 \cite{brown_2021}.
#| out-width: NA
#| out-height: NA
#| fig-asp: 0.6

# set random seed for reproducibility

set.seed(123)

# create dataframe for plotting
# values have been manually calculated

df <- data.frame(
  IV = c(0, 2, 4, 6, 8, 10),
  DV_mean = c(500, 700, 900, 1100, 1300, 1500),
  DV_1 = c(700, 800, 900, 1000, 1100, 1200),
  DV_2 = c(600, 700, 800, 900, 1000, 2000),
  DV_3 = c(800, 800, 900, 1000, 1100, 2300),
  DV_4 = c(400, 500, 600, 700, 800, 1200)
)

# create plot for illustrating linear mixed effects modelling

ggplot(df, aes(x = IV)) +
  geom_textsmooth(aes(y = DV_mean, label = "Grand Mean",
                      hjust = 1, vjust = 1.1),
                  method = "lm", se = FALSE, colour = "black", linewidth = 0.75) +
  geom_textsmooth(aes(y = DV_1, label = "P1",
                      hjust = 1, vjust = 1.1),
                  method = "lm", se = FALSE, colour = "grey",
                  linetype = "dashed", size = 3, linewidth = 0.5) +
  geom_textsmooth(aes(y = DV_2, label = "P2",
                      hjust = 1, vjust = -0.1),
                  method = "lm", se = FALSE, colour = "grey",
                  linetype = "dashed", size = 3, linewidth = 0.5) +
  geom_textsmooth(aes(y = DV_3, label = "P3",
                      hjust = 1, vjust = -0.1),
                  method = "lm", se = FALSE, colour = "grey",
                  linetype = "dashed", size = 3, linewidth = 0.5) +
  geom_textsmooth(aes(y = DV_4, label = "P4",
                      hjust = 1, vjust = 1.1),
                  method = "lm", se = FALSE, colour = "grey",
                  linetype = "dashed", size = 3, linewidth = 0.5) +
  theme_ggdist() +
  labs(x = "Independent Variable", y = "Dependent Variable")

# remove extraneous plotting dataframe

rm(df)
```

Throughout the course of this thesis, analyses attempt to model both random
intercepts and slopes in order to capture the maximum amount of variability
present in our datasets. In order to ascertain the goodness-of-fit of models,
their ability to explain variance is compared to that of a nested null model
\cite{singmann_2019}; such a model is identical bar the removal of the fixed
effect of interest. The likelihood ratio test (LRT) is used here to assess
goodness-of-fit. In cases where a model has in total more than two levels (here,
all experiments bar Experiment 5), the `emmeans` package (version 1.11.2-8 \cite{lenth_2024}) is
used to calculate estimated marginal means between levels of fixed effects.

## Ordinal Modelling

In Experiment 5, participants used Likert scales to provide responses. These
scales capture whether one rating is higher or lower than another, however they
do not quantify the magnitude of the difference between levels of rating. Metric
modelling, such as linear regression, treats the response options to a Likert
scale as if they were numeric. Doing so assumes equal levels of difference
between ratings, when in reality there is no theoretical reason to make this
assumption. Metric modelling is therefore considered inappropriate for modelling
Likert scale response data \cite{liddell_2018}. In light of these issues,
the `ordinal` package (version 2023.12-4.1 \cite{ordinal}) in R was used to build cumulative link
mixed-effects models for the analysis of Likert scale data. This allows for the
treatment of Likert responses as ordered factors as opposed to continuous
response scales.

## Model Construction {#model-construction}

Choices are inherent in every type of statistical analysis, and can play a large
role in the conclusions that are drawn from them. In linear mixed-effects
modelling, deciding *what* is a fixed or random effect is straightforward;
deciding *how to specify* random effects is a more complicated matter. Barr et
al. \cite{barr_2013} argue that for fully repeated measures designs, a maximal
model should be preferred; one with random intercepts and slopes for each
participant and experimental item. More recently, Bates et al. \cite{bates_2018}
have argued that attempting to specify maximal models for insufficiently rich
datasets may lead to overfitting and unreliable conclusions. In light of this I
sought a more systematic approach to selecting the random effects structure of a
given model.

In an attempt to balance simplicity, explanatory power, and model convergence
(whether or not a solution can be found), the `buildmer` package (version 2.12 \cite{buildmer})
in R was used to automate the selection of model specifications. Having been
provided with a maximal model, `buildmer` uses stepwise regression to select the
most complex model structure that successfully converges. Following this,
terms that fail to explain a significant amount of variance in the
dataset are dropped; this stepwise elimination of terms is evaluated using
successive likelihood ratio tests. This results in a model that captures the
maximal amount of feasible variability while minimising redundancy. Note that
`buildmer` was not relied upon as a modelling *panacea*; models are still based
on theoretical underpinnings and are evaluated critically.

## Effects Sizes

My approach to effects sizes evolved throughout the course of the research
project due to reviewer feedback and a growing appreciation of the complexities
of effect sizes when discussing linear mixed-effects models. Experiments 1, 2,
and 3 featured a condition with no scatterplot manipulation present (henceforth
referred to as a *baseline*); accordingly, the `EMAtools` package
\cite{ematools} was used to calculate equivalent Cohen's *d* effect sizes of
manipulation-present conditions relative to the baseline. Experiment 4 did not
feature a baseline condition, meaning Cohen's *d* was deemed inappropriate. The
`r2glmm` package (version 0.1.3 \cite{r2glmm}) was used instead to calculate semi-partial R^2^.
In lieu of a traditional measure of effect size, this demonstrated the unique
variance in the dependent variable explained by each level of the independent
\cite{nakagawa_2013}. Experiment 5 features a much simpler modelling situation,
and returns to providing equivalent Cohen's *d* values for the pre- vs. post-
plot viewing conditions, this time calculated by converting odds ratios using
the `effectsize` package (version 1.0.1 \cite{effectsize}). More details on specific
calculations, measures, and conclusions can be found *in situ*.

## Reporting Analyses

Throughout this thesis, a broad approach to the reporting of statistical
analyses was taken; while I consider our analytical methods and conclusions
valid, I also present a range of statistics to allow the reader to draw their
own conclusions, should they wish. Statistical results are visualised where
appropriate, and where visualisation aids understanding and interpretation. In
addition, details about model structures and the issues I tackled when modelling
are included for transparency \cite{meteyard_2020}.

# Computational Methods

The approach to computational methods in this thesis sought to marry
practicality, simplicity, and reproducibility. Often, this meant that what would
otherwise be a makeshift script followed by copy-pasting of results into
[Overleaf](https://www.overleaf.com/) ended up being an involved exercise in
literate programming \cite{knuth_1984} and code wrangling. This involved effort
and time, particularly in the early stages of the project, however has yielded a
number of benefits. Many of the techniques developed early in the project proved
to be instrumental later on, resulting in time-savings overall. Additionally,
these techniques, principles, and practices are shared to enable future
researchers to learn, where I struggled. In this section, I detail my approach
to computational methods, including how the idea of **executable papers** was
utilised, and how containerised environments were used to capture a freeze-frame
of the analyses.

## Executable Reporting

Each paper published throughout this project, and this thesis, has been written
to be executable. Packaging research in such a way means a lay person can follow
simple instructions to recreate the work, while also facilitating and
encouraging literate programming, or the close alignment of documentation and
underlying code \cite{piccolo_2016}.

The use of a literate programming paradigm to generate reports (usually using
LaTeX) has a rich history. This section focuses on this history as it pertains
to the language used throughout this project, R. `Sweave` \cite{leisch_2002},
written in 2002, allowed R code to be integrated into LaTeX documents. This was
followed by Yihui Xie's `knitr` \cite{xie_2015}, which expanded `Sweave`
functionality and improved integration with tools such as `pandoc`
\cite{pandoc}. `knitr` uses `Rmarkdown` \cite{xie_2020} to mix
markdown-flavoured text with code chunks into a document that can be rendered
into an appropriately-formatted conference or journal pdf; this workflow was
used for the papers associated with Experiments 1, 2, and 3. `Quarto`
\cite{allaire_2024}, released in 2022, further expands on `Rmarkdown`
functionality, and removes reliance on R or Rstudio. `Quarto` was used for the
remainder of the papers associated with this project, and for the present
thesis.

Writing executable or dynamic documents allows results to be re-generated
whenever the document is rendered. This includes any associated data
visualisation and statistical modelling. Structuring documents like this
effectively "opens up" research by allowing others to view the code that
performed the analysis and generated the data visualisations, in addition to
guarding against accusations of questionable research practices (QRPs) through
high levels of transparency \cite{holmes_2021}. This paradigm also allows for
the caching of computationally expensive statistical models.

## Containerised Environments

Providing the code associated with a project, even when that code is integrated
into a literately programmed executable paper, is necessary, but not sufficient,
for enabling adequate reproducibility. Previous work has found many instances
where publicly-accessible code could not reproduce the results included in the
corresponding document or failed to run entirely
\cite{collberg_2016, trisovic_2022, samuel_2024}. Poor programming practices
accounted for a significant portion of these problems, highlighting the issue of
researchers without technical backgrounds being expected to produce high quality
technical documentation. Elsewhere, differences in computational environment,
package versions, and operating systems have been identified as responsible for
the non-replication of results. Large research projects, such as this, can
include hundreds of functions from scores of packages, meaning that small
changes can critically break code.

These issues were addressed using containers, specifically, those created by
[Docker](https://www.docker.com/) \cite{merkel_2014, boettiger_2015}. 1979 saw
the development of `chroot` (`change root`), which is able to isolate an
application's file access to a ‘chroot jail’. Since then, containerisation
software has experienced rapid development and uptake, mostly within the
software development and security communities. Docker, released in 2013, is a
popular, lightweight containerisation tool that enables a precise recreation of
computational environments. Recording software versions and dependencies avoids
the potential for broken code in the future, and publicly hosting papers as
GitHub repositories that build into Docker containers ensures that future
researchers can interact with code and data in the same computational
environment used when carrying out the research. While virtual machines make
isolated sections of hardware available, containers abstract protected parts of
the operating system \cite{merkel_2014}. This makes containers smaller and more
lightweight than full virtual machines, while still conferring the advantages of
virtualisation. For the Docker implementation here, portable R environments
provided by the Rocker project \cite{boettiger_2017} are used. These
environments are agnostic regarding the host operating system, allowing the
reader to reproduce the analyses featured here in a replica of the computational
environment they were conducted in.

Building Docker containers is facilitated through a Dockerfile. This file
instructs Docker to build a container with the appropriate version of R, the
files required, and the correct package versions used during analysis. Below is
a minimal example of a Dockerfile.

I first specify the Rocker image that will form the basis of the container. This
includes the version of R required (version 4.5.1), the Rstudio Integrated
Development Environment (IDE), Quarto, and the `tidyverse` package.

`FROM rocker/version:4.5.1`

Next, I add the files and folders required, including the Quarto document and
related files, chapter folder, bibliography, additional scripts, LaTeX class
file and template, the folders containing the cached models and raw data, and
the R project file:

`ADD thesis.qmd /home/rstudio/`

`ADD _quarto.yml /home/rstudio/`

`ADD chapters_quarto/ /home/rstudio/chapters_quarto/`

`ADD thesis.bib /home/rstudio/`

`ADD reformat_tex.R /home/rstudio/`

`ADD finalise_thesis.R /home/rstudio/`

`ADD uom_thesis_casson.cls /home/rstudio/`

`ADD main.tex /home/rstudio/`

`ADD data/ /home/rstudio/data/`

`ADD cache/ /home/rstudio/cache/`

`ADD thesis.Rproj /home/rstudio/`

Finally, I add the specific versions of the R packages used throughout the
course of this thesis. For brevity, I only display the addition of the first
three here:

`RUN R -e "devtools::install_version('MASS', version = '7.3-65', dependencies = T)"`

`RUN R -e "devtools::install_version('ggdist', version = '3.3.3', dependencies = T)"`

`RUN R -e "devtools::install_version('ggpubr', version = '0.6.1', dependencies = T)"`

`...`

# Reproducibility In This Thesis

Reproducibility is a broad spectrum \cite{peng_2011} (see
@fig-reproducibility-spectrum). As discussed above, even when code and data are
provided, results are often not replicable, and this is before issues around
poor research practice, inappropriate analysis, and dishonest science even rear
their heads. While for most, the reproducibility crisis \cite{osf_2015}
crystallised in the early 2010s \cite{peng_2015}, serious concerns had been
voiced since at least the late 1960s \cite{romero_2019}. Since coming into the
wider academic consciousness, numerous studies have identified reasons for the
crisis, ranging from poor practice (e.g. Potti et al., 2006 \cite{potti_2006})
to outright deception and fabrication (e.g. the Woo-Suk Hwang scandal
\cite{saunders_2008}). These issues led this project to strive for a gold
standard \cite{peng_2011} of reproducibility throughout. In this section, I
detail how this was accomplished, and in doing so, expose my work to welcome
critique.

## Sharing Data and Code

The open and public sharing of data and code facilitates external assessment
\cite{klein_2018, alter_2018} and secondary use of data \cite{tamuhla_2023}, and
guards against reproducibility issues \cite{miyakawa_2020}. Quite aside from
external motivating factors, I found that developing and embedding the
reproducibility practices described here have resulted in longer term savings in
time and effort. Favouring a gold-standard reproducible approach is also a way
of "paying it forward"; having come from a non-technical background, I found
previous work that adhered to the same standard critical for my own learning and
development. GitHub is used to host both this thesis and the papers associated
with the project; links to these repositories can be found throughout. I favour
permissive and lenient licencing, such as the MIT licence \cite{mit} for GitHub
repositories and the CC-BY 4.0 license for pre-registrations. These enable
future researchers to re-use data and code while providing clear guidance for
appropriate use and facilitating long-term sustainability \cite{jimenez_2017}.

```{r}
#| label: fig-reproducibility-spectrum
#| include: true
#| fig-cap: Peng's (2011) Reproducibility Spectrum. This figure has been reproduced from Peng (2011) \cite{peng_2011}.
#| out-width: NA
#| out-height: NA
#| fig-asp: 0.2

# create plotting dataframe for reproducibility spectrum

df <- data.frame(
  section = c("Publication\nonly",
              "Code",
              "Code\nand\ndata",
              "Linked and\nexecutable\ncode and data",
              "Full\nreplication"),
  
  # manually specify locations for each label
  
  xmin = c(0, 1, 2, 3, 4),
  xmax = c(1, 2, 3, 4, 5),
  ymin = c(1, 1, 1, 1, 1),
  ymax = c(2, 2, 2, 2, 2)
)

# create plot with geom_rect()

ggplot() +
  geom_rect(data = df, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), 
            fill = "lightgrey", color = "black") +
  geom_text(data = df, aes(x = (xmin + xmax)/2, y = 1.5, label = section), size = 3) +
  annotate("segment", x = 2, xend = 5, y = 2.1, yend = 2.1, 
           arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "grey") +
  annotate("segment", x = 2, xend = 0, y = 2.1, yend = 2.1, 
           arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "grey") +
  annotate("text", x = 0.5, y = 2.3, label = "Not reproducible", size = 3) +
  annotate("text", x = 4.5, y = 2.3, label = "Gold standard", size = 3) +
  
  # use theme_void() to remove extra stuff we don't want
  
  theme_void()
```

## Executable Papers and Docker Containers

As detailed above, Quarto and Docker were used to produce executable
journal/conference papers for each of the published works this thesis describes.
For simplicity, all analyses from these papers have been repeated using up to
date packages here. Accordingly, a single implementation of Docker is provided
to reproduce this thesis. All statistics have been checked against those
provided in the original analyses, and repositories for the corresponding papers
are provided complete with original Docker implementations.

## Pre-Registration of Hypotheses and Analysis Plans

Often touted as a low-cost entry point into reproducible research practices
\cite{logg_2021}, pre-registration is the practice of formally clarifying
hypotheses and analysis plans prior to data collection. While this may not be
able to prevent research fraud and QRPs entirely, it does lend credibility to
the researcher \cite{simmons_2021}. All hypotheses and analysis plans were
pre-registered with the Open Science Framework \cite{OSF}. Pre-registrations are
embargoed by the research team prior to data collection, and then made public in
a frozen state following publication of the corresponding research. Where I felt
it necessary to deviate from these plans, details are provided in the methods
sections of the corresponding experiments.

## Experimental Resources {#experimental-resources}

Everything needed to run each experiment is included in the corresponding GitLab
repository. Links to these repositories are also provided in the sections
concerning each experiment.

\textbf{Chapter 4}

Experiment 1:

[gitlab.pavlovia.org/Strain/exp_uniform_adjustments](https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments)

Experiment 2:

[gitlab.pavlovia.org/Strain/exp_spatially_dependent](https://gitlab.pavlovia.org/Strain/exp_spatially_dependent)

\textbf{Chapter 5}

Experiment 3:

[gitlab.pavlovia.org/Strain/exp_size_only](https://gitlab.pavlovia.org/Strain/exp_size_only)

\textbf{Chapter 6}

Experiment 4:

[gitlab.pavlovia.org/Strain/size_and_opacity_additive_exp](https://gitlab.pavlovia.org/Strain/size_and_opacity_additive_exp)

\textbf{Chapter 7}

Experiment 5 Pre-Study:

[gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest](https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest)

Experiment 5 Main Study (Group A):

[gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a](https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a)

Experiment 5 Main Study (Group B):

[gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t](https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t)

# Conclusion

In this chapter, I have established the broad methodological approach taken by
this thesis. This project sought to investigate novel ways of visualising data
and their effects on perception and cognition. I have provided justifications
for the designs used, the methodological challenges faced, and how the use of a
broad array of tools and techniques was able to overcome these challenges.
Throughout, I have detailed how I have learnt from my mistakes. Open research
and reproducibility is at the core of the work described here, and I hope this
thesis can serve as an example for future work facing similar challenges and
with similar commitments to open science. To this end, I have produced a
template[^2] to facilitate future reproducible theses. FAIR (**F**indable, **A**ccessible,
**I**nteropable, and **R**eusable) data principles \cite{wilkinson_2016} are
satisfied through public sharing of data and code, literate programming, and
containerisation.

[^2]: https://github.com/gjpstrain/UoM_reproducible_thesis_template
