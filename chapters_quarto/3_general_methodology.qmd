---
title: "general_methodology"
output:
  format: 
    latex:
      
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

```{r}
#| label: setup

# load required packages

library(tidyverse)
library(MASS)

```


# Introduction

In this chapter we describe our research methodologies. Chapters 4, 5, and 6 share
most aspects of experimental method, while the experiment described in chapter 7 differs
substantially. Throughout this chapter, the reader should assume that we are 
referring to the entire body of experimental work this thesis describes. Methods
that differ regarding the final experiment in chapter 7 are detailed along the way.
In this chapter, we discuss our experimental designs, the tools we use to build
and run our experiments, our approach to statistical analyses, and the computational
methods and practices we employed particularly with regards to reproducibility
and open science.

# Experimental Methods

It is important to acknowledge that the way in which we conduct experiments influences
what we find and the conclusions that we may draw from those findings. The decisions
that lead us to designing experiments in certain ways must be based not only
on theory, but also on the practical constraints imposed by external factors on
the research team. Concerns such as time, convenience, and cost must be addressed,
and a compromise between research that is *valuable* and research that is *doable*
must be reached. We focused on pragmatism and impact throughout the course of this
research project; happily, the research journey we embarked on resulted in methodologies
that satisfied both principles. It is for this reason that we consider the framework
we present to be a key contribution of this thesis.

## Experimental Design

All but our final experiment utilised within-participants designs. Each
participant saw all experimental stimuli and provided a judgement 
of correlation using a sliding scale between 0 and 1 (see @fig-slider). Experiments 1 to 3 featured
featured a single experimental factor of design, all with 4 levels corresponding
to scatterplots with different design features. Experiment 4 employed a factorial
2 $\times$ 2 design. Experiment 5 is a departure from the shared experimental
paradigm of the previous experiments, and features a 1 factor, 2 level 
between-participants design.

```{r}
#| label: fig-slider
#| include: true
#| fig-cap: An example of the slider participants used to estimate correlation in experiments 1-4.
#| out-width: NA
#| out-height: NA

# find slider png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example_slider.png", dpi = NA)
```

## Tools for Testing

Whatever the design of our experiments, software plays a crucial role in allowing 
us to carry them out. Fortunately, at the time of writing, there is a wealth of 
tools available to facilitate the testing of visualisations both in traditional
lab-based tests and in online experiments. As we adhere to the principles of 
open and reproducible research \cite{ayris_2018}, we discount closed-source
software, such as Gorilla \cite{anwyl_2020} or E-prime \cite{eprime_2020}, as these
rely on paid licenses and do not allow us to share code with future researchers.
We settled on using PsychoPy \cite{peirce_2019} due to its open-source status,
flexibility regarding graphical and code-based experimental design, and high 
level of timings accuracy \cite{bridges_2020}. Using such a open-source tool not only
facilitated our own learning with regard to experiment building, but also enables
to contribute further examples of visualisation studies by hosting the resulting
experiments online for use and modification by future researchers.

We elected to pursue online testing throughout this thesis. Doing so is 
much quicker than carrying out in-person lab-based testing, meaning we can collect
data from a much larger number of participants. This reduces the chances of detecting
false positives during analysis and ensures adequate levels of power despite the
potential for small effects sizes. Online testing also affords us access to 
diverse groups of participants across our populations of interest, especially
when compared to the relatively homogeneous student populations usually accessed
by doctoral researchers. Research has identified online experimentation as producing
reliable results that closely match those found in traditional lab-based 
experiments \cite{arechar_2018, hirao_2021, prisse_2022}, especially with large
sample sizes. Due to its integration with PsychoPy, we chose to use
Pavlovia (pavlovia.org) to host all the experiments described in this thesis.
Section \ref{experimental-resources} contains links to all experiments publicly hosted
on Pavlovia's GitLab instance.

## Recruitment \& Participants

Recruitment of participants online is possible through a range of service providers,
each with advantages and disadvantages. Research evaluating a number of these
providers recently found that Prolific \cite{prolific_2024} and CloudResearch 
provide the highest quality data for the lowest cost \cite{douglas_2023}; we elected
to use the former due to familiarity with the system. Despite these findings,
there has also been evidence of low data quality and skewed demographics affecting
even high quality platforms tailored towards academic research. On the 24th of July, 2021,
the Prolific.co platform went viral on social media \cite{chara_2021}, leading to a participant
pool heavily skewed towards young people identifying as female. At the time,
Prolific did not manually balance the participants recruited for a study. We addressed
this in our pilot study (see Section \ref{pilot}) by preventing participants
who joined after this data from participating, in addition to manually requesting
a 1:1 ratio of male to female participants. The demographic issues settled quickly,
however we maintained our screened 1:1 ratio for the remainder of the experiments.

The first experiment we conducted was a pilot study (see Section \ref{pilot} for full details)
investigating a very early iteration of the point opacity manipulation in combination
with exploratory work around plot size and correlation estimation. At the time,
the author was relatively naive to the intricacies of recruiting research participants
online, and thus experienced issues with regards to participant engagement. Each
experiment, including the pilot, included attention check questions in which participants
were instructed to ignore the stimulus and provide a specific answer. We stated in
the advert for each experiment that failure of more than 2 attention check items
would result in a submission being rejected. This pilot study suffered from a 
rejection rate of 57.5\%, indicating that we were experiencing low levels of
participant engagement. For our following studies, we therefore followed published
guidelines \cite{peer_2021} to address these issues; specifically, we required
that participants:

  - Had previously completed at least 100 studies on Prolific.
  - Had an acceptance rate of at least 99% for those studies. [^1]
  
Following implementation of these pre-screen criteria, the rejection rate
for our next experiment fell to ~5\%. Rejection rates were similar for the remainder
of our experiments. Exact numbers of accepted and rejected participants can be
found in the \textbf{Participants} sections of each experiment.

[^1]: this is a more strict rate than the 95% recommended by Peer et al. \cite{peer_2021}.

## Creating Stimuli

All our stimuli were created using `ggplot2` in R. Specific versions are cited separately
with regard to the specific visualisations produced for each experiment. We followed
identical principles regarding data visualisation design for each experiment bar the
last, which is discussed \textit{in situ}. 

We designed with the intention of isolating and addressing a perceptual effect;
the underestimation of correlation in positively correlated scatterplots. For this
reason, we sought to remove the potential for other design factors to have effects
on correlation estimation. To this end, we removed most of the conventionally
present visual features of scatterplots, including axis labels, tick labels,
grid lines, and titles. We elected to preserve the axis ticks themselves.
@fig-scatter-example demonstrates the basic design of the scatterplots
used in experiments 1 to 4.

```{r}
#| label: fig-scatter-example
#| include: true
#| fig-cap: The basic design of scatterplots in experiments 1 to 4. 
#| out-width: NA
#| out-height: NA
#| fig-asp: 1

 set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
plot_example_function <- function (d) {
  
  set.seed(1234)
  
  ggplot(d, aes(x = V1, y = V2)) +
  scale_alpha_identity() +
  geom_point(shape = 16, size = 1)  +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 15))
}
  
plot_example_function(data_with_resid)  
```

# Analytical Methods

## Linear Mixed-Effects Models

To investigate whether the experimental manipulations we test have actual effects
on the interpretations participants provide, we must employ appropriate statistical
testing. This involves taking into account the variability in responses that can be
attributed to an experimental experimental against the backdrop of other variability
inherent in the dataset. To accomplish this, we utilise linear mixed-effects modelling,
a broadly applicable and reliable approach that is also resistant to a variety
of distributional assumption violations \cite{schielzeth_2020}.

- fixed and random effects
- how does modelling work?

## Advantages Over Aggregate-Level Statistical Tests

## Model Construction

## Effects Sizes

## Reporting Analyses

# Computational Methods

## Executable Reporting

## Containerised Environments

# Reproducibility In This Thesis

## Sharing Data and Code

## Executable Papers and Docker Containers

## Experimental Resources {#experimental-resources}

Everything needed to run each experiment is included in the corresponding
GitLab repository. Links to these repositories are also provided in the sections
concerning each experiment.

\textbf{Chapter 4}

Experiment 1: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments

Experiment 2: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent

\textbf{Chapter 5}

Experiment 3: https://gitlab.pavlovia.org/Strain/exp_size_only

\textbf{Chapter 6}

Experiment 4: https://gitlab.pavlovia.org/Strain/size_and_opacity_additive_exp

\textbf{Chapter 7}

Experiment 5 Pre-Study: https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest

Experiment 5 Main Study (Group A): https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a

Experiment 5 Main Study (Group B): https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t


# Conclusion
