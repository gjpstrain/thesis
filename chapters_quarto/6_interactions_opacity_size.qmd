---
title: "interactions_opacity_size"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false         
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_e4 <- c(A = "Typical Orientation Size\nTypical Orientation Opacity",
               B = "Inverted Orientation Size\nInverted Orientation Opacity",
               X = "Typical Orientation Size\nInverted Orientation Opacity",
               Y = "Inverted Orientation Size\nTypical Orientation Opacity")

labels_e4_regex <- c(
  "\\bA\\b" = "Typical Orientation Size Typical Orientation Opacity",
  "\\bB\\b" = "Inverted Orientation Size Inverted Orientation Opacity",
  "\\bX\\b" = "Typical Orientation Size Inverted Orientation Opacity",
  "\\bY\\b" = "Inverted Orientation Size\nTypical Orientation Opacity"
)
```

```{r}
#| label: load-data

exp4_anon <- read_csv("../data/exp_4_data.csv", guess_max = 17000) %>%
  mutate("expName" = recode(expName,
                            "size_and_opacity_exp" = "E4_interactions"))

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models-chap6

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels",
             "participant",
             "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct",
           "VT_no_correct",
           "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*1.777,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch",
                  "participant",
                  "res_width",
                  "width",
                  "height")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-I")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item",
                     "opacity",
                     "size"),
           sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    opacity == "S" & size == "S" ~ "A",
    opacity == "I" & size == "I" ~ "B",
    opacity == "S" & size == "I" ~ "X",
    opacity == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
            "item",
            "size",
            "opacity",
            "slider.response",
            "my_rs",
            "total_residuals",
            "unique_item_no",
            "session",
            "trials.thisN",
            "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "opacity", "size", "condition_abs")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(exp4_anon)

# set deviation coding for experimental model

contrasts(E4_interactions_tidy$size) <- matrix(c(.5, -.5))
contrasts(E4_interactions_tidy$opacity) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(exp4_anon)

# extract age and gender data

extract_age(E4_interactions_tidy)

extract_gender(E4_interactions_tidy)

extract_literacy(E4_interactions_tidy)
```

# Abstract {#abstract-interactions}

# Introduction {#introduction-interactions}

# Related Work {#related-work-interactions}

# Hypotheses {#hypotheses-interactions}

# Methods {#methods-e4}

## Stimuli {#stimuli-e4}

## Design {#design-e4}

## Procedure {#procedure-e4}

## Participants {#participants-e4}

Normal to corrected-to-normal vision and English fluency were required. Participants who
had completed any of the experiments described in \chap{chap:adjusting_opacity} or \chap{chap:adjusting_size}
were prevented from participating. Data were collected from 158 participants.
8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E4_interactions_tidy_gender$M, digits = 0)`% male, `r printnum(E4_interactions_tidy_gender$F, digits = 0)`
% female, and `r printnum(E4_interactions_tidy_gender$NB, digits = 0)`% non-binary). 
Participants' mean age was `r printnum(E4_interactions_tidy_age$mean)`
(*SD* = `r printnum(E4_interactions_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E4_interactions_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E4_interactions_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 37 minutes (SD = 12.3 minutes). 

# Results {#results-e4}

```{r}
#| label: model-e4
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e4_model <- buildmer(difference ~ size * opacity + 
                       (1 + size * opacity | participant) +
                       (1 + size * opacity | item),
                     data = E4_interactions_tidy)
```

```{r}
#| label: model-e4-lit-dot-pitch
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build additional models (literacy, dot pitch, and training)

e4_lit_model <- add_fixed_effect(e4_model, "literacy", "E4_interactions_tidy")

e4_dot_pitch_model <- add_fixed_effect(e4_model, "dot_pitch", "E4_interactions_tidy")

e4_VT_model <- add_fixed_effect(e4_model, "VT_no_correct", "E4_interactions_tidy")

e4_training_model <- add_fixed_effect(e4_model, "half", "E4_interactions_tidy")

e4_trial_number_model <- add_fixed_effect(e4_model, "trials.thisN", "E4_interactions_tidy")
```

```{r}
#| label: model-e4-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e4_model_cmpr <- comparison(e4_model)
```

```{r}
#| label: anova-results-e4

# do all anovas now

anova_results(e4_model, e4_model_cmpr)

anova_results(e4_lit_model, e4_model)

anova_results(e4_dot_pitch_model, e4_model)

anova_results(e4_VT_model, e4_model)

anova_results(e4_training_model, e4_model)

anova_results(e4_trial_number_model, e4_model)
```

To investigate the effects of combining point size and opacity decay functions on
participants' estimates of correlation, a linear mixed effects model was built
whereby the particular combination of point size and opacity decay function
employed is a predictor for the difference between objective *r* values
for each plot and participants' estimates of *r*. Deviation coding was used
for each of the experimental factors, which allows comparison between means of
*r* estimation error and the grand mean. This model has random intercepts for 
items and participants, and random slopes for participants with regards to the
size decay factor. A likelihood ratio test revealed that the model including
the size and opacity decay conditions as predictors explained significantly
more variance than the null ($\chi^2$(`r in_paren(e4_model.df)`) = 
`r printnum(e4_model.Chisq)`, *p* `r printp(e4_model.p, add_equals = TRUE)`).
There were significant fixed effects of size and opacity decay function, as 
well as a significant interaction between the two. @fig-e4-estimates shows 
the mean errors in correlation estimation for combination of conditions,
along with 95% confidence intervals.

```{r}
#| label: fig-e4-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 4. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e4_model) == "buildmer") e4_model <- e4_model@model

as_tibble(emmeans(e4_model, pairwise ~ size * opacity)[[1]]) %>%
  mutate("size" = recode(size,
                         "S" = "Typical Orientation Size",
                         "I" = "Inverted Orientation Size"),
          "opacity" = recode(opacity,
                         "S" = "Typical Orientation Opacity",
                         "I" = "Inverted Orientation Opacity")) %>%
  ggplot(aes(x = reorder(size:opacity, emmean), y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  scale_x_discrete(labels = ~ gsub(":", "\n", .x)) +
  theme_ggdist() + 
  labs(y = "Estimated Marginal Mean of Error",
       x = " Size : Opacity") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)
```

```{r}
#| label: tbl-contrasts-e4
#| include: true
#| tbl-cap: Contrasts between different levels of the size and opacity decay factors in experiment 4.

# assign slot, if it hasn't been assigned already

if (class(e4_model) == "buildmer") e4_model <- e4_model@model

# make table df

contrasts_extract(e4_model, "size:opacity") %>%
  mutate('Contrast' = recode(Contrast,
         "S I - I I" = "TO Size x IO Opacity | IO Size x IO Opacity",
         "S I - S S" = "TO Size x IO Opacity | TO Size x TO Opacity",
         "S I - I S" = "TO Size x IO Opacity | IO Size x TO Opacity",
         "I I - S S" = "IO Size x IO Opacity | TO Size x TO Opacity",
         "I I - I S" = "IO Size x IO Opacity | IO Size x TO Opacity",
         "S S - I S" = "TO Size x TO Opacity | IO Size x TO Opacity")) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

The effects found were driven by significant difference between means of correlation
estimation error between all conditions besides that which compares the two
incongruent decay conditions. Statistical testing for contrasts were performed
using the `emmeans` package \cite{lenth_2024}, and are provided in @tbl-contrasts-e4.

Models including participants' graph literacy, their performance on the
point visibility task, the dot pitch of participants'
monitors, and which half a particular correlation judgement took place were
built and compared with the experimental model.
While no significant effects of graph literacy ($\chi^2$(`r in_paren(e4_lit_model.df)`) = 
`r printnum(e4_lit_model.Chisq)`, *p* `r printp(e4_lit_model.p, add_equals = TRUE)`),
performance on the point visibility task ($\chi^2$(`r in_paren(e4_VT_model.df)`) = 
`r printnum(e4_VT_model.Chisq)`, *p* `r printp(e4_VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(e4_dot_pitch_model.df)`) = `r printnum(e4_dot_pitch_model.Chisq)`,
*p* `r printp(e4_dot_pitch_model.p, add_equals = TRUE)`) were found, there was a 
significant effect of training ($\chi^2$(`r in_paren(e4_training_model.df)`) = 
`r printnum(e4_training_model.Chisq)`, *p* `r printp(e4_training_model.p, add_equals = TRUE)`),
with participants rating correlation .01 lower during the second half. This drop
suggests that having more recently viewed the training plots may have increased
participants estimates of correlation. To further analyse this variability,
a model was built including trial number, allowing for the analysis of error.
A significant effect of trial number is also found 
($\chi^2$(`r in_paren(e4_trial_number_model.df)`) = 
`r printnum(e4_trial_number_model.Chisq)`, *p* 
`r printp(e4_trial_number_model.p, add_equals = TRUE)`)
on participants' correlation estimation errors.

```{r}
#| label: fig-e4-trial-number
#| include: true
#| fig-cap: Comparing mean errors in correlation estimation by trial number. Points represent unsigned mean errors for each trial number. The plotted line is the locally estimated smoothed curve, with the ribbon representing standard errors.
#| fig-asp: 0.4

E4_interactions_tidy %>%
  drop_na() %>%
  group_by(trials.thisN) %>%
  summarise(sd = sd(abs(difference)), mean = mean(abs(difference))) %>%
  ggplot(aes(x = trials.thisN, y = mean)) +
  geom_point(alpha = 0.4, shape = 16) +
  geom_smooth(se = T, span = 0.5, colour = "black", size = 0.7,) +
  scale_x_continuous(breaks = seq(0,180, by = 20), limits = c(0, 180)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_vline(xintercept = 90, linetype = 2) +
  geom_vline(xintercept = 180, linetype = 2) +
  labs(x = "Trial Number",
       y = "Unsigned Mean Error") +
  annotate("text",
           x = 45,
           y = 0.145,
           label = "1st Half") +
    annotate("text",
           x = 135,
           y = 0.145,
           label = "2nd Half") +
  theme_ggdist() +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) 
```

@fig-e4-trial-number is great.






















# Discussion {#discussion-e4}

# Conclusion {#conclusion-interactions}
