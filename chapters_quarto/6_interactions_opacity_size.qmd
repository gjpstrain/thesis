---
title: "interactions_opacity_size"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false         
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)
library(r2glmm)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_e4 <- c(A = "Typical Orientation Size\nTypical Orientation Opacity",
               B = "Inverted Orientation Size\nInverted Orientation Opacity",
               X = "Typical Orientation Size\nInverted Orientation Opacity",
               Y = "Inverted Orientation Size\nTypical Orientation Opacity")

labels_e4_regex <- c(
  "\\bA\\b" = "Typical Orientation Size Typical Orientation Opacity",
  "\\bB\\b" = "Inverted Orientation Size Inverted Orientation Opacity",
  "\\bX\\b" = "Typical Orientation Size Inverted Orientation Opacity",
  "\\bY\\b" = "Inverted Orientation Size\nTypical Orientation Opacity"
)
```

```{r}
#| label: load-data

exp4_anon <- read_csv("../data/exp_4_data.csv", guess_max = 17000) %>%
  mutate("expName" = recode(expName,
                            "size_and_opacity_exp" = "E4_interactions"))

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models-chap6

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels",
             "participant",
             "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct",
           "VT_no_correct",
           "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*1.777,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch",
                  "participant",
                  "res_width",
                  "width",
                  "height")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-I")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item",
                     "opacity",
                     "size"),
           sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    opacity == "S" & size == "S" ~ "A",
    opacity == "I" & size == "I" ~ "B",
    opacity == "S" & size == "I" ~ "X",
    opacity == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
            "item",
            "size",
            "opacity",
            "slider.response",
            "my_rs",
            "total_residuals",
            "unique_item_no",
            "session",
            "trials.thisN",
            "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "opacity", "size", "condition_abs")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(exp4_anon)

# set deviation coding for experimental model

contrasts(E4_interactions_tidy$size) <- matrix(c(.5, -.5))
contrasts(E4_interactions_tidy$opacity) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(exp4_anon)

# extract age and gender data

extract_age(E4_interactions_tidy)

extract_gender(E4_interactions_tidy)

extract_literacy(E4_interactions_tidy)

# significance table function. This function is only used in this chapter, and 
# hence does not appear in the shared_functions.R file

make_sig_table <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  table_df <- as.data.frame(summary(model)[10]) %>%
    rename("Estimate" = "coefficients.Estimate",
           "Standard Error" = "coefficients.Std..Error",
           "df" = "coefficients.df",
           "t-value" = "coefficients.t.value",
           "p" = "coefficients.Pr...t..") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p")
  
  # get semi-partial r^2, rename rows so they match with sig table
  # take the last 3 rows corresponding to fixed effects and interaction term
  # remove first cell of r^2
  # reformat text for latex output
  
  r_squared <- r2beta(model, method = "nsj") %>%
    mutate("Effect" = recode(Effect,
                             "size" = "Size Decay",
                             "opacity" = "Opacity Decay",
                             "size:opacity" = "Size Decay x Opacity Decay")) %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
  # tidy up row names
  
  rownames(table_df) <- c("(Intercept)",
                          "Size Decay",
                          "Opacity Decay",
                          "Size Decay x Opacity Decay")
  
  sig_and_squared <- cbind(table_df, r_squared) %>% select(-c("Effect")) 
  
  table <- kbl(sig_and_squared, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```

# Abstract {#abstract-interactions}

# Introduction {#introduction-interactions}

# Related Work {#related-work-interactions}

# Hypotheses {#hypotheses-interactions}

A single experiment is present in this chapter based on the effects of adjusting
point opacity and size on correlation estimation established in \chap{chap:adjusting_opacity}
and \chap{chap:adjusting_size}. Here, previously independently tested point opacity and size
manipulations are tested in both typical orientation (point opacity/size is 
reduced with residual magnitude) and inverted orientation (point opacity/size
is increased with residual magnitude). Throughout this chapter, referral is made
to *congruent* and *incongruent* conditions with respect to the combination of 
point opacity and size decay functions. *Congruent* conditions are those in which
point opacity and size decay act in the same direction (typical or inverted),
while for *incongruent* conditions, point opacity and size decay act against 
each other. Due to previous findings that non-linearly reducing point opacity 
or size as a function of residual magnitude can bias correlation estimates upwards,
it is hypothesised that:

 - H1: an increased reduction in correlation estimation error will be observed
 when congruent typical orientation decay functions are used.
 H2: the use of a congruent inverted function, such that point opacity and size
 are both increased with residual magnitude, will produce the least accurate
 estimates of correlation.
 
Owing to the finding from \chap{chap:@adjusting_size} that point size is a stronger
channel for biasing correlation estimation than point opacity, it is also 
hypothesised that:

 - H3: there will be a significant difference in correlation estimates between
 the two incongruent orientation conditions.

# Methods {#methods-e4}

## Stimuli {#stimuli-e4}

## Design {#design-e4}

## Procedure {#procedure-e4}

## Participants {#participants-e4}

Normal to corrected-to-normal vision and English fluency were required. Participants who
had completed any of the experiments described in \chap{chap:adjusting_opacity} or \chap{chap:adjusting_size}
were prevented from participating. Data were collected from 158 participants.
8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E4_interactions_tidy_gender$M, digits = 0)`% male, `r printnum(E4_interactions_tidy_gender$F, digits = 0)`
% female, and `r printnum(E4_interactions_tidy_gender$NB, digits = 0)`% non-binary). 
Participants' mean age was `r printnum(E4_interactions_tidy_age$mean)`
(*SD* = `r printnum(E4_interactions_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E4_interactions_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E4_interactions_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 37 minutes (SD = 12.3 minutes). 

# Results {#results-e4}

```{r}
#| label: model-e4
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e4_model <- buildmer(difference ~ size * opacity + 
                       (1 + size * opacity | participant) +
                       (1 + size * opacity | item),
                     data = E4_interactions_tidy)
```

```{r}
#| label: model-e4-lit-dot-pitch
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build additional models (literacy, dot pitch, and training)

e4_lit_model <- add_fixed_effect(e4_model, "literacy", "E4_interactions_tidy")

e4_dot_pitch_model <- add_fixed_effect(e4_model, "dot_pitch", "E4_interactions_tidy")

e4_VT_model <- add_fixed_effect(e4_model, "VT_no_correct", "E4_interactions_tidy")

e4_training_model <- add_fixed_effect(e4_model, "half", "E4_interactions_tidy")

e4_trial_number_model <- add_fixed_effect(e4_model, "trials.thisN", "E4_interactions_tidy")
```

```{r}
#| label: model-e4-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e4_model_cmpr <- comparison(e4_model)
```

```{r}
#| label: anova-results-e4

# do all anovas now

anova_results(e4_model, e4_model_cmpr)

anova_results(e4_lit_model, e4_model)

anova_results(e4_dot_pitch_model, e4_model)

anova_results(e4_VT_model, e4_model)

anova_results(e4_training_model, e4_model)

anova_results(e4_trial_number_model, e4_model)
```

To investigate the effects of combining point opacity and size decay functions on
participants' estimates of correlation, a linear mixed effects model was built
whereby the particular combination of point opacity and size decay function
employed is a predictor for the difference between objective *r* values
for each plot and participants' estimates of *r*. Deviation coding was used
for each of the experimental factors, which allows comparison between means of
*r* estimation error and the grand mean. This model has random intercepts for 
items and participants, and random slopes for participants with regards to the
size decay factor. A likelihood ratio test revealed that the model including
the opacity and size decay conditions as predictors explained significantly
more variance than the null ($\chi^2$(`r in_paren(e4_model.df)`) = 
`r printnum(e4_model.Chisq)`, *p* `r printp(e4_model.p, add_equals = TRUE)`).
There were significant fixed effects of opacity and size decay function, as 
well as a significant interaction between the two. @fig-e4-estimates shows 
the mean errors in correlation estimation for each combination of conditions,
along with 95% confidence intervals.

```{r}
#| label: fig-e4-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 4. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e4_model) == "buildmer") e4_model <- e4_model@model

as_tibble(emmeans(e4_model, pairwise ~ size * opacity)[[1]]) %>%
  mutate("size" = recode(size,
                         "S" = "Typical Orientation Size",
                         "I" = "Inverted Orientation Size"),
          "opacity" = recode(opacity,
                         "S" = "Typical Orientation Opacity",
                         "I" = "Inverted Orientation Opacity")) %>%
  ggplot(aes(x = reorder(size:opacity, emmean), y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  scale_x_discrete(labels = ~ gsub(":", "\n", .x)) +
  theme_ggdist() + 
  labs(y = "Estimated Marginal Mean of Error",
       x = " Size : Opacity") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)
```

```{r}
#| label: tbl-contrasts-e4
#| include: true
#| tbl-cap: Contrasts between different levels of the opacity and size decay factors in experiment 4.

# assign slot, if it hasn't been assigned already

if (class(e4_model) == "buildmer") e4_model <- e4_model@model

# make table df

contrasts_extract(e4_model, "size:opacity") %>%
  mutate('Contrast' = recode(Contrast,
         "S I - I I" = "TO Size x IO Opacity | IO Size x IO Opacity",
         "S I - S S" = "TO Size x IO Opacity | TO Size x TO Opacity",
         "S I - I S" = "TO Size x IO Opacity | IO Size x TO Opacity",
         "I I - S S" = "IO Size x IO Opacity | TO Size x TO Opacity",
         "I I - I S" = "IO Size x IO Opacity | IO Size x TO Opacity",
         "S S - I S" = "TO Size x TO Opacity | IO Size x TO Opacity")) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

The effects found were driven by significant difference between means of correlation
estimation error between all conditions besides that which compares the two
incongruent decay conditions. Statistical testing for contrasts were performed
using the `emmeans` package \cite{lenth_2024}, and are provided in @tbl-contrasts-e4.
Experiments 1, 2, and 3 all featured a comparative baseline condition. In the former
two experiments, this was the full contrast condition (see \chap{chap:adjusting_opacity}),
while in the latter, this was the standard size condition (see \chap{chap:adjusting_size}).
In the current experiment, no baseline condition was used. Owing both to this
and the use of a linear mixed effects model with an interaction term, the use of
Cohen's *d* as a measure of effect size would be inappropriate. In its place,
the amounts of variance in participants' errors in correlation explanation
explained by each fixed effect term and the interaction term is represented as 
semi-partial R^2^ \cite{nakagawa_2013}. These statistics were calculated using
the `r2glmm` package (version 0.1.2) \cite{r2glmm}, and are presented along with model statistics
in @tbl-model-stats-e4.

```{r}
#| label: tbl-model-stats-e4
#| include: true
#| tbl-cap: Significances of fixed effects and the interaction between them. Semi-partial R^2^ for each fixed effect and the interaction term is also displayed in lieu of effect sizes.

make_sig_table(e4_model)

```

Models including participants' graph literacy, their performance on the
point visibility task, the dot pitch of participants'
monitors, and which half a particular correlation judgement took place were
built and compared with the experimental model.
While no significant effects of graph literacy ($\chi^2$(`r in_paren(e4_lit_model.df)`) = 
`r printnum(e4_lit_model.Chisq)`, *p* `r printp(e4_lit_model.p, add_equals = TRUE)`),
performance on the point visibility task ($\chi^2$(`r in_paren(e4_VT_model.df)`) = 
`r printnum(e4_VT_model.Chisq)`, *p* `r printp(e4_VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(e4_dot_pitch_model.df)`) = `r printnum(e4_dot_pitch_model.Chisq)`,
*p* `r printp(e4_dot_pitch_model.p, add_equals = TRUE)`) were found, there was a 
significant effect of training ($\chi^2$(`r in_paren(e4_training_model.df)`) = 
`r printnum(e4_training_model.Chisq)`, *p* `r printp(e4_training_model.p, add_equals = TRUE)`),
with participants rating correlation .01 lower during the second half. This drop
suggests that having more recently viewed the training plots may have increased
participants estimates of correlation. To further analyse this variability,
a model was built including trial number, allowing for the analysis of error.
A significant effect of trial number is also found 
($\chi^2$(`r in_paren(e4_trial_number_model.df)`) = 
`r printnum(e4_trial_number_model.Chisq)`, *p* 
`r printp(e4_trial_number_model.p, add_equals = TRUE)`)
on participants' correlation estimation errors.

```{r}
#| label: fig-e4-trial-number
#| include: true
#| fig-cap: Comparing mean errors in correlation estimation by trial number. Points represent unsigned mean errors for each trial number. The plotted line is the locally estimated smoothed curve, with the ribbon representing standard errors.
#| fig-asp: 0.4

E4_interactions_tidy %>%
  drop_na() %>%
  group_by(trials.thisN) %>%
  summarise(sd = sd(abs(difference)), mean = mean(abs(difference))) %>%
  ggplot(aes(x = trials.thisN, y = mean)) +
  geom_point(alpha = 0.4, shape = 16) +
  geom_smooth(se = T, span = 0.5, colour = "black", size = 0.7,) +
  scale_x_continuous(breaks = seq(0,180, by = 20), limits = c(0, 180)) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_vline(xintercept = 90, linetype = 2) +
  geom_vline(xintercept = 180, linetype = 2) +
  labs(x = "Trial Number",
       y = "Unsigned Mean Error") +
  annotate("text",
           x = 45,
           y = 0.145,
           label = "1st Half") +
    annotate("text",
           x = 135,
           y = 0.145,
           label = "2nd Half") +
  theme_ggdist() +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) 
```

@fig-e4-trial-number shows participants' unsigned mean errors in correlation
estimation against trial number. Variability in error, as represented by the
ribbon, stabilised quickly and remained stable for most the experiment,
only widening again around trial number 170. The simplest explanation for this
is that participants, knowing they were coming to the end of the experiment,
became less vigilant and rushed their judgements more. Regardless of statistical
significance, this effect is not large enough to warrant further investigation,
at least as it pertains to correlation estimation in scatterplots.

```{r}
#| label: fig-estimates-by-r-e4
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by factor and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring the non-linear size decay function. Error bars show standard deviations of estimates.

plot_error_bars_function(E4_interactions_tidy %>%
                           mutate(grouping_var = fct_relevel(condition_abs,
                                                             c("A",
                                                               "B",
                                                               "X",
                                                               "Y"))),
                         "condition_abs",
                         "difference",
                         labels_e4) +
  geom_hline(yintercept = 0, linetype = 2)
```

# Discussion {#discussion-e4}

Hypothesis 1 received full support in this experiment. The combination of 
typical orientation opacity and size decay functions produced the most accurate
estimates of correlation, although this also resulted in a marked over-correction
and consequent overestimation for many values of *r* (see @fig-estimates-by-r-e4).
The second hypothesis also received support; the combination of inverted opacity
and size decay functions produced the least accurate estimates of correlation.
No support was found for the third hypothesis, that there would be a significant
difference in correlation estimates between inverted orientation opacity/typical
orientation size plots and typical orientation opacity/inverted orientation
size plots. There was, however, a significant interaction term present,
providing evidence that the combination of opacity and size decay functions is
not additive in natures.

## Combining Manipulations

## Estimation Precision

## Relative Contributions of Opacity and Srize Decay

## Mechanisms

## Limitations

## Future Work











































# Conclusion {#conclusion-interactions}
