---
title: "belief_change"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false 
---
```{r}
#| label: setup

set.seed(1234) # random seed for number generation

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(ggtext)
library(Matrix)
library(irr)
library(bbplot)
library(geomtextpath)
library(ggh4x)
library(ordinal)
library(ggrain)
library(effectsize)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
options(knitr.kable.NA = "")
```

```{r}
#| label: load-data

exp_5_pre_test_anon <- read_csv("../data/exp_5_pre_test_data.csv") %>%
  mutate("expName" = recode(expName,
                            "beliefs_scatterplots_pretest" = "E5_beliefs_pre_test"))

exp_5_main_A_anon <- read_csv("../data/exp_5_main_data_A.csv") %>%
  mutate("expName" = recode(expName,
                            "atypical_scatterplots_main_test_A" = "E5_beliefs_A"))
  
exp_5_main_T_anon <- read_csv("../data/exp_5_main_data_T.csv") %>%
  mutate("expName" = recode(expName,
                            "atypical_scatterplots_main_test_T" = "E5_beliefs_T"))

source("../shared_functions.R")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymisation, data are provided as-is from 
## pavlovia (survey tool). Wrangling functions *must* be run first to make
## the data sets usable

wrangle_pre_test <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))
  
# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "avg_corr",
           "avg_emot",
           "slider_emotion.response",
           "slider_belief.response",
           "statement",
           "item_no",
           "label",
           "session",
           )) %>%
  filter(item_no < 26) %>%
  full_join(demographics, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# wrangle function for main experiment

wrangle_main_exp <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

# extract time taken information

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))

# extract defensive confidence testing scores

defensive_confidence <- anon_file %>%
  select(contains(c("DC", "participant"))) %>% 
  select(-contains(c("stopped", "started"))) %>%
  rename(DC_1 = q1_slider_DC.response, DC_2 = q2_slider_DC.response,
         DC_3 = q3_slider_DC.response, DC_4 = q4_slider_DC.response,
         DC_5 = q1_slider_DC_2.response, DC_6 = q2_slider_DC_2.response,
         DC_7 = q3_slider_DC_2.response, DC_8 = q4_slider_DC_2.response,
         DC_9 = q1_slider_DC_3.response, DC_10 = q2_slider_DC_3.response,
         DC_11 = q3_slider_DC_3.response, DC_12 = q4_slider_DC_3.response) %>%
  mutate(DC_3 = 6 - DC_3,  # reverse score items 3, 4, 10, 12
         DC_4 = 6 - DC_4,
         DC_10 = 6 - DC_10,
         DC_12 = 6 - DC_12) %>%
  group_by(participant) %>%
  summarise(
    dc_score = sum(DC_1, DC_2, DC_3, DC_4,
                   DC_5, DC_6, DC_7, DC_8,
                   DC_9, DC_10, DC_11, DC_12,
                   na.rm = T)
  )

# extract literacy info

literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(lit_time = literacy.stopped - literacy.started) %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
  select(participant,
         literacy,
         lit_time)

# split unique item no column into number (dataset used) and letter (condition)

anon_file <- anon_file %>%
  separate(starts_with("unique"), into = c("item_no", "condition"),
           sep = "(?<=\\d)(?=\\D)")

# extract pre belief, post belief, and belief_diff

belief <- anon_file %>%
  group_by(participant) %>%
  summarise(
    pre_bel = sum(slider_belief.response, na.rm = T),
    post_bel = sum(slider_belief_post.response, na.rm = T)
  ) %>%
  mutate(belief_diff = post_bel - pre_bel) %>%
  select(participant, belief_diff, pre_bel, post_bel)

emotion <- anon_file %>%
  filter(!is.na(slider_emotion.response)) %>%
  group_by(participant) %>%
  select(participant, slider_emotion.response)

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "item_no",
           "condition",
           "slider.response",
           "trials.thisN"
           )) %>%
  mutate(half = case_when(
    trials.thisN < 23 ~ "first",
    trials.thisN > 23 ~ "second")) %>%
  filter(item_no < 46) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(belief, by = "participant") %>%
  inner_join(emotion, by = "participant") %>%
  inner_join(defensive_confidence, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  mutate(across(matches(c("item_no", "condition")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  pivot_longer(cols = c("pre_bel", "post_bel"),
               values_to = "rating",
               names_to = "rating_time") %>%
  mutate(rating_time = as_factor(rating_time)) %>%
  mutate(across(c("rating"), as.ordered)) %>%                                
  mutate("condition" = recode(condition,
                              "T" = "S")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),                         
           value = ., envir = .GlobalEnv)
}

# use wrangling function on anonymized data files

wrangle_pre_test(exp_5_pre_test_anon)

wrangle_main_exp(exp_5_main_A_anon)

wrangle_main_exp(exp_5_main_T_anon)

# add 75 to each participant number for second part of main exp

E5_beliefs_T_tidy <- E5_beliefs_T_tidy %>%
  mutate(participant = participant + 75)

# main experiment is between participants, so rbind main_exp dfs together

E5_beliefs_main_tidy <- rbind(E5_beliefs_A_tidy,
                              E5_beliefs_T_tidy)

# remove excess dataframes

rm(E5_beliefs_A_tidy, E5_beliefs_T_tidy, exp_5_main_A_anon,
   exp_5_main_T_anon, exp_5_pre_test_anon)

# write functions to extract chapter-specific

# extract gender, age, and graph literacy

extract_age(E5_beliefs_pre_test_tidy)

extract_gender(E5_beliefs_pre_test_tidy)

extract_age(E5_beliefs_main_tidy)

extract_gender(E5_beliefs_main_tidy)

extract_literacy(E5_beliefs_main_tidy)

```

```{r}
#| label: additional-functions

# functions for this chapter included below. Many are only used in this chapter,
# so are not included in the shared_functions.R file.

add_and_to_numbers <- function(numbers) {
  num_str <- as.character(numbers)
  len <- length(num_str)
  
  if (len == 1) {
    return(num_str)
    
  } else if (len == 2) {
    
    return(paste(num_str[1], "and", num_str[2]))
    
  } else {
    return(paste(paste(num_str[1:(len-1)], collapse = ", "), ", and ", num_str[len], sep = ""))
  }
}

# functions to create tables with fixed effects and interactions for models
# first do function for rating time only model

make_sig_table_rating_time <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(1)
  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(Estimate)) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_timepost_bel" = "Rating Time")) %>%
    column_to_rownames(var = "Effect")
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

# next do table for rating time * condition model

make_sig_table_interact <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(3)
  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(abs(Estimate))) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_timepost_bel" = "Rating Time",
                               "conditionS" = "Condition",
                               "rating_timepost_bel:conditionS" = "Rating Time x Condition")) %>%
    column_to_rownames(var = "Effect")
 
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```

```{r}
#| label: extracting-other-dfs

# extract time taken data into separate dfs

time_taken_e5a <- distinct(E5_beliefs_pre_test_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE)) 

time_taken_e5 <- distinct(E5_beliefs_main_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE))

# extract literacy data into separate df

literacy_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy),
            mean_time = mean(lit_time), sd_time = sd(lit_time))

# extract defensive confidence data into separate df

defensive_confidence_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(dc_score), sd = sd(dc_score))

# extract topic emotionality data into separate df

topic_emotionality_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(slider_emotion.response),
            sd = sd(slider_emotion.response))
```

# Abstract {#abstract-beliefs}

\chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and \chap{chap:interactions_opacity_size}
show through four experiments that point opacity and size changes can have powerful
effects on participants' estimates of correlation in positively correlated
scatterplots. In \chap{chap:adjusting_opacity}, global and spatially-dependent 
adjustments in point opacity were employed, and a small, but statistically 
significant level of correction for a historic underestimation bias of positive 
correlation was found. Spatially-dependent adjustment of point size, in which
size is reduced as a function of residual error, was found in \chap{chap:adjusting_size}
to produce much stronger effects on estimates of correlation; the non-linear decay
function used in that experiment produced higher levels of correction and resulted
in highly accurate correlation estimates. In \chap{chap:interactions_opacity_size},
these point opacity and size manipulations were combined. Their combination was
found to produce stronger effects than would be expected if they were linearly
additive. While my efforts at correcting for the underestimation bias were successful,
my work has not yet attempted to investigate whether any of the techniques developed
in this thesis may be used to change people's cognitions about data. Therefore, for my final 
experimental chapter, I show that scatterplot manipulations that are able to correct for a historic
correlation underestimation bias are also able to induce stronger levels of belief
change in viewers compared to conventional plots showing identical data. In a pre-study
and main experiment, I provide evidence that adjusting visual features in scatterplots
can go beyond simple perceptual effects to influence beliefs about information
from trusted news sources.

# Introduction {#intro-beliefs}

Research consistently finds that participants underestimate the correlation 
displayed in positively correlated scatterplots \cite{strahan_1978,bobko_1979, cleveland_1982,
collyer_1990, lane_1985, lauer_1989, meyer_1992, rensink_2017}. This underestimation 
is particularly pronounced for Pearson's *r* values of 0.2 < *r* < 0.6, and has
been replicated extensively in the current thesis. If scatterplots were solely
used for communication between experts, then the presence of this bias would
not be especially problematic; those trained in statistics and data visualisation
are more likely to be aware of, and make allowances for, their biases. Unfortunately,
this is not the case; lay people are expected to be able to use and interpret
data visualisations on an almost daily basis. It is therefore the duty of those who design visualisations
to design with the naive, inexperienced viewer in mind. Doing so requires us to 
understand *how* visualisations work, and to gain an appreciation for the hidden processes
that allow pictorial representations to convey more than words and numbers alone
ever could.

In this thesis, \chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and
\chap{chap:interactions_opacity_size} demonstrate how changing the opacities and
sizes on points on scatterplots is able to significantly alter participants' 
estimates of correlation in positively correlated scatterplots. Substantial progress
has been made in correcting for the underestimation bias, however these efforts
have only provided evidence about perceptual effects using a simple direct
estimation paradigm. While successful, this work has not yet investigated whether,
and to what extent, these techniques can influence cognition in the context of
real-word data visualisations and the relatedness between variables.

Visualisation is a powerful tool. After all, if numerical data were sufficient
for understanding, there would be no need to visualise beyond aesthetic 
preference. Pattern recognition, attention, and familiarity are aspects of human
perception and cognition that can be exploited by visualisation designers to 
facilitate more efficient, enjoyable, and effective communication \cite{franconeri_2021}.
This, however, is a double-edged sword; poor design, be it malevolent
or misguided, can cause distrust, confusion, and misunderstanding amongst viewers.
It is for these reasons that belief change in scatterplots as a consequence of 
alternative designs is the next logical research direction for this project.
Scatterplots, like many other data visualisations, have been submitted as 
evidence in court cases \cite{bobko_1979}, and play key roles in organisational 
decision-making, including in healthcare \cite{poly_2019}. It is reasonable to 
assume that data visualisations are used to make decisions that result in positive
or negative outcomes with regard to health and policy more generally, especially 
given findings that in certain contexts, they are more persuasive than textual 
information \cite{pandey_2014}. Studying the potential for new designs to alter
beliefs about relatedness facilitates better visualisation techniques,
but also facilitates understanding about how these designs might be used by
malevolent actors with a view to inoculating those who engage with them.
To this end, I present a two-experiment study. First, crowdsourcing is used to
select part of our experimental stimuli. The propensity for previously established
alternative scatterplot designs to alter beliefs about relatedness is then tested,
taking into account the emotional content of the statement and the graph literacy
and defensive confidence of participants.

# Related Work {#related-work-beliefs}

## Scatterplots: Developments in This Thesis

```{r}
#| label: fig-previous-manipulations
#| include: true
#| fig-cap: "Top row: Examples of scatterplot manipulations from previous work using an \\textit{r} value of 0.6. Bottom row: the corresponding correlation estimation behaviour across values of \\textit{r} between 0.2 and 0.99. The dashed diagonal line represents hypothetically accurate estimation, while the solid line is what is observed when participants are asked to estimate correlation."

# dataframe containing values from previous work and the current is included
# in the data folder 
# set facet orders

facet_order <- c("standard_plot", "opacity_manipulated", "size_manipulated", "additive_manipulation")

# make data frame of behaviours observed with standard plot

standard_alone <- read_csv("../data/exp_1_to_4_combined.csv") %>%
    drop_na() %>%
    filter(factor == "standard_plot") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>%
    arrange(my_rs) %>%
    mutate(group = ceiling(row_number() / 2)) %>%
    group_by(group) %>%
    summarise(
      factor = first(factor),
      my_rs = mean(my_rs),
      sd = mean(sd),
      mean = mean(mean)
    ) %>%
  ungroup() %>%
  select(-group) %>%
  mutate(slider.response = (log(1-0.88*my_rs)/log(1-0.88)))

all_exp_df <- read_csv("../data/exp_1_to_4_combined.csv") %>%
    drop_na() %>%
    filter(factor != "standard_plot") %>%
    group_by(factor, my_rs) %>%
    mutate(factor = factor(factor, levels = facet_order))

plotting_df <- rbind(all_exp_df, standard_alone)

plot_est_prev_no_y <- function(filter) {
  
  plot <- plotting_df %>%
    filter(factor == filter) %>%
    ggplot(aes(x = my_rs, y = slider.response)) + 
    theme_ggdist() +
    theme(scale_y_continuous(breaks = seq(-0.4,1, 0.2)),
        axis.text = element_text(size = 7),
        axis.title.x = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.margin = unit(c(0,0,0.2,0), "cm")) +
    geom_abline(slope = 1, intercept = 0, linetype = 3) +
    labs(x = "Objective *r") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    ylim(0.1,1)
  
  return(plot)
}

example_plots_prev <- ggarrange(
                      plot_example_function(slopes,
                                            "Standard Scatterplot",
                                            1,
                                            0.2,
                                            7),
                      plot_example_function(slopes,
                                            "Opacity Manipulation",
                                            (1-slopes$slope_0.25),
                                            0.2,
                                            title_size = 7),
                      plot_example_function(slopes,
                                            "Size Manipulation",
                                            1,
                                            (1-slopes$slope_0.25),
                                            title_size = 7),
                      plot_example_function(slopes,
                                            "Both Manipulations",
                                            (1-slopes$slope_0.25),
                                            (1-slopes$slope_0.25),
                                            title_size = 7),
                      nrow = 1)

est_prev <- ggarrange(
  plot_est_prev_no_y("standard_plot"),
  plot_est_prev_no_y("opacity_manipulated"),
  plot_est_prev_no_y("size_manipulated"),
  plot_est_prev_no_y("additive_manipulation"), nrow = 1
)

p <- ggarrange(example_plots_prev, est_prev, nrow = 2) + annotate("line", x = 0, end = 1, y = 0.5, yend = 0)

x_label <- text_grob(expression("Objective " * italic("r") * " value"))

y_label <- text_grob(expression("Subjective " * italic("r") * " value"),
                     rot = 90, hjust = 0.9)

rm(example_plots_prev, plot_prev_est_no_y,    # trying to prevent env clogging
   plot_prev_est, plotting_df,                # up when working on thesis
   all_exp_df, standard_alone,
   facet_order)

annotate_figure(p,
                bottom = x_label,
                left = y_label)
```

In contradiction to previous work \cite{rensink_2014, rensink_2017}, this thesis
has found clear and powerful effects of systematically changing the opacities
and sizes of scatterplot points on participants' estimates of correlation in
positively correlated scatterplots. While \chap{chap:adjusting_size}, in which
point sizes were lowered as a function of residual distance using equation 1, provided
the best level of correction seen so far, \chap{chap:interactions_opacity_size}
featured the most dramatic level of correction. In that that particular condition,
both point opacity and size were lowered as a function of residual distance
using equation 1:

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

@ fig-previous-manipulations contains a summary of the most promising point opacity 
and size manipulations from the previous three chapters in this thesis, along with
their effects on performance on a correlation estimation task.

## Perception & Cognition in Data Visualisation



# Open Research {#open-research-chap7}

# Pre-Study: Investigating Beliefs About Relatedness Statements {#beliefs-e5a}

## Introduction {#intro-e5a}

## Method {#method-e5a}

### Design {#design-e5a}

Each participant saw all survey items (see Appendix A), along with the six
attention check items, in a fully randomised order. All experimental code, 
materials, and instructions are hosted on GitLab [^1].

[^1]: https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest

### Procedure {#procedure-e5a}

Participants viewed the PIS and were asked to provide through key presses
in response to consent statements. They were prompted to provide their
age in a free text box and their gender identity. Participants were told
that they would be asked to read statements about the relatedness between a 
pair of variables, after which they would be asked to answer some questions.
To familiarise themselves with the sliders used to collect responses, they were
asked to complete a practice trial in response to the statement: "As participation
in online experiments increases, society becomes happier". Following each
statement, a pair of Likert scales were presented labelled "Statement Emotionality"
and "Strength of Relatedness; these scales were identical to those used by 
myself a co-author in Section \ref{beliefs-e5a}.

### Participants {#participants-e5a}

100 participants were recruited using the Prolific platform \cite{prolific}. English fluency and
UK residency were required for participation, as the main experiment relied on familiarity
with data visualisations from a popular British news source. In addition to 25
experimental items, six attention check items were included that instructed participants
to ignore the statement and provide specific answers to the Likert scale sliders.
No participants failed more than 2 out of 6 attention
check items, and therefore data from all 100 were included in the full analysis
(`r printnum(E5_beliefs_pre_test_tidy_gender$Male, digits = 0)` male and 
`r printnum(E5_beliefs_pre_test_tidy_gender$Female, digits = 0)` female). 
Participants' mean age was `r printnum(E5_beliefs_pre_test_tidy_age$mean, digits = 1)` (*SD* = 
`r printnum(E5_beliefs_pre_test_tidy_age$sd, digits = 1)`). The average time 
taken to complete the survey was `r printnum(time_taken_e5$mean, digits = 1)`
minutes (*SD* = `r printnum(time_taken_e5a$sd, digits = 1)` minutes).

## Results {#results-e5a}

```{r}
#| label: kappa-pre-test

# Calculate Fleiss' Kappa for 100 raters on emotional valence

pre_test_emot <- E5_beliefs_pre_test_tidy %>%
  select(c("participant", "slider_emotion.response", "item_no"))

emot_matrix <- xtabs(slider_emotion.response ~ item_no + participant, data = pre_test_emot)

emot_matrix <- as.matrix(emot_matrix)

emot_kappa <- kappam.fleiss(emot_matrix)

# do the same for strength of belief

pre_test_belief <- E5_beliefs_pre_test_tidy %>%
  select(c("participant", "slider_belief.response", "item_no"))

belief_matrix <- xtabs(slider_belief.response ~ item_no + participant, data = pre_test_belief)

belief_matrix <- as.matrix(belief_matrix)

belief_kappa <- kappam.fleiss(belief_matrix)

# calculate mean ratings and standard deviations for each statement

agreement_df_emot <- E5_beliefs_pre_test_tidy %>%
  group_by(item_no) %>%
  summarise(mean_emot = mean(slider_emotion.response),
            std_dev_emot = sd(slider_emotion.response)) %>%
  arrange(std_dev_emot)
agreement_df_belief <- E5_beliefs_pre_test_tidy %>%
  group_by(item_no) %>%
  summarise(mean_belief = mean(slider_belief.response),
            std_dev_belief = sd(slider_belief.response)) %>%
  arrange(std_dev_belief)

ratings_df <- full_join(agreement_df_belief, agreement_df_emot, by = "item_no")

# calculate average topic emotionality ratings

avg_emot <- ratings_df %>%
  filter(between(mean_emot, 3, 5))

# calculate consensus by summing standard deviations

consensus_df <- avg_emot %>%
  mutate(consensus = std_dev_belief + std_dev_emot) %>% 
  arrange(consensus) %>%
  slice_head(n =  2)
```

As before, the `irr` package \cite{irr} to measure interrater agreement on 
statement emotional valence and strength of relatedness for the 25 experimental
items. This analysis revealed that participants agreed above chance on statement
emotional valence ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`,
*p* `r printp(emot_kappa$p.value, add_equals = TRUE)`)
and strength of relatedness ($\kappa$ = `r printnum(belief_kappa$value, digits = 2)`,
*p* `r printp(belief_kappa$p.value, add_equals = TRUE)`).

## Selecting Statements for the Main Experiment {#selecting-statements-e5a}

```{r}
#| label: tbl-candidate-statements
#| include: true
#| tbl-cap: Statements with neutral average emotional valence ratings.

candidates <- avg_emot$item_no

candidate_statements <- read_csv("../data/experimental_dataframes/e5a_exp_dataframe.csv") %>%
  filter(item_no %in% candidates) %>%
  select(c("item_no","statement")) %>%
  rename("Statement" = "statement",
         "Item" = "item_no")

kbl(candidate_statements, booktabs = TRUE, format = "latex")
```

Statements represents neutral emotional valence were selected to control for the
potential effects of statement emotionality in the main experiment. Statements
with average emotionality ratings between 3 and 5 are statements 
`r printnum(add_and_to_numbers(avg_emot$item_no), digits = 0)`, which can be seen
in @tbl-candidate-statements. To ascertain which statements represent the
greatest consensus, standard deviations of ratings for statement emotional valence
and strength of relatedness were summed. Due to concerns about experimental
power, and in line with evidence that propensity for belief change is highest
when prior beliefs are not strongly held \cite{xiong_2023, markant_2023},
at this point the decision was made to test only the statement corresponding to weak beliefs
about the strength of relatedness between the variables in question. Statement number
`r printnum(add_and_to_numbers(consensus_df$item_no[2]), digits = 0)` was therefore
selected: "Higher consumption of spicy foods is associated with a lower risk of 
certain types of cancer", however the wording was modified such that the variables
(food consumption and cancer risk) are positively correlated. While the work
carried out in \chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and
\chap{chap:interactions_opacity_size} show that opacity and size adjustments
in scatterplots can affect estimates of positive correlation, no work regarding
the effects of these manipulations in negatively correlated scatterplots has 
been completed.

## Discussion {#discussion-e5a}

Fleiss' Kappa values for interrater agreement on both statement emotional valence
and strength of correlation scales are low ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`
and $\kappa$ = `r printnum(belief_kappa$value, digits = 2)` respectively),
however do exceed that which would be expected by chance. In light of this, 
decisions regarding which statement to use are not based 
on the values of Fleiss' Kappa observed, but rather on the standard deviations of 
ratings across all raters. Statement emotionality and strength of relatedness are
tested with participants in the main study and included as fixed effects as part
of the analyses.

# Main Experiment: Alternative Scatterplot Designs and Beliefs about Relatedness {#beliefs-main-e5}

The statement selected exhibits the lowest average level of belief about strength
of relatedness and the 2^nd^ highest level of consensus. Modified for directionality,
the statement reads:

```{=tex}
\begin{quotation}
``Higher consumption of plain (non-spicy) foods is associated with a higher risk of certain types of cancer.''
\end{quotation}
```

To maximise the likelihood of finding an effect of viewing alternative scatterplots,
the stimuli were designed based on a popular British news source and the data
were falsely credited as being supplied by the British National Health Service (NHS).
Participants were told that the news source had requested their identity be obscured.
They were debriefed that this was not the case, and in fact the data were false,
at the end of the experiment. Based on evidence that beliefs can change after
viewing visualisations \cite{karduni_2020, markant_2023}, and that scatterplots
employing point opacity and size manipulations described in \ref{related-work-beliefs}
are able to affect perceptual estimates, the following hypotheses were made:

## Hypotheses {#hypotheses-e5}

  - H1: there will be a significant difference between ratings of strength of
  relatedness made before and after participants viewed scatterplots in either
  the standard or alternative conditions.
  - H2: this difference will be greatest when participants are exposed
  to scatterplots in the alternative scatterplot condition.
  
Exploratory investigations taking into account participants' scores on a defensive
confidence test, their scores on a graph literacy test, and each participant's rating
of the emotional valence of the correlative statement used. Analysis including each of
these factors can be found in Section \ref{add-analyses-e5}, and their inclusion is
justified below.

### Defensive Confidence {#def-con-e5}

In line with evidence that those who are more confident in their ability to
defend their own positions are more susceptible to having those positions changed
\cite{albarracin_2004}, participant's defensive confidence was measured using 
Albarracín and Mitchell's \cite{albarracin_2004} 12-item scale. This scale is 
replicated from previous work in Appendix B, and has 
been utilised more recently \cite{markant_2023} to explore the potential for attitude
change specifically with regard to correlations in scatterplots. Participants
provide answers to the 12 scale items using a 5-point Likert scale anchored at points 1
(*not at all characteristic of me*) and 5 (*extremely characteristic of me*), with 
all other points being unlabelled.

### Graph Literacy {#graph-literacy-e5}

No effect of graph literacy was found in experiments 1 to 4 (see \chap{chap:adjusting_opacity},
\chap{chap:adjusting_size}, and \chap{chap:interactions_opacity_size}). Despite 
this, the scale was included here due to the higher predicted cognitive load
of the current tasks, along with evidence that graph literacy may affect performance on more cognitively
demanding visualisation tasks \cite{canham_2010, okan_2012}. Additionally, the graph
literacy test used \cite{garcia_2016} is extremely short; in the present study,
this took participants an average of `r printnum(literacy_e5$mean_time, digits = 0)`
seconds (*SD* = `r printnum(literacy_e5$sd_time, digits = 0)` seconds).

### Emotionality

The emotional content of a visualisation and the emotional state of a participant
may have cognitive and perceptual effects on performance in visualisation tasks
\cite{phelps_2006, harrison_2013, thoresen_2016}; this was the primary
motivation behind performing the pre-study. Nevertheless, it is not guaranteed
that each participant considers the emotional content of the correlative
statement to be the same. To account for these individual differences, ratings
of emotional valence are also collected during the main study.

## Method {#method-e5}

### Stimuli {#stimuli-e5}

```{r}
#| label: fig-exp5-examples-chap7
#| include: true
#| fig-cap: Examples of the experimental stimuli for experiment 5 using an \textit{r} value of 0.6. Group A saw the alternative scatterplot presented on the right, while group B saw the typical design on the left. 
#| fig-asp: 0.7

ggarrange(example_plot_function_exp5(slopes_exp5,
                                     0.6,
                                     typical,
                                     standard_alpha,
                                     bbc_style()),
          example_plot_function_exp5(slopes_exp5,
                                     0.6,
                                     slopeI,
                                     slopeI_floored,
                                     bbc_style()),
          nrow = 1) +
  annotate(geom = "text",
           label = "Typical Scatterplot",
           x = 0.25,
           y = 0.1,
           size = 3) +
  annotate(geom = "text",
           label = "Atypical Scatterplot",
           x = 0.75,
           y = 0.1,
           size = 3)
```

After selecting a correlative statement describing a weak relationship and with
a high level of consensus between participants, the `ggplot2` package (version 3.5.1)
\cite{wickham_2016} was used to create the stimuli for the main experiment. As
the statement was rated as describing a low level of relatedness, scatterplots
describing a strong relationship (0.6 < *r* < 0.99) were used with the intent
of inducing belief change. Plots in the alternative scatterplot condition were
created using a combination of non-linear opacity and size decay, as this
particular condition was shown to bias correlation estimates to a greater
degree than either point opacity or size decay alone (see experiment 4 in 
\chap{chap:interactions_opacity_size}). 45 *r* values uniformly distributed
between 0.6 and 0.99 were used to create 45 scatterplots for each condition.
Examples of stimuli using an *r* value of 0.6 for both the
standard and alternative scatterplot conditions can be seen in @fig-exp5-examples-chap7.

### Design {#design-e5}

Unlike all previous experiments, a between-participants design was employed
here. Each participant was randomly assigned either to group A, in which they
viewed standard scatterplots, or group B, in which they viewed alternative scatterplots
designed deliberately to elicit higher levels of belief change.
Participants saw all 45 experimental items for their group, along
with 4 attention check items, in a fully randomised order. The dependent variable 
was the level of belief change induced by viewing the scatterplot visualisations,
so participants were tested on how strongly related they believed the variables 
described by the correlative statement were both **before** and **after** viewing the
experimental items. All experimental code, materials, and instructions are 
hosted on GitLab as two separate experiments [^2] [^3].

[^2]: https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t
[^3]: https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a

### Procedure {#procedure-e5}

Participants viewed the PIS and provided consent through key presses in response
to displayed consent statements. Participants were then asked to provide
their age and gender identity. Following this, participants completed
the 5-item Subjective Graph Literacy scale as in previous experiments
\cite{garcia_2016}, and Albarracín and Mitchell's \cite{albarracin_2004} 12-item defensive confidence
scale . To give legitimacy to the data visualisations with hope of
maximising any potential belief change, participants were told that the graphs
they would see were taken from a well-known British news source, but that the
identity of this source had been obscured at their request. To promote 
engagement with the visualisations, participants were instructed to use a slider
to estimate the correlation displayed in each scatterplot; no hypotheses were made 
based on these data, and in the published paper this chapter was based on, they
were not analysed. For completeness sake, I analyse this data in Section \ref{add-analyses-e5}
here. Following instructions, which included textual descriptions of scatterplots
and Pearson's *r*, participants were given two practice trials; these trials
took the form of a typical "standard scatterplot" trial from experiment 1.
Participants were then asked to indicate their beliefs about emotional valence
and strength of relatedness described in the chosen correlative statement; these
data were captured using Likert scales identical to those described previously.
After completing 45 experimental trials, participants were then asked again,
using the same Likert scales, to indicate their beliefs about emotional valence
and strength of relatedness. Interspersed among the experimental items were 
four attention check trials which explicitly asked participants to set the slider
to 0 or 1.

### Participants {#participants-e5}

150 participants were recruited using the Prolific platform \cite{prolific}.
Normal to corrected-to-normal vision and English fluency were required. As in
the pre-study, UK residency was required of participants, as the experiment relied
on familiarity with the style of a British news source. Participants who took part
in the pre-study, or in any of the experiments described in \chap{chap:adjusting_opacity},
\chap{chap:adjusting_size}, or \chap{chap:interactions_opacity_size} were prevented
from completing this experiment. Data were collected from 77 participants in 
each condition. 2 participants failed more than 2 out of 4 attention check
questions for each condition, meaning their data were excluded per
pre-registration stipulations. Data from the remaining 150 participants were
included in the full analysis (`r printnum(E5_beliefs_main_tidy_gender$Male, digits = 2)` male,
`r printnum(E5_beliefs_main_tidy_gender$Female, digits = 2)` female, and 
`r printnum(E5_beliefs_main_tidy_gender$'Non-binary', digits = 2)` non-binary).
Participants' mean age was `r printnum(E5_beliefs_main_tidy_age$mean, digits = 1)`
(*SD* = `r printnum(E5_beliefs_main_tidy_age$sd, digits = 1)`). Participants' 
mean graph literacy score was `r printnum(E5_beliefs_main_tidy_graph_literacy$mean, digits = 1)`
(*SD* = `r printnum(E5_beliefs_main_tidy_graph_literacy$sd, digits = 1)`)
out of 30, their mean defensive confidence score was `r printnum(defensive_confidence_e5$mean, digits = 0)`
(*SD* = `r printnum(defensive_confidence_e5$sd, digits = 1)`) out of 60, and their mean rating of 
statement emotional valence was `r printnum(topic_emotionality_e5$mean, digits = 1)`
(*SD* = `r printnum(topic_emotionality_e5$sd, digits = 1)`) on a 7-point Likert scale.
On average, participants took `r printnum(time_taken_e5$mean, digits = 1)`
minutes to complete the experiment (*SD* = `r printnum(time_taken_e5$sd)`).

## Results {#results-e5}

### Additional Analyses {#add-analyses-e5}

## Discussion {#discussion-e5}

### Graph Literacy, Defensive Confidence, and Statement Emotional Valence

# General Discussion {#general-discussion-e5}

# Limitations {#limitations-e5}

# Future Work {#future-work-e5}
