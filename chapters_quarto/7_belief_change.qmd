---
title: "belief_change"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false 
---
```{r}
#| label: setup

set.seed(1234) # random seed for number generation

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(ggtext)
library(Matrix)
library(irr)
library(bbplot)
library(geomtextpath)
library(ggh4x)
library(ordinal)
library(ggrain)
library(effectsize)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
options(knitr.kable.NA = "")
```

```{r}
#| label: load-data

exp_5_pre_test_anon <- read_csv("../data/exp_5_pre_test_data.csv") %>%
  mutate("expName" = recode(expName,
                            "beliefs_scatterplots_pretest" = "E5_beliefs_pre_test"))

exp_5_main_A_anon <- read_csv("../data/exp_5_main_data_A.csv") %>%
  mutate("expName" = recode(expName,
                            "atypical_scatterplots_main_test_A" = "E5_beliefs_A"))
  
exp_5_main_T_anon <- read_csv("../data/exp_5_main_data_T.csv") %>%
  mutate("expName" = recode(expName,
                            "atypical_scatterplots_main_test_T" = "E5_beliefs_T"))

source("../shared_functions.R")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymisation, data are provided as-is from 
## pavlovia (survey tool). Wrangling functions *must* be run first to make
## the data sets usable

wrangle_pre_test <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))
  
# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "avg_corr",
           "avg_emot",
           "slider_emotion.response",
           "slider_belief.response",
           "statement",
           "item_no",
           "label",
           "session",
           )) %>%
  filter(item_no < 26) %>%
  full_join(demographics, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# wrangle function for main experiment

wrangle_main_exp <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

# extract time taken information

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))

# extract defensive confidence testing scores

defensive_confidence <- anon_file %>%
  select(contains(c("DC", "participant"))) %>% 
  select(-contains(c("stopped", "started"))) %>%
  rename(DC_1 = q1_slider_DC.response, DC_2 = q2_slider_DC.response,
         DC_3 = q3_slider_DC.response, DC_4 = q4_slider_DC.response,
         DC_5 = q1_slider_DC_2.response, DC_6 = q2_slider_DC_2.response,
         DC_7 = q3_slider_DC_2.response, DC_8 = q4_slider_DC_2.response,
         DC_9 = q1_slider_DC_3.response, DC_10 = q2_slider_DC_3.response,
         DC_11 = q3_slider_DC_3.response, DC_12 = q4_slider_DC_3.response) %>%
  mutate(DC_3 = 6 - DC_3,  # reverse score items 3, 4, 10, 12
         DC_4 = 6 - DC_4,
         DC_10 = 6 - DC_10,
         DC_12 = 6 - DC_12) %>%
  group_by(participant) %>%
  summarise(
    dc_score = sum(DC_1, DC_2, DC_3, DC_4,
                   DC_5, DC_6, DC_7, DC_8,
                   DC_9, DC_10, DC_11, DC_12,
                   na.rm = T)
  )

# extract literacy info

literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(lit_time = literacy.stopped - literacy.started) %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
  select(participant,
         literacy,
         lit_time)

# split unique item no column into number (dataset used) and letter (condition)

anon_file <- anon_file %>%
  separate(starts_with("unique"), into = c("item_no", "condition"),
           sep = "(?<=\\d)(?=\\D)")

# extract pre belief, post belief, and belief_diff

belief <- anon_file %>%
  group_by(participant) %>%
  summarise(
    pre_bel = sum(slider_belief.response, na.rm = T),
    post_bel = sum(slider_belief_post.response, na.rm = T)
  ) %>%
  mutate(belief_diff = post_bel - pre_bel) %>%
  select(participant, belief_diff, pre_bel, post_bel)

emotion <- anon_file %>%
  filter(!is.na(slider_emotion.response)) %>%
  group_by(participant) %>%
  select(participant, slider_emotion.response)

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "item_no",
           "condition",
           "slider.response",
           "trials.thisN"
           )) %>%
  mutate(half = case_when(
    trials.thisN < 23 ~ "first",
    trials.thisN > 23 ~ "second")) %>%
  filter(item_no < 46) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(belief, by = "participant") %>%
  inner_join(emotion, by = "participant") %>%
  inner_join(defensive_confidence, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  mutate(across(matches(c("item_no", "condition")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  pivot_longer(cols = c("pre_bel", "post_bel"),
               values_to = "rating",
               names_to = "rating_time") %>%
  mutate(rating_time = as_factor(rating_time)) %>%
  mutate(across(c("rating"), as.ordered)) %>%                                
  mutate("condition" = recode(condition,
                              "T" = "S")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),                         
           value = ., envir = .GlobalEnv)
}

# use wrangling function on anonymized data files

wrangle_pre_test(exp_5_pre_test_anon)

wrangle_main_exp(exp_5_main_A_anon)

wrangle_main_exp(exp_5_main_T_anon)

# add 75 to each participant number for second part of main exp

E5_beliefs_T_tidy <- E5_beliefs_T_tidy %>%
  mutate(participant = participant + 75)

# main experiment is between participants, so rbind main_exp dfs together

E5_beliefs_main_tidy <- rbind(E5_beliefs_A_tidy,
                              E5_beliefs_T_tidy)

# remove excess dataframes

rm(E5_beliefs_A_tidy, E5_beliefs_T_tidy, exp_5_main_A_anon,
   exp_5_main_T_anon, exp_5_pre_test_anon)

# write functions to extract chapter-specific

# extract gender, age, and graph literacy

extract_age(E5_beliefs_pre_test_tidy)

extract_gender(E5_beliefs_pre_test_tidy)

extract_age(E5_beliefs_main_tidy)

extract_gender(E5_beliefs_main_tidy)

extract_literacy(E5_beliefs_main_tidy)

```

```{r}
#| label: additional-functions

# functions for this chapter included below. Many are only used in this chapter,
# so are not included in the shared_functions.R file.

add_and_to_numbers <- function(numbers) {
  num_str <- as.character(numbers)
  len <- length(num_str)
  
  if (len == 1) {
    return(num_str)
    
  } else if (len == 2) {
    
    return(paste(num_str[1], "and", num_str[2]))
    
  } else {
    return(paste(paste(num_str[1:(len-1)], collapse = ", "), ", and ", num_str[len], sep = ""))
  }
}

# functions to create tables with fixed effects and interactions for models
# first do function for rating time only model

make_sig_table_rating_time <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(1)
  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(Estimate)) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_timepost_bel" = "Rating Time")) %>%
    column_to_rownames(var = "Effect")
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

# next do table for rating time * condition model

make_sig_table_interact <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(3)
  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(abs(Estimate))) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_timepost_bel" = "Rating Time",
                               "conditionS" = "Condition",
                               "rating_timepost_bel:conditionS" = "Rating Time x Condition")) %>%
    column_to_rownames(var = "Effect")
 
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```

```{r}
#| label: extracting-other-dfs

# extract time taken data into separate dfs

time_taken_e5a <- distinct(E5_beliefs_pre_test_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE)) 

time_taken_e5 <- distinct(E5_beliefs_main_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE))

# extract literacy data into separate df

literacy_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy),
            mean_time = mean(lit_time), sd_time = sd(lit_time))

# extract defensive confidence data into separate df

defensive_confidence_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(dc_score), sd = sd(dc_score))

# extract topic emotionality data into separate df

topic_emotionality_e5 <- distinct(E5_beliefs_main_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(slider_emotion.response),
            sd = sd(slider_emotion.response))
```

# Abstract {#abstract-beliefs}

\chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and \chap{chap:interactions_opacity_size}
show through four experiments that point opacity and size changes can have powerful
effects on participants' estimates of correlation in positively correlated
scatterplots. In \chap{chap:adjusting_opacity}, global and spatially-dependent 
adjustments in point opacity were employed, and a small, but statistically 
significant level of correction for a historic underestimation bias of positive 
correlation was found. Spatially-dependent adjustment of point size, in which
size is reduced as a function of residual error, was found in \chap{chap:adjusting_size}
to produce much stronger effects on estimates of correlation; the non-linear decay
function used in that experiment produced higher levels of correction and resulted
in highly accurate correlation estimates. In \chap{chap:interactions_opacity_size},
these point opacity and size manipulations were combined. Their combination was
found to produce stronger effects than would be expected if they were linearly
additive. While my efforts at correcting for the underestimation bias were successful,
my work has not yet attempted to investigate whether any of the techniques developed
in this thesis may be used to change people's cognitions about data. Therefore, for my final 
experimental chapter, I show that scatterplot manipulations that are able to correct for a historic
correlation underestimation bias are also able to induce stronger levels of belief
change in viewers compared to conventional plots showing identical data. In a pre-study
and main experiment, I provide evidence that adjusting visual features in scatterplots
can go beyond simple perceptual effects to influence beliefs about information
from trusted news sources.

# Introduction {#intro-beliefs}

# Related Work {#related-work-beliefs}

## From Perception to Cognition

## From Cognition to Belief

# Open Research {#open-research-chap7}

# Pre-Study: Investigating Beliefs About Relatedness Statements {#beliefs-e5a}

## Introduction {#intro-e5a}

## Method {#method-e5a}

### Design {#design-e5a}

Each participant saw all survey items (see Appendix A), along with the six
attention check items, in a fully randomised order. All experimental code, 
materials, and instructions are hosted on GitLab [^1].

[^1]: https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest

### Procedure {#procedure-e5a}

Participants viewed the PIS and were asked to provide through key presses
in response to consent statements. They were prompted to provide their
age in a free text box and their gender identity. Participants were told
that they would be asked to read statements about the relatedness between a 
pair of variables, after which they would be asked to answer some questions.
To familiarise themselves with the sliders used to collect responses, they were
asked to complete a practice trial in response to the statement: "As participation
in online experiments increases, society becomes happier". Following each
statement, a pair of Likert scales were presented labelled "Statement Emotionality"
and "Strength of Relatedness; these scales were identical to those used by 
myself a co-author in Section \ref{beliefs-e5a}.

### Participants {#participants-e5a}

100 participants were recruited using the Prolific platform \cite{prolific}. English fluency and
UK residency were required for participation, as the main experiment relied on familiarity
with data visualisations from a popular British news source. In addition to 25
experimental items, six attention check items were included that instructed participants
to ignore the statement and provide specific answers to the Likert scale sliders.
No participants failed more than 2 out of 6 attention
check items, and therefore data from all 100 were included in the full analysis
(`r printnum(E5_beliefs_pre_test_tidy_gender$Male, digits = 0)` male and 
`r printnum(E5_beliefs_pre_test_tidy_gender$Female, digits = 0)` female). 
Participants' mean age was `r printnum(E5_beliefs_pre_test_tidy_age$mean, digits = 1)` (*SD* = 
`r printnum(E5_beliefs_pre_test_tidy_age$sd, digits = 1)`). The average time 
taken to complete the survey was `r printnum(time_taken_e5$mean, digits = 1)`
minutes (*SD* = `r printnum(time_taken_e5a$sd, digits = 1)` minutes).

## Results {#results-e5a}

```{r}
#| label: kappa-pre-test

# Calculate Fleiss' Kappa for 100 raters on emotional valence

pre_test_emot <- E5_beliefs_pre_test_tidy %>%
  select(c("participant", "slider_emotion.response", "item_no"))

emot_matrix <- xtabs(slider_emotion.response ~ item_no + participant, data = pre_test_emot)

emot_matrix <- as.matrix(emot_matrix)

emot_kappa <- kappam.fleiss(emot_matrix)

# do the same for strength of belief

pre_test_belief <- E5_beliefs_pre_test_tidy %>%
  select(c("participant", "slider_belief.response", "item_no"))

belief_matrix <- xtabs(slider_belief.response ~ item_no + participant, data = pre_test_belief)

belief_matrix <- as.matrix(belief_matrix)

belief_kappa <- kappam.fleiss(belief_matrix)

# calculate mean ratings and standard deviations for each statement

agreement_df_emot <- E5_beliefs_pre_test_tidy %>%
  group_by(item_no) %>%
  summarise(mean_emot = mean(slider_emotion.response),
            std_dev_emot = sd(slider_emotion.response)) %>%
  arrange(std_dev_emot)
agreement_df_belief <- E5_beliefs_pre_test_tidy %>%
  group_by(item_no) %>%
  summarise(mean_belief = mean(slider_belief.response),
            std_dev_belief = sd(slider_belief.response)) %>%
  arrange(std_dev_belief)

ratings_df <- full_join(agreement_df_belief, agreement_df_emot, by = "item_no")

# calculate average topic emotionality ratings

avg_emot <- ratings_df %>%
  filter(between(mean_emot, 3, 5))

# calculate consensus by summing standard deviations

consensus_df <- avg_emot %>%
  mutate(consensus = std_dev_belief + std_dev_emot) %>% 
  arrange(consensus) %>%
  slice_head(n =  2)
```

As before, the `irr` package \cite{irr} to measure interrater agreement on 
statement emotional valence and strength of relatedness for the 25 experimental
items. This analysis revealed that participants agreed above chance on statement
emotional valence ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`,
*p* `r printp(emot_kappa$p.value, add_equals = TRUE)`)
and strength of relatedness ($\kappa$ = `r printnum(belief_kappa$value, digits = 2)`,
*p* `r printp(belief_kappa$p.value, add_equals = TRUE)`).

## Selecting Statements for the Main Experiment {#selecting-statements-e5a}

```{r}
#| label: tbl-candidate-statements
#| include: true
#| tbl-cap: Statements with neutral average emotional valence ratings.

candidates <- avg_emot$item_no

candidate_statements <- read_csv("../data/experimental_dataframes/e5a_exp_dataframe.csv") %>%
  filter(item_no %in% candidates) %>%
  select(c("item_no","statement")) %>%
  rename("Statement" = "statement",
         "Item" = "item_no")

kbl(candidate_statements, booktabs = TRUE, format = "latex")
```

Statements represents neutral emotional valence were selected to control for the
potential effects of statement emotionality in the main experiment. Statements
with average emotionality ratings between 3 and 5 are statements 
`r printnum(add_and_to_numbers(avg_emot$item_no), digits = 0)`, which can be seen
in @tbl-candidate-statements. To ascertain which statements represent the
greatest consensus, standard deviations of ratings for statement emotional valence
and strength of relatedness were summed. Due to concerns about experimental
power, and in line with evidence that propensity for belief change is highest
when prior beliefs are not strongly held \cite{xiong_2023, markant_2023},
at this point the decision was made to test only the statement corresponding to weak beliefs
about the strength of relatedness between the variables in question. Statement number
`r printnum(add_and_to_numbers(consensus_df$item_no[2]), digits = 0)` was therefore
selected: "Higher consumption of spicy foods is associated with a lower risk of 
certain types of cancer", however the wording was modified such that the variables
(food consumption and cancer risk) are positively correlated. While the work
carried out in \chap{chap:adjusting_opacity}, \chap{chap:adjusting_size}, and
\chap{chap:interactions_opacity_size} show that opacity and size adjustments
in scatterplots can affect estimates of positive correlation, no work regarding
the effects of these manipulations in negatively correlated scatterplots has 
been completed.

## Discussion {#discussion-e5a}

Fleiss' Kappa values for interrater agreement on both statement emotional valence
and strength of correlation scales are low ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`
and $\kappa$ = `r printnum(belief_kappa$value, digits = 2)` respectively),
however do exceed that which would be expected by chance. In light of this, 
decisions regarding which statement to use are not based 
on the values of Fleiss' Kappa observed, but rather on the standard deviations of 
ratings across all raters. Statement emotionality and strength of relatedness are
tested with participants in the main study and included as fixed effects as part
of the analyses.

# Main Experiment: Alternative Scatterplot Designs and Beliefs about Relatedness {#beliefs-main-e5}

The statement selected exhibits the lowest average level of belief about strength
of relatedness and the 2^nd^ highest level of consensus. Modified for directionality,
the statement reads:

```{=tex}
\begin{quotation}
    ``Higher consumption of plain (non-spicy) foods
is associated with a higher risk of certain types of cancer.''
    
\end{quotation}
```

To maximise the likelihood of finding an effect of viewing alternative scatterplots,
the stimuli were designed based on a popular British news source and the data
were falsely credited as being supplied by the British National Health Service (NHS).
Participants were told that the news source had requested their identity be obscured.
They were debriefed that this was not the case, and in fact the data were false,
at the end of the experiment. Based on evidence that beliefs can change after
viewing visualisations \cite{karduni_2020, markant_2023}, and that scatterplots
employing point opacity and size manipulations described in \ref{related-work-beliefs}
are able to affect perceptual estimates, the following hypotheses were made:

## Hypotheses {#hypotheses-e5}

  - H1: there will be a significant difference between ratings of strength of
  relatedness made before and after participants viewed scatterplots in either
  the standard or alternative conditions.
  - H2: this difference will be greatest when participants are exposed
  to scatterplots in the alternative scatterplot condition.
  
Exploratory investigations taking into account participants' scores on a defensive
confidence test, their scores on a graph literacy test, and each participant's rating
of the emotional valence of the correlative statement used. Analysis including each of
these factors can be found in Section \ref{add-analyses-e5}, and their inclusion is
justified below.

### Defensive Confidence {#def-con-e5}

In line with evidence that those who are more confident in their ability to
defend their own positions are more susceptible to having those positions changed
\cite{albarracin_2004}, participant's defensive confidence was measured using 
Albarracín and Mitchell's \cite{albarracin_2004} 12-item scale. This scale is 
replicated from previous work in Appendix B, and has 
been utilised more recently \cite{markant_2023} to explore the potential for attitude
change specifically with regard to correlations in scatterplots. Participants
provide answers to the 12 scale items using a 5-point Likert scale anchored at points 1
(*not at all characteristic of me*) and 5 (*extremely characteristic of me*), with 
all other points being unlabelled.

### Graph Literacy {#graph-literacy-e5}

No effect of graph literacy was found in experiments 1 to 4 (see \chap{chap:adjusting_opacity},
\chap{chap:adjusting_size}, and \chap{chap:interactions_opacity_size}). Despite 
this, the scale was included here due to the higher predicted cognitive load
of the current tasks, along with evidence that graph literacy may affect performance on more cognitively
demanding visualisation tasks \cite{canham_2010, okan_2012}. Additionally, the graph
literacy test used \cite{garcia_2016} is extremely short; in the present study,
this took participants an average of `r printnum(literacy_e5$mean_time, digits = 0)`
seconds (*SD* = `r printnum(literacy_e5$sd_time, digits = 0)` seconds).

### Emotionality

The emotional content of a visualisation and the emotional state of a participant
may have cognitive and perceptual effects on performance in visualisation tasks
\cite{phelps_2006, harrison_2013, thoresen_2016}; this was the primary
motivation behind performing the pre-study. Nevertheless, it is not guaranteed
that each participant considers the emotional content of the correlative
statement to be the same. To account for these individual differences, ratings
of emotional valence are also collected during the main study.

## Method {#method-e5}

### Stimuli {#stimuli-e5}

```{r}
#| label: fig-exp5-examples-chap7
#| include: true
#| fig-cap: Examples of the experimental stimuli for experiment 5 using an \textit{r} value of 0.6. Group A saw the alternative scatterplot presented on the right, while group B saw the typical design on the left. 
#| fig-asp: 0.7

ggarrange(example_plot_function_exp5(slopes_exp5,
                                     0.6,
                                     typical,
                                     standard_alpha,
                                     bbc_style()),
          example_plot_function_exp5(slopes_exp5,
                                     0.6,
                                     slopeI,
                                     slopeI_floored,
                                     bbc_style()),
          nrow = 1) +
  annotate(geom = "text",
           label = "Typical Scatterplot",
           x = 0.25,
           y = 0.1,
           size = 3) +
  annotate(geom = "text",
           label = "Atypical Scatterplot",
           x = 0.75,
           y = 0.1,
           size = 3)
```

After selecting a correlative statement describing a weak relationship and with
a high level of consensus between participants, the `ggplot2` package (version 3.5.1)
\cite{wickham_2016} was used to create the stimuli for the main experiment. As
the statement was rated as describing a low level of relatedness, scatterplots
describing a strong relationship (0.6 < *r* < 0.99) were used with the intent
of inducing belief change. Plots in the alternative scatterplot condition were
created using a combination of non-linear opacity and size decay, as this
particular condition was shown to bias correlation estimates to a greater
degree than either point opacity or size decay alone (see experiment 4 in 
\chap{chap:interactions_opacity_size}). 45 *r* values uniformly distributed
between 0.6 and 0.99 were used to create 45 scatterplots for each condition.
Examples of stimuli using an *r* value of 0.6 for both the
standard and alternative scatterplot conditions can be seen in @fig-exp5-examples-chap7.

### Design {#design-e5}

Unlike all previous experiments, a between-participants design was employed
here. Each participant was randomly assigned either to group A, in which they
viewed standard scatterplots, or group B, in which they viewed alternative scatterplots
designed deliberately to elicit higher levels of belief change.
Participants saw all 45 experimental items for their group, along
with 4 attention check items, in a fully randomised order. The dependent variable 
was the level of belief change induced by viewing the scatterplot visualisations,
so participants were tested on how strongly related they believed the variables 
described by the correlative statement were both **before** and **after** viewing the
experimental items. All experimental code, materials, and instructions are 
hosted on GitLab as two separate experiments [^2] [^3].

[^2]: https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_t
[^3]: https://gitlab.pavlovia.org/Strain/atypical_scatterplots_main_a

### Procedure {#procedure-e5}

Participants viewed the PIS and provided consent through key presses in response
to displayed consent statements. Participants were then asked to provide
their age and gender identity. Following this, participants completed
the 5-item Subjective Graph Literacy scale as in previous experiments
\cite{garcia_2016}, and Albarracín and Mitchell's \cite{albarracin_2004} 12-item defensive confidence
scale . To give legitimacy to the data visualisations with hope of
maximising any potential belief change, participants were told that the graphs
they would see were taken from a well-known British news source, but that the
identity of this source had been obscured at their request. To promote 
engagement with the visualisations, participants were instructed to use a slider
to estimate the correlation displayed in each scatterplot; no hypotheses were made 
based on these data, and in the published paper this chapter was based on, they
were not analysed. For completeness sake, I analyse this data in Section \ref{add-analyses-e5}
here. Following instructions, which included textual descriptions of scatterplots
and Pearson's *r*, participants were given two practice trials; these trials
took the form of a typical "standard scatterplot" trial from experiment 1.
Participants were then asked to indicate their beliefs about emotional valence
and strength of relatedness described in the chosen correlative statement; these
data were captured using Likert scales identical to those described previously.
After completing 45 experimental trials, participants were then asked again,
using the same Likert scales, to indicate their beliefs about emotional valence
and strength of relatedness. Interspersed among the experimental items were 
four attention check trials which explicitly asked participants to set the slider
to 0 or 1.

### Participants {#participants-e5}

150 participants were recruited using the Prolific platform \cite{prolific}.
Normal to corrected-to-normal vision and English fluency were required. As in
the pre-study, UK residency was required of participants, as the experiment relied
on familiarity with the style of a British news source. Participants who took part
in the pre-study, or in any of the experiments described in \chap{chap:adjusting_opacity},
\chap{chap:adjusting_size}, or \chap{chap:interactions_opacity_size} were prevented
from completing this experiment. Data were collected from 77 participants in 
each condition. 2 participants failed more than 2 out of 4 attention check
questions for each condition, meaning their data were excluded per
pre-registration stipulations. Data from the remaining 150 participants were
included in the full analysis (`r printnum(E5_beliefs_main_tidy_gender$Male, digits = 2)` male,
`r printnum(E5_beliefs_main_tidy_gender$Female, digits = 2)` female, and 
`r printnum(E5_beliefs_main_tidy_gender$'Non-binary', digits = 2)` non-binary).
Participants' mean age was `r printnum(E5_beliefs_main_tidy_age$mean, digits = 1)`
(*SD* = `r printnum(E5_beliefs_main_tidy_age$sd, digits = 1)`). Participants' 
mean graph literacy score was `r printnum(E5_beliefs_main_tidy_graph_literacy$mean, digits = 1)`
(*SD* = `r printnum(E5_beliefs_main_tidy_graph_literacy$sd, digits = 1)`)
out of 30, their mean defensive confidence score was `r printnum(defensive_confidence_e5$mean, digits = 0)`
(*SD* = `r printnum(defensive_confidence_e5$sd, digits = 1)`) out of 60, and their mean rating of 
statement emotional valence was `r printnum(topic_emotionality_e5$mean, digits = 1)`
(*SD* = `r printnum(topic_emotionality_e5$sd, digits = 1)`) on a 7-point Likert scale.
On average, participants took `r printnum(time_taken_e5$mean, digits = 1)`
minutes to complete the experiment (*SD* = `r printnum(time_taken_e5$sd)`).

## Results {#results-e5}

### Additional Analyses {#add-analyses-e5}

## Discussion {#discussion-e5}

### Graph Literacy, Defensive Confidence, and Statement Emotional Valence

# General Discussion {#general-discussion-e5}

# Limitations {#limitations-e5}

# Future Work {#future-work-e5}
