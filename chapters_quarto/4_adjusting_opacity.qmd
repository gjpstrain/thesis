---
title: "adjusting_opacity"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false      
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_pilot_size <- c(S = "Small (52\\\\%)",
                       M = "Medium (100\\\\%)",
                       L = "Large (252\\\\%)")

labels_pilot_present <- c(N = "Absent",
                          Y = "Present")

labels_e1 <- c(A = "Low Opacity\n(alpha = 0.25)",
               B = "Medium Opacity\n(alpha = 0.5)",
               C = "High Opacity\n(alpha = 0.75)",
               D = "Full Opacity\n(alpha = 1.0)")

labels_e2 <- c(A = "Non-Linear\nDecay",
               B = "Linear\nDecay",
               C = "Inverted\nDecay",
               D = "Full\n Opacity")

labels_e2_regex <- c(
  "\\bA\\b" = "Non-Linear Decay",
  "\\bB\\b" = "Linear Decay",
  "\\bC\\b" = "Inverted Decay",
  "\\bD\\b" = "Full Opacity"
)
```

```{r}
#| label: load-data

exp1_anon <- read_csv("../data/exp_1_data.csv")
exp2_anon <- read_csv("../data/exp_2_data.csv")# all experimental data lives in this folder

# load in shared functions

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models-chap4

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data-chap4

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  dplyr::select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "contrast"), sep = "-") %>%
  mutate(contrast = str_replace(contrast, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  dplyr::select(c("participant",
                  "item",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "trials.thisN",
                  "session")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  full_join(demographics, by = "participant") %>%
  mutate(across(c("item", "contrast", "half"), as_factor)) %>%
  dplyr::select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(contrast, c('D', 'C', 'B', 'A'))) %>%
  rename("opacity" = "contrast") %>% #contrast was an earlier term, so is changed here for consistency                                        

  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

walk(list(exp1_anon,
          exp2_anon),
     wrangle)

# Experiments were incorrectly named, so rename them

E1_uniform_adjustments_tidy <- E3_full_exp_tidy
E2_spatially_dependent_tidy <- E2_full_exp_tidy

# remove incorrectly named and anon dfs from environment

rm(E3_full_exp_tidy, E2_full_exp_tidy, exp1_anon, exp2_anon, wrangle)

# extract age and gender information

extract_age(E1_uniform_adjustments_tidy)

extract_gender(E1_uniform_adjustments_tidy)

extract_literacy(E1_uniform_adjustments_tidy)

extract_age(E2_spatially_dependent_tidy)

extract_gender(E2_spatially_dependent_tidy)

extract_literacy(E2_spatially_dependent_tidy)
```

# Abstract {#abstract-adjusting-opacity}

Scatterplots are common data visualisations utilised for communication with experts 
and lay people alike. Despite being widely studied, it is common for people to 
underestimate the level of correlation displayed in them. The weight of evidence
points toward changes in the opacities of scatterplot points being unable to change
perceptions of correlation, however this was not tested rigorously using systematic
adjustments. Drawing on evidence that the shape of a scatterplot's point cloud
may drive correlation perception, I conducted exploratory work addressing
this underestimation bias. In two experiments (total *N* = 300), evidence is provided
that changing the opacities of scatterplot points *can* have small effects on participants'
performance on a correlation estimation task. The systematic adjustment of 
point opacity as a function of residual distance is able to alter estimates to a greater 
degree and correct for the underestimation bias. In this chapter, I also present
an early pilot study that was ultimately not included in any published works.

# Preface: Learning From an Early Pilot Study {#pilot-study}

The research proposal that kickstarted this project in 2021 set out a plan to 
investigate the perception of correlation in scatterplots as a function of screen size.
This was prompted by recent research demonstrating consistent perceptual biases
in scatterplots due to geometric scaling \cite{wei_2020}, the increasing prevalence
of data visualisations in lay people's daily lives due to the COVID-19 pandemic, and
the increasing adoption of wearable devices. The first experiment conducted therefore
examined how perceptions of correlation changed according to the size of a scatterplot.
Additionally, a very early version of the opacity decay factor from experiment 2
was included. The implementation of this factor was immature. In experiment 2
onwards, if a scatterplot point resided in a particular place, it would always
have the same opacity or size. In the pilot study, the code that set the opacity
of each point always scaled the opacity values such that the point with the highest
residual had the lowest possible opacity, and vice versa, resulting in the plots 
seen in @fig-pilot-study-examples.

```{r}
#| label: fig-pilot-study-examples
#| include: true
#| fig-cap: Examples of the experimental stimuli used in the pilot study (opacity decay factor). On the left, the opacity decay function is visible. Note the linear scaling used.

knitr::include_graphics(path = "../supplied_graphics/pilot_examples.png")
```

This provided no consistency between different experimental stimuli, making it 
difficult to comment on the effects of changing levels of opacity in various 
parts of a plot on correlation estimation. This was later addressed by hardcoding
residual size to a specific value of opacity or size. The pilot also suffered
extensively from poor data quality. Of the 260 participants tested, data from only 
118 was included in the final analyses due to failed attention checks.
It is for this reason that the pre-screen requirements detailed in Section
\ref{recruitment} were implemented.

Participants viewed 180 experimental plots in a 3x2 factorial design. The first
independent variable, plot size, had three levels, 63%, 100%, and 252% scales. The second
was the presence or absence of the opacity decay function (see @fig-pilot-study-examples).
I aimed to recruit 150 participants, but stopped after 118 due to data quality
issues. Nevertheless, the results provided were crucial in informing the future
direction of the research project. I present these results in brief below.

## Pilot Study: Results

```{r}
#| label: pilot-study-wrangle

pilot_data <- read_csv("../data/pilot_data.csv") %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "S", replacement = "-S-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "M", replacement = "-M-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "L", replacement = "-L-")) %>%
  separate(plots_with_labels, c("item", "size", "present"), sep = "-") %>%
  mutate(present = str_replace(present, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = "")) %>%
  dplyr::select(c("item",
                  "size", 
                  "present",
                  "participant",
                  "unique_item_no",
                  "my_rs",
                  "slider.response")) %>%
  filter(unique_item_no < 181) %>%
  mutate(size = as.factor(size)) %>%
  mutate(present = as.factor(present)) %>%
  mutate(difference = my_rs - slider.response) %>%
  filter(!is.na(difference))

contrasts(pilot_data$size) = contr.sum(levels(pilot_data$size))
contrasts(pilot_data$present) = contr.sum(levels(pilot_data$present))

```

```{r}
#| label: pilot-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

pilot_model <- buildmer(difference ~ size * present + 
                          (1 + size * present | participant) +
                          (1 + size * present | item),
                        data = pilot_data)

```

```{r}
#| label: pilot-null-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

pilot_null <- comparison(pilot_model)
```

```{r}
#| label: anova-results-pilot

anova_results(pilot_model, pilot_null)
```

To investigate the effects of plot size and the presence or absence of an opacity
decay manipulation on participants' estimates of correlation, a linear mixed
effects model was built whereby participants' errors in correlation estimation
was predicted by plot size and the presence of the opacity decay function. This model
features random intercepts for participants and items, as well as random slopes for
both participants and items relevant to the presence or absence of the opacity 
decay function. A likelihood ratio test between the experimental model and a 
null model with the fixed effects removed revealed that the experimental
model explained significantly more variance than the null ($\chi^2$(`r in_paren(pilot_model.df)`)
= `r printnum(pilot_model.Chisq)`, *p* `r printp(pilot_model.p, add_equals = TRUE)`).
There was no interaction between plot size and the presence or absence of
the opacity decay function. The **emmeans** \cite{emmeans} package was used 
to explore estimated marginal means (see @tbl-emmeans-pilot) and contrasts (see @tbl-contrasts-pilot)
separately for each factor.

```{r}
#| label: tbl-emmeans-pilot
#| include: true
#| tbl-cap: Estimated Marginal Means for plot size (left) and the presence of the opacity decay function (right).
#| tbl-subcap: ["test1", "test2"]
#| layout-ncol: 2

if (class(pilot_model) == "buildmer") pilot_model <- pilot_model@model

emmeans(pilot_model, pairwise ~ "size")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Size = size,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Size" = recode(Size,
                         "L" = "Large (252%)",
                         "M" = "Medium (100%)",
                         "S" = "Small (62%)")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

emmeans(pilot_model, pairwise ~ "present")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Decay = present,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Decay" = recode(Decay,
                         "N" = "Absent",
                         "Y" = "Present")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

```

```{r}
#| label: tbl-contrasts-pilot
#| include: true
#| tbl-cap: Contrasts between levels of the size factor (left) and opacity decay factor (right).
#| tbl-subcap: ["A", "B"]
#| layout-ncol: 2

contrasts_extract(pilot_model, "size") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_size)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 2), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))

contrasts_extract(pilot_model, "present") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_present)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 2), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))
```

## Pilot Study: Discussion

For the factor of plot size, the effect observed was driven by significant differences
in correlation estimation error between large and small plots. Participants
estimated more accurately when the plot was large and when the decay function
was present. Participants still underestimated correlation in all conditions.
The finding that estimation error was lower for larger plots is in line
with previous evidence that geometrically scaling a scatterplot up can increase
perceptions of the strength of the correlation displayed \cite{wei_2020}. Despite
the statistical significance of this finding, we elected at this point to abandon
the plot size factor due to the extremely small effect (see @tbl-emmeans-pilot) and
lack of novelty.

The impact of even an immature point opacity decay function on correlation estimation
was a novel finding that we felt deserved further, and more rigorous, study. Its
implementation was based on findings that changing the opacities of scatterplot
points could bias estimates of means \cite{hong_2021}, and on limited evidence for
the perception of correlation being based on the perceived width of a probability
distribution represented by the arrangement of scatterplot points. I did not foresee
the decay function, being novel, having a greater effect on correlation estimation
than size. Once evidence had been found that changing the opacity of points in 
scatterplots could have effects on correlation estimation, in opposition to previous
research \cite{rensink_2012, rensink_2014}, the door was opened for a rigorous
investigation into how this worked and how it could be used systematically to
correct for the historic underestimation bias.

# Introduction {#introduction-adjusting-opacity}

Findings from the pilot study suggest that changing the opacities of points
in scatterplots is able to change participants' estimates of the correlation being
displayed. The effect found in that study was too small to make a real difference
with a view to correcting for the underestimation bias, and does not provide information
on *how* changing opacity might change the percept (only that *it does*). Failing
to understand the ways in which opacity is able to change the perception of correlation
prevents future work from tuning what was a small effect in the pilot study
into something with real potential for producing more perceptually optimised
scatterplots.

## Overview {#overview-adjusting-opacity}

In two experiments, the opacities of points in scatterplots was manipulated while 
participants were asked to make judgements of correlation. In the first, point opacity
is changed in a uniform manner, while in the second, point opacity is systematically
altered as a function of the size of a particular point's residual. By comparing
participants' performance on a correlation estimation task for data-identical
scatterplots that vary only in the opacities of their points, it is demonstrated that;
lower global point opacity results in greater errors in the estimation of positive
correlation (experiment 1); and lowering point opacity as a function of the size
of a point's residual is able to bias estimates of positive correlation upwards to 
partially correct for a historic underestimation bias.

# Related Work {#related-work-adjusting-opacity}

## Transparency, Contrast, Opacity, and Formal Definitions

The original paper that this chapter is based on is titled "The Effects of Contrast
on Correlation Perception in Scatterplots". In response to reviewer comments to
the paper that forms Chapter 6, the term "contrast" was changed to "opacity". In order
to maintain consistency throughout this thesis, the more up-to-date wording (opacity)
is used throughout.

Adjusting the opacity of points in scatterplots is an established technique
used to address issues of overplotting or clutter \cite{bertini_2004, matejka_2015},
in which scatterplots with large numbers of data points suffer from visibility issues
caused by excessive point density. Lowering the opacity of all scatterplot points
using alpha blending \cite{few_2008} addresses this, and makes data trends and 
distributions easier to see and interpret for the reader. @fig-overplotting-example
demonstrates the impact of lowering global point opacity in a scatterplot with
a very high number of data points.

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.6
#| fig-cap: Adjusting point opacity to address overplotting. Contrast between the points and the background is full (alpha = 1, full opacity points, Left) or low (alpha = .1, low opacity points, Right). The dataset used has 40,000 points.
 
set.seed(123)

data <- data.frame(x = c(rnorm(20000, mean = -1),
                         rnorm(20000, mean = 1)),
                   y = rnorm(40000))

ggplot(data, aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

Lowering opacity leads to a reduction of the contrast of isolated points with the background,
and for regions with overlapping points, colour intensities are summed. The stimuli
used in the experiments throughout this chapter had 128 small points, meaning
the majority of points were clearly visible at all times. For this reason, the effects
of point overlap were not taken into account when designing and analysing the experiments
described here. Due to this, the approach to opacity described in this chapter would
not be useful when dealing with much larger datasets where clutter becomes an issue.

The **ggplot2** \cite{wickham_2016} package was used in R to create stimuli for 
this experiment. This package uses an alpha parameter to set point opacity. Alpha
here refers to the level of linear interpolation \cite{stone_2008} between foreground
and background pixel values; alpha values of 0 (full transparency) and 1 (full opacity)
result in no interpolation and rendering of either the background or foreground
pixel values respectively. Alpha values between 0 and 1 correspond to different
ratios of interpolation, and are illustrated in @fig-alpha-examples.

```{r}
#| label: fig-alpha-examples
#| include: true
#| fig-asp:  0.2
#| fig-cap: The relationship between alpha values and rendered point opacity. Higher alpha values result in greater contrast between the foreground (scatterplot point) and background. When alpha = 0, the foreground is ignored and the background is rendered.

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)
ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 14, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11)) +
rm(x1,y1,df)
```

Definitions of contrast, opacity, and transparency are fuzzy. Often, different 
works will use the terms interchangeably. As mentioned above, I initially
elected to use the term "contrast", given the fundamentality of contrast as a
feature of human visual perception \cite{ginsburg_2003}, however later reviewer
comments prompted the change to "opacity". Nevertheless, we can consider *opacity*
as it is used here to be shorthand for "the contrast between foreground and
background objects", as visually, these concepts are the same. There are numerous
psychophysical definitions of perceived contrast \cite{zuffi_2007} based on what
is being presented, for example, models that take into account visibility
limits (CIELAB lightness), or contrast in periodic patterns such as sinusoidal
gratings (Michelson's contrast). The common thread running through these definitions
is the use of a ratio between target and background luminances. The experiments
described here take place online, with participants completing experiments
on their personal laptop or desktop computers. Due to this, the experimenter
has no control over the exact luminances of stimuli, only over the relative
luminance between targets (scatterplot points) and backgrounds. Given my interest
in relative differences in correlation perception averaged over a series of 
180 single-plot trials, this lack of control over the exact nature of the stimuli
was not problematic. It does mean that reporting the exact luminance values would be
pointless however, so where a value for opacity is referred to in this chapter, 
it is the alpha value specified by **ggplot2**.

## Effects of Point Opacity on Correlation Estimation

Despite the popularity of adjusting opacity to address overplotting issues,
little investigation had taken place into the effects of reducing point opacity
on people's perceptions of correlation. In 2012, Rensink \cite{rensink_2012} found
correlation perception to be invariant to changes in point opacity, although this
work took place with only small sample sizes (n = 12), and using only bisection/JND
methodologies.

Changing the contrast between a stimulus and its background (lowering its opacity)
effectively reduces the strength of its signal. A likely consequence oif this is 
greater levels of uncertainty in aspects of that stimulus, for example, the locations
of points in scatterplots. Consequently, one might anticipate that increased uncertainty
could lead to altered perceptions of correlation and/or the presence of greater
levels of noise in correlation estimates due to effects on the perceived position
of points within a scatterplot point cloud. While there is evidence \cite{wehrhahn_1990}
that perception of stimulus position becomes exponentially worse as contrast
is reduced (as measured by vernier acuity tasks), this is only true for a narrow
range of low contrast stimuli just above the detection threshold. For stimuli
that feature higher contrasts between them and their backgrounds, vernier acuity
appears robust to such changes. Nevertheless, there is evidence that
other perceptual estimates become more uncertain with reduced contrast, such as speed
perception \cite{warren_2017}. With this in mind, I argue that the effects of contrast
on perceived correlation in scatterplots warrants further investigation.

Hong et al., in 2021 \cite{hong_2021} used point opacity and size to encode the third
variable in trivariate scatterplots. Participants were asked to judge the average
position of all the points displayed. It was found that participants' estimates
of average point position were biased towards areas of larger or darker points; this
was termed the *weighted average illusion*. Together with evidence that darker (more opaque)
and larger points are more salient \cite{healey_2012}, the implication I took from this
work was that I could use point opacity to systematically lower the salience of 
of the points representing the widest parts of the probability distribution; if
participants promptly perceived a narrower distribution, I expected this to be able
to (at least partially) correct for the underestimation bias.

One way to correct for an underestimation in correlation would be to simply remove
outer data points until correlation perception is aligned with the actual correlation
value. However, this would necessitate hiding data and thus changing the information
presented to the viewer. An alternative approach is to manipulate the opacity of
only some of the points; it would seem most sensible to do so for the points
that are more extreme relative to the underlying regression line. In the present
study these questions are explored in two online experiments with large sample sizes.
In the first, the effects of point opacity over the entire scatterplot on correlation estimates
is investigated. In the second experiment we examine how changing contrast as a function 
of distance to the regression line affects perceived correlation.
To pre-empt our results we find clear effects of both manipulations.

# General Methods {#shared-methods-adjusting-opacity}

The experiments described in this chapter share multiple aspects of their procedures.
Both experiments were built using PsychoPy \cite{peirce_2019} and are hosted on pavlovia.org.
Both use 1-factor, 4-level designs. Ethical approval for both experiments was granted
by the University of Manchester's Computer Science Departmental Panel (Ref:
2022-14660-24397). In each experiment, participants were shown the respective
Participant Information Sheet (henceforth PIS) and provided consent through key presses
in response to consent statements. Participants were asked to provide their age and 
gender identity, after which they completed the 5-item Subjective Graph Literacy
test described by Garcia-Retamero et al. \cite{garcia_2016} and discussed in 
Section \ref{graph-literacy-lit-review} of the literature review. Early piloting
with a graduate student in humanities suggested the potential for participants to
be unfamiliar with the visual nature of different values of Pearson's *r*. Participants
were therefore shown examples of *r* = 0.2, 0.5, 0.8, and 0.95 (see @fig-training-slide-adjusting-opacity);
a discussion of the effects of this training is provided in Section \ref{training-adjusting-opacity}.
Participants were given two practice trials to familiarise themselves with the
response slider.

```{r}
#| label: fig-training-slide-adjusting-opacity
#| include: true
#| fig-cap: Participants viewed these plots for at least eight seconds before being allowed to continue to the practice trials.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example-plots.png", dpi = NA)
```

```{r}
#| label: fig-mask-adjusting-opacity
#| include: true
#| fig-cap: An example of a visual mask displayed for 2.5 seconds before each experimental trial.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/visual_mask.png")
```

Each trial was preceded by text that either told the participant:

 - Please look at the following plot and use the slider to estimate the correlation (n = 180).
 - Please IGNORE the correlation displayed and set the slider to 1 (n = 3) or 0 (n = 3).
 
 The latter instructions were attention checks, and were formatted with red text 
 to increase their visibility. Each experimental trial was preceded by a visual
 mask (see @fig-mask-adjusting-opacity) that was displayed for 2.5 seconds. Participants
 were instructed to make their judgements as quickly and accurately as possible,
 but there was no time limit per trial. Both experiments described here use a
 fully repeated-measures, within-participants design. Participants saw all 180 experimental
 items, corresponding to ~27,000 individual judgements per experiment, in a fully
 randomised order.
 
 Both experiments were conducted according to principles of open and reproducible
 research. All data and analysis code for the origin paper is available on GitHub [^1].
 Experiment 1 [^2] and 2 [^3] are hosted on Pavlovia.org, while the Open Science
 Framework hosts pre-registrations [^4]. It is important to note at this point
 that experiment 2 was conducted prior to experiment 1; when the original paper
 was written, the order of presentation of the experiments was swapped to make the
 narrative more cohesive. I preserve this order in the present chapter.
 
 [^1]: https://github.com/gjpstrain/contrast_and_scatterplots
 [^2]: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments
 [^3]: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent
 [^4]: Experiment 1 - https://osf.io/tuexh. Experiment 2 - https://osf.io/6f5ev
 
# Experiment 1: Uniform Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e1}

Owing to the robust effects of altering stimulus opacity on perception described 
above \cite{wehrhahn_1990, champion_2017}, it was hypothesised that there would be 
a greater spread of estimates of correlation for plots with lower global opacity
compared to higher opacity plots.

## Method {#methods-adjusting-opacity-e1}

### Participants

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E1_uniform_adjustments_tidy_gender$M)`% male, `r printnum(E1_uniform_adjustments_tidy_gender$F)`
% female, and `r printnum(E1_uniform_adjustments_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E1_uniform_adjustments_tidy_age$mean)`
(*SD* = `r printnum(E1_uniform_adjustments_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E1_uniform_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$sd)`). The mean
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design

For each of the 45 *r* values, there were four versions of each plot corresponding
to the four levels of point opacity. Examples of each of these can be seen in
@fig-exp1-examples-chap4, demonstrated with an *r* value of 0.6.

```{r}
#| label: fig-exp1-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 1, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by ggplot.
#| fig-asp: 0.275

ggarrange(plot_example_function(data_with_resid, "Opacity = 0.25",
                                0.25, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.5",
                                0.5, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.75",
                                0.75, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 1",
                                1, 0.2, 7),
          nrow = 1)
```

## Analysis {#analysis-adjusting-opacity-e1}

```{r}
#| label: model-e1
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E1_uniform_adjustments_tidy)
```

```{r}
#| label: model-e1-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e1_lit_model <- add_fixed_effect(e1_model, "literacy", "E1_uniform_adjustments_tidy")
```

```{r}
#| label: model-e1-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model_cmpr <- comparison(e1_model)
```

```{r}
#| label: anova-results-e1

anova_results(e1_model, e1_model_cmpr)

# also do anova for literacy model

anova_results(e1_lit_model, e1_model)
```

To investigate the effects of opacity condition on participants' estimates
of correlation, a linear mixed effects model was built whereby opacity condition
is a predictor for the difference between objective *r* values for each plot
and participants' estimates of *r*. This model has random intercepts for 
items and participants. A likelihood ratio test revealed that the mode including
global opacity as a fixed effect explained significantly more variance than a 
null model ($\chi^2$(`r in_paren(e1_model.df)`) = `r printnum(e1_model.Chisq)`,
*p* `r printp(e1_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity condition, along with 95% confidence intervals.

```{r}
#| label: fig-e1-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 1. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error. The overestimation zone is included to facilitate comparison to later work.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

plotting_df <- as_tibble(emmeans(e1_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D"))) %>%
  mutate(opacity = recode(opacity, !!!labels_e1)) %>%
  rename("Condition" = "opacity")

ggplot(aes(x = Condition, y = emmean*-1), data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e1
#| include: true
#| tbl-cap: Contrasts between different levels of the opacity factor in experiment 1.

# assign slot, if it hasn't been assigned already

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

# make table df

contrasts_extract(e1_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e1)) %>%
  separate(Contrast, c(" ", "  "), sep = "-") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))
```

This effect was driven by significant difference between means of correlation 
estimation error between all conditions bar high and full opacity. Statistical
tests for contrasts were performed using the **emmeans** package \cite{emmeans}, 
and are shown in @tbl-contrasts-e1. To test whether the observed results could be
explained by difference in participants' levels of graph literacy, an additional
model was built. This model is identical to the experimental model, but also
includes graph literacy as a fixed effect. Including graph literacy as a fixed effect
explained no additional variance ($\chi^2$(`r in_paren(e1_lit_model.df)`) =
`r printp(e1_lit_model.Chisq)`, *p* `r printp(e1_lit_model.p, add_equals = TRUE)`),
indicating that the differences observed in participants' correlation estimation 
performance were not as a result of differences in levels of graph literacy.

```{r}
#| label: tbl-efs-e1
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes for levels of the opacity factor in experiment 1. Each effect size is compared to the reference level, full contrast (alpha = 1).

# this approximation uses t values and the pooled standard deviation of the differences

options(knitr.kable.NA = "")

efs <- lme.dscore(e1_model, E1_uniform_adjustments_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Low Opacity (alpha = 0.25)",
                         "opacityB" = "Medium Opacity (alpha = 0.5)",
                         "opacityC" = "High Opacity (alpha = 0.75)",
                         "opacityD" = "Full Opacity (alpha = 1.0)")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)
```

A function from the now archived **EMAtools** package \cite{ematools} was used to 
calculate an approximation of Cohen's *d* between the reference level (full contrast, 
alpha = 1.0) and each other level of the opacity factor. These statistics can be
seen in @tbl-efs-e1. The largest effect size observed (*d* ~ 0.16) is between the 
low and full opacity conditions, and is small. This was unsurprising given the lack 
of previously reported effects on correlation perception of global point opacity
\cite{rensink_2012}.

## Discussion {#discussion-adjusting-opacity-e1}

# Experiment 2: Spatially-Dependent Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e2}

## Methods {#methods-adjusting-opacity-e2}

### Participants {#participants-adjusting-opacity-e2}

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 7 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E2_spatially_dependent_tidy_gender$M)`% male, `r printnum(E2_spatially_dependent_tidy_gender$F)`
% female, and `r printnum(E2_spatially_dependent_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E2_spatially_dependent_tidy_age$mean)`
(*SD* = `r printnum(E2_spatially_dependent_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E2_spatially_dependent_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design {#design-adjusting-opacity-e2}

For each of the 45 *r* values in experiment 2, there were four versions 
of each plot corresponding to the three levels of point opacity decay function and 
the baseline global full opacity condition. Examples of each of these can be 
seen in @fig-exp2-examples-chap4, demonstrated with an *r* value of 0.6.

```{r}
#| label: fig-exp2-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 2, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by ggplot.
#| fig-asp: 0.275

ggarrange(plot_example_function(slopes, "Non-Linear Decay",
                                (1-slopes$slope_0.25), 0.2, 7),
          plot_example_function(slopes, "Linear Decay",
                                (1-slopes$slope_linear), 0.2, 7),
          plot_example_function(slopes, "Inverted Decay",
                                (1-slopes$slope_inverted), 0.2, 7),
          plot_example_function(slopes, "Full Opacity",
                                1, 0.2, 7),
          nrow = 1)
```

## Analysis {#analysis-adjusting-opacity-e2}

```{r}
#| label: model-e2
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e2_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E2_spatially_dependent_tidy)
```

```{r}
#| label: model-e2-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e2_lit_model <- add_fixed_effect(e2_model, "literacy", "E2_spatially_dependent_tidy")
```

```{r}
#| label: model-e2-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e2_model_cmpr <- comparison(e2_model)
```

```{r}
#| label: anova-results-e2

anova_results(e2_model, e2_model_cmpr)

# also do anova for literacy model

anova_results(e2_lit_model, e2_model)
```

To investigate the effects of the opacity decay functions on participants' estimates
of correlation, a linear mixed effects model was built whereby decay function condition
is a predictor for the difference between objective *r* values for each plot
and participants' estimates of *r*. This model has random intercepts for 
items and participants. A likelihood ratio test revealed that the mode including
global opacity as a fixed effect explained significantly more variance than a 
null model ($\chi^2$(`r in_paren(e2_model.df)`) = `r printnum(e2_model.Chisq)`,
*p* `r printp(e2_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity decay function condition,
along with 95% confidence intervals.

```{r}
#| label: fig-e2-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 2. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

plotting_df <- as_tibble(emmeans(e2_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D"))) %>%
  mutate(opacity = recode(opacity, !!!labels_e2)) %>%
  rename("Condition" = "opacity")

ggplot(aes(x = Condition, y = emmean*-1), data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e2
#| include: true
#| tbl-cap: Contrasts between different levels of opacity decay function in experiment 2.

# assign slot, if it hasn't been assigned already

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

# make table df

contrasts_extract(e2_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e2_regex)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 1), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))

```

The effect seen in experiment 2 was driven by significant differences in means of 
correlation estimation error between all levels of opacity decay function condition
bar full opacity and linear decay. Statistical testing for contrasts was performed
using the **emmeans** package \cite{emmeans}, and are shown in @tbl-contrasts-e2.
To test whether the observed results could be explained by differences in graph 
literacy, a model including participants' graph literacy scores as a fixed effect
was built. Including graph literacy as a fixed effect again explained no additional
variance ($\chi^2$(`r in_paren(e2_lit_model.df)`) = `r printp(e2_lit_model.Chisq)`,
*p* `r printp(e2_lit_model.p, add_equals = TRUE)`).

```{r}
#| label: tbl-efs-e2
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes for levels of the opacity decay function factor in experiment 2. Each effect size is compared to the reference level, full contrast (alpha = 1).

# this approximation uses t values and the pooled standard deviation of the differences

options(knitr.kable.NA = "")

efs <- lme.dscore(e2_model, E2_spatially_dependent_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Non-Linear Decay",
                         "opacityB" = "Linear Decay",
                         "opacityC" = "Inverted Decay",
                         "opacityD" = "Full Opacity")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)
```

Approximated Cohen's *d* effect sizes between the baseline (global full opacity)
and each other condition can be seen in @tbl-efs-e2. The largest effect size observed
(~0.23 between full opacity and inverted decay conditions) is small to moderate,
and the effect size between the baseline and the non-linear decay condition (~0.19) is small.

## Discussion {#discussion-adjusting-opacity-e2}

# General Discussion {#general-discussion-adjusting-opacity}

## Training {#training-adjusting-opacity}
