---
title: "adjusting_opacity"
output:
  format: 
    latex:
      
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA

cache-dir: "../cache/pdf"
    
execute: 
  echo: false
  warning: false
  message: false
  include: false      
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_e1 <- c(A = "Low Contrast", B = "Medium Contrast", C = "High Contrast", D = "Full Contrast")
labels_e2 <- c(A = "Non-Linear Decay", B = "Linear Decay", C = "Inverted Decay", D = "Full Contrast")
```

```{r}
#| label: load-data

exp1_anon <- read_csv("../data/exp_1_data.csv")
exp2_anon <- read_csv("../data/exp_2_data.csv")# all experimental data lives in this folder
```

```{r}
#| label: retrieve-cached-models

if (!params$eval_models){ lazyload_cache_dir("../cache/pdf") }
```

```{r}
#| label: wrangle-data

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  dplyr::select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "contrast"), sep = "-") %>%
  mutate(contrast = str_replace(contrast, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  dplyr::select(c("participant",
                  "item",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "trials.thisN",
                  "session")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  mutate(across(c("item", "contrast", "half"), as_factor)) %>%
  dplyr::select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(contrast, c('A', 'B', 'C', 'D'))) %>%
  rename("opacity" = "contrast") %>% #contrast was an earlier term, so is changed here for consistency                                        

  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

walk(list(exp1_anon,
          exp2_anon),
     wrangle)

# Experiments were incorrectly named, so rename them

E1_uniform_adjustments_tidy <- E3_full_exp_tidy
E2_spatially_dependent_tidy <- E2_full_exp_tidy

# remove incorrectly named and anon dfs from environment

rm(E3_full_exp_tidy, E2_full_exp_tidy, exp1_anon, exp2_anon, wrangle)

# extract age and gender information
# first grab demographics functions from shared functions file

source("../shared_functions.R")

extract_age(E1_uniform_adjustments_tidy)

extract_gender(E1_uniform_adjustments_tidy)

extract_literacy(E1_uniform_adjustments_tidy)

extract_age(E2_spatially_dependent_tidy)

extract_gender(E2_spatially_dependent_tidy)

extract_literacy(E2_spatially_dependent_tidy)
```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# terms removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size * opacity)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
}
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean*-1)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = -1*mean + sd, ymax = -1*mean - sd),
                width = 0.01,
                size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6,
                                  margin = margin(1,0,1,0, "mm")),
        aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title.y = ggtext::element_markdown(size = 8),
        axis.title.x = ggtext::element_markdown(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective *r*",
         y = "Mean *r* Estimation Error") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

# Abstract {#abstract-adjusting-opacity}

Scatterplots are common data visualisations utilised for communication with experts 
and lay people alike. Despite being widely studied, it is common for people to 
underestimate the level of correlation displayed in them. The weight of evidence
points toward changes in the opacities of scatterplot points being unable to change
perceptions of correlation, however this was not tested rigorously using systematic
adjustments. Drawing on evidence that the shape of a scatterplot's point cloud
may drive correlation perception, I conducted exploratory work addressing
this underestimation bias. In two experiments (total *N* = 300), evidence is provided
that changing the opacities of scatterplot points *can* have small effects on participants'
performance on a correlation estimation task. The systematic adjustment of 
point opacity as a function of residual distance is able to alter estimates to a greater 
degree and correct for the underestimation bias. In this chapter, I also present
an early pilot study that was ultimately not included in any published works.

# Preface: Learning From an Early Pilot Study {#pilot-study}

# Introduction {#introduction-adjusting-opacity}

## Overview {#overview-adjusting-opacity}

# Related Work {#related-work-adjusting-opacity}

## Transparency, Contrast, Opacity, and Formal Definitions

- include the "formalising contrast" part of the original papers general methods section here
- also include justification for referring to "opacity" instead of contrast

## Effects of Point Opacity on Correlation Estimation

# General Methods {#shared-methods-adjusting-opacity}

The experiments described in this chapter share multiple aspects of their procedures.
Both experiments were built using PsychoPy \cite{peirce_2019} and are hosted on pavlovia.org.
Both use 1-factor, 4-level designs. Ethical approval for both experiments was granted
by the University of Manchester's Computer Science Departmental Panel (Ref:
2022-14660-24397). In each experiment, participants were shown the respective
Participant Information Sheet (henceforth PIS) and provided consent through key presses
in response to consent statements. Participants were asked to provide their age and 
gender identity, after which they completed the 5-item Subjective Graph Literacy
test described by Garcia-Retamero et al. \cite{garcia_2016} and discussed in 
Section \ref{graph-literacy-lit-review} of the literature review. Early piloting
with a graduate student in humanities suggested the potential for participants to
be unfamiliar with the visual nature of different values of Pearson's *r*. Participants
were therefore shown examples of *r* = 0.2, 0.5, 0.8, and 0.95 (see @fig-training-slide-adjusting-opacity);
a discussion of the effects of this training is provided in Section \ref{training-adjusting-opacity}.
Participants were given two practice trials to familiarise themselves with the
response slider.

```{r}
#| label: fig-training-slide-adjusting-opacity
#| include: true
#| fig-cap: Participants viewed these plots for at least eight seconds before being allowed to continue to the practice trials.
#| out-width: NA
#| out-height: NA
# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example-plots.png", dpi = NA)
```

```{r}
#| label: fig-mask-adjusting-opacity
#| include: true
#| fig-cap: An example of a visual mask displayed for 2.5 seconds before each experimental trial.
#| out-width: NA
#| out-height: NA
# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/visual_mask.png", dpi = NA)
```

Each trial was preceded by text that either told the participant:

 - Please look at the following plot and use the slider to estimate the correlation (n = 180).
 - Please IGNORE the correlation displayed and set the slider to 1 (n = 3) or 0 (n = 3).
 
 The latter instructions were attention checks, and were formatted with red text 
 to increase their visibility. Each experimental trial was preceded by a visual
 mask (see @fig-mask-adjusting-opacity) that was displayed for 2.5 seconds. Participants
 were instructed to make their judgements as quickly and accurately as possible,
 but there was no time limit per trial. Both experiments described here use a
 fully repeated-measures, within-participants design. Participants saw all 180 experimental
 items, corresponding to ~27,000 individual judgements per experiment, in a fully
 randomised order.
 
 Both experiments were conducted according to principles of open and reproducible
 research. All data and analysis code for the origin paper is available on GitHub [^1].
 Experiment 1 [^2] and 2 [^3] are hosted on Pavlovia.org, while the Open Science
 Framework hosts pre-registrations [^4]. It is important to note at this point
 that experiment 2 was conducted prior to experiment 1; when the original paper
 was written, the order of presentation of the experiments was swapped to make the
 narrative more cohesive. I preserve this order in the present chapter.
 
 [^1]: https://github.com/gjpstrain/contrast_and_scatterplots
 [^2]: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments
 [^3]: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent
 [^4]: Experiment 1 - https://osf.io/tuexh. Experiment 2 - https://osf.io/6f5ev
 
# Experiment 1: Uniform Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e1}

Owing to the robust effects of altering stimulus opacity on perception described 
above \cite{wehrhahn_1990, champion_2017}, it was hypothesised that there would be 
a greater spread of estimates of correlation for plots

## Methods {#methods-adjusting-opacity-e1}

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E1_uniform_adjustments_tidy_gender$M)`% male, `r printnum(E1_uniform_adjustments_tidy_gender$F)`
% female, and `r printnum(E1_uniform_adjustments_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E1_uniform_adjustments_tidy_age$mean)`
(*SD* = `r printnum(E1_uniform_adjustments_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E1_uniform_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

## Analysis {#analysis-adjusting-opacity-e1}

## Discussion {#discussion-adjusting-opacity-e1}

# Experiment 2: Spatially-Dependent Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e2}

## Methods {#methods-adjusting-opacity-e2}

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 7 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E2_spatially_dependent_tidy_gender$M)`% male, `r printnum(E2_spatially_dependent_tidy_gender$F)`
% female, and `r printnum(E2_spatially_dependent_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E2_spatially_dependent_tidy_age$mean)`
(*SD* = `r printnum(E2_spatially_dependent_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E2_spatially_dependent_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 33 minutes (SD = 10 minutes).


## Analysis {#analysis-adjusting-opacity-e2}

## Discussion {#discussion-adjusting-opacity-e2}

# General Discussion {#general-discussion-adjusting-opacity}

## Training {#training-adjusting-opacity}
