---
title: "adjusting_opacity"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false      
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_pilot_size <- c(S = "Small (52\\\\%)",
                       M = "Medium (100\\\\%)",
                       L = "Large (252\\\\%)")

labels_pilot_present <- c(N = "Absent",
                          Y = "Present")

labels_e1 <- c(A = "Low Opacity\n(alpha = 0.25)",
               B = "Medium Opacity\n(alpha = 0.5)",
               C = "High Opacity\n(alpha = 0.75)",
               D = "Full Opacity\n(alpha = 1.0)")

labels_e2 <- c(A = "Non-Linear\nDecay",
               B = "Linear\nDecay",
               C = "Inverted\nDecay",
               D = "Full\n Opacity")

labels_e2_regex <- c(
  "\\bA\\b" = "Non-Linear Decay",
  "\\bB\\b" = "Linear Decay",
  "\\bC\\b" = "Inverted Decay",
  "\\bD\\b" = "Full Opacity"
)
```

```{r}
#| label: load-data

exp1_anon <- read_csv("../data/exp_1_data.csv")
exp2_anon <- read_csv("../data/exp_2_data.csv")# all experimental data lives in this folder

# load in shared functions

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models-chap4

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data-chap4

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  dplyr::select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "contrast"), sep = "-") %>%
  mutate(contrast = str_replace(contrast, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  dplyr::select(c("participant",
                  "item",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "trials.thisN",
                  "session")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  full_join(demographics, by = "participant") %>%
  mutate(across(c("item", "contrast", "half"), as_factor)) %>%
  dplyr::select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(contrast, c('D', 'C', 'B', 'A'))) %>%
  rename("opacity" = "contrast") %>% #contrast was an earlier term, so is changed here for consistency                                        

  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

walk(list(exp1_anon,
          exp2_anon),
     wrangle)

# Experiments were incorrectly named, so rename them

E1_uniform_adjustments_tidy <- E3_full_exp_tidy
E2_spatially_dependent_tidy <- E2_full_exp_tidy

# remove incorrectly named and anon dfs from environment

rm(E3_full_exp_tidy, E2_full_exp_tidy, exp1_anon, exp2_anon, wrangle)

# extract age and gender information

extract_age(E1_uniform_adjustments_tidy)

extract_gender(E1_uniform_adjustments_tidy)

extract_literacy(E1_uniform_adjustments_tidy)

extract_age(E2_spatially_dependent_tidy)

extract_gender(E2_spatially_dependent_tidy)

extract_literacy(E2_spatially_dependent_tidy)
```

# Abstract {#abstract-adjusting-opacity}

Scatterplots are common data visualisations utilised for communication with experts 
and lay people alike. Despite being widely studied, it is common for people to 
underestimate the level of correlation displayed in them. The weight of evidence
points toward changes in the opacities of scatterplot points being unable to change
perceptions of correlation, however this was not tested rigorously using systematic
adjustments. Drawing on evidence that the shape of a scatterplot's point cloud
may drive correlation perception, I conducted exploratory work addressing
this underestimation bias. In two experiments (total *N* = 300), evidence is provided
that changing the opacities of scatterplot points *can* have small effects on participants'
performance on a correlation estimation task. The systematic adjustment of 
point opacity as a function of residual distance is able to alter estimates sufficiently to
correct for the underestimation bias. In this chapter, I also present
an early pilot study that was ultimately not included in any published works.

# Preface: Learning From an Early Pilot Study {#pilot-study}

The research proposal that kickstarted this project in 2021 set out a plan to 
investigate the perception of correlation in scatterplots as a function of screen size.
This proprosal is included in the supplemental materials.
This was prompted by recent research demonstrating consistent perceptual biases
in scatterplots due to geometric scaling \cite{wei_2020}, the increasing prevalence
of data visualisations in lay people's daily lives due to the COVID-19 pandemic, and
the increasing adoption of wearable devices. The first experiment conducted therefore
examined how perceptions of correlation changed according to the size of a scatterplot.
Additionally, a very early version of the opacity decay factor from experiment 2
was included, however the implementation of this factor was immature. In experiment 2
onwards, if a scatterplot point resided in a particular place, it would always
have the same opacity or size. In the pilot study, the code that set the opacity
of each point always scaled the opacity values such that the point with the highest
residual had the lowest possible opacity, and vice versa, resulting in the plots 
seen in @fig-pilot-study-examples.

```{r}
#| label: fig-pilot-study-examples
#| include: true
#| fig-cap: Examples of the experimental stimuli used in the pilot study (opacity decay factor). On the left, the opacity decay function is visible. Note the linear scaling used.

knitr::include_graphics(path = "../supplied_graphics/pilot_examples.png")
```

This provided no consistency between different experimental stimuli, making it 
difficult to comment on the effects of changing levels of opacity in various 
parts of a plot on correlation estimation. This was later addressed by hardcoding
residual size to a specific value of opacity or size. The pilot also suffered
extensively from poor data quality. Of the 260 participants tested, data from only 
118 was included in the final analyses due to failed attention checks.
It is for this reason that the pre-screen requirements detailed in Section
\ref{recruitment} were implemented.

Participants viewed 180 experimental plots in a 3x2 factorial design. The first
independent variable, plot size, had three levels, 63%, 100%, and 252% scales. The second
was the presence or absence of the opacity decay function (see @fig-pilot-study-examples).
I aimed to recruit 150 participants, but stopped after 118 due to data quality
issues. Nevertheless, the results provided were crucial in informing the future
direction of the research project. I present these results in brief below.

## Pilot Study: Results

```{r}
#| label: pilot-study-wrangle

pilot_data <- read_csv("../data/pilot_data.csv") %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "S", replacement = "-S-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "M", replacement = "-M-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels, pattern = "L", replacement = "-L-")) %>%
  separate(plots_with_labels, c("item", "size", "present"), sep = "-") %>%
  mutate(present = str_replace(present, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = "")) %>%
  dplyr::select(c("item",
                  "size", 
                  "present",
                  "participant",
                  "unique_item_no",
                  "my_rs",
                  "slider.response")) %>%
  filter(unique_item_no < 181) %>%
  mutate(size = as.factor(size)) %>%
  mutate(present = as.factor(present)) %>%
  mutate(difference = my_rs - slider.response) %>%
  filter(!is.na(difference))

contrasts(pilot_data$size) = contr.sum(levels(pilot_data$size))
contrasts(pilot_data$present) = contr.sum(levels(pilot_data$present))

```

```{r}
#| label: pilot-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

pilot_model <- buildmer(difference ~ size * present + 
                          (1 + size * present | participant) +
                          (1 + size * present | item),
                        data = pilot_data)

```

```{r}
#| label: pilot-null-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

pilot_null <- comparison(pilot_model)
```

```{r}
#| label: anova-results-pilot

anova_results(pilot_model, pilot_null)
```

To investigate the effects of plot size and the presence or absence of an opacity
decay manipulation on participants' estimates of correlation, a linear mixed
effects model was built whereby participants' errors in correlation estimation
were predicted by plot size and the presence or absence of the opacity decay function. This model
features random intercepts for participants and items, as well as random slopes for
both participants and items relevant to the presence or absence of the opacity 
decay function. A likelihood ratio test between the experimental model and a 
null model with the fixed effects removed revealed that the experimental
model explained significantly more variance than the null ($\chi^2$(`r in_paren(pilot_model.df)`)
= `r printnum(pilot_model.Chisq)`, *p* `r printp(pilot_model.p, add_equals = TRUE)`).
There was no interaction between plot size and the presence or absence of
the opacity decay function. The **emmeans** \cite{lenth_2024} package was used 
to explore estimated marginal means (see @tbl-emmeans-pilot) and contrasts (see @tbl-contrasts-pilot)
separately for each factor.

```{r}
#| label: tbl-emmeans-pilot
#| include: true
#| tbl-cap: Estimated Marginal Means of correlation estimation error for plot size (left) and the presence of the opacity decay function (right).
#| tbl-subcap: ["test1", "test2"]
#| layout-ncol: 2

if (class(pilot_model) == "buildmer") pilot_model <- pilot_model@model

emmeans(pilot_model, pairwise ~ "size")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Size = size,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Size" = recode(Size,
                         "L" = "Large (252%)",
                         "M" = "Medium (100%)",
                         "S" = "Small (62%)")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

emmeans(pilot_model, pairwise ~ "present")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Decay = present,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Decay" = recode(Decay,
                         "N" = "Absent",
                         "Y" = "Present")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

```

```{r}
#| label: tbl-contrasts-pilot
#| include: true
#| tbl-cap: Contrasts between levels of the size factor (left) and opacity decay factor (right).
#| tbl-subcap: ["A", "B"]
#| layout-ncol: 2

contrasts_extract(pilot_model, "size") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_size)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 2), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))

contrasts_extract(pilot_model, "present") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_present)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 2), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))
```

## Pilot Study: Discussion

For the factor of plot size, the effect observed was driven by significant differences
in correlation estimation error between large and small plots. Participants
estimated more accurately when the plot was large and when the decay function
was present. Participants still underestimated correlation in all conditions.
The finding that estimation error was lower for larger plots is in line
with previous evidence that geometrically scaling a scatterplot up can increase
perceptions of the strength of the correlation displayed \cite{wei_2020}. Despite
the statistical significance of this finding, we elected at this point to abandon
the plot size factor due to the extremely small effect (see @tbl-emmeans-pilot) and
lack of novelty.

The impact of even an immature point opacity decay function on correlation estimation
was a novel finding that we felt deserved further, and more rigorous, study. Its
implementation was based on findings that changing the opacities of scatterplot
points could bias estimates of means \cite{hong_2022}, and on limited evidence for
the perception of correlation being based on the perceived width of a probability
distribution represented by the arrangement of scatterplot points. I did not foresee
the decay function, being novel, having a greater effect on correlation estimation
than size. Once evidence had been found that changing the opacity of points in 
scatterplots could have effects on correlation estimation, in opposition to previous
research \cite{rensink_2012, rensink_2014}, the door was opened for a rigorous
investigation into how this worked and how it could be used systematically to
correct for the historic underestimation bias.

# Introduction {#introduction-adjusting-opacity}

Findings from the pilot study suggest that changing the opacities of points
in scatterplots is able to change participants' estimates of the correlation being
displayed. The effect found in that study was too small to make a real difference
with regards to correcting for the underestimation bias, and does not provide information
on *how* changing opacity might change the percept (only that *it does*). Failing
to understand the ways in which opacity is able to change the perception of correlation
prevents future work from tuning what was a small effect in the pilot study
into something with real potential for producing more perceptually optimised
scatterplots.

## Overview {#overview-adjusting-opacity}

In two experiments, the opacities of points in scatterplots were manipulated while 
participants were asked to make judgements of correlation. In the first, point opacity
is changed in a uniform manner, while in the second, point opacity is systematically
altered as a function of the size of a particular point's residual. By comparing
participants' performance on a correlation estimation task for data-identical
scatterplots that vary only in the opacities of their points, it is demonstrated that;
lower global point opacity results in greater errors in the estimation of positive
correlation (experiment 1); and lowering point opacity as a function of the size
of a point's residual is able to bias estimates of positive correlation upwards to 
partially correct for a historic underestimation bias.

# Related Work {#related-work-adjusting-opacity}

## Transparency, Contrast, Opacity, and Formal Definitions

The original paper that this chapter is based on is titled "The Effects of Contrast
on Correlation Perception in Scatterplots". In response to reviewer comments to
the paper that forms Chapter 6, the term "contrast" was changed to "opacity". In order
to maintain consistency throughout this thesis, the more up-to-date wording (opacity)
is used, although I discuss the issue of terminology below.

Adjusting the opacity of points in scatterplots is an established technique
used to address issues of overplotting or clutter \cite{bertini_2004, matejka_2015},
in which scatterplots with large numbers of data points suffer from visibility issues
caused by excessive point density. Lowering the opacity of all scatterplot points
using alpha blending \cite{few_2008} addresses this, and makes data trends and 
distributions easier to see and interpret for the reader. @fig-overplotting-examples
demonstrates the impact of lowering global point opacity in a scatterplot with
a very high number of data points.

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.4
#| fig-cap: Adjusting point opacity to address overplotting. Contrast between the points and the background is full (alpha = 1, full opacity points, Left) or low (alpha = .1, low opacity points, Right). The dataset used has 20,000 points.
 
set.seed(123)

data <- data.frame(x = c(rnorm(10000, mean = -1),
                         rnorm(10000, mean = 1)),
                   y = rnorm(20000))

ggplot(data, aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

Lowering opacity leads to a reduction of the contrast of isolated points with the background,
and for regions with overlapping points, colour intensities are summed. The stimuli
used in the experiments throughout this chapter had 128 small points, meaning
the majority of points were clearly visible at all times. For this reason, the effects
of point overlap were not taken into account when designing and analysing the experiments
described here. Due to this, the approach to opacity described in this chapter would
not be useful when dealing with much larger datasets where clutter becomes an issue.

The **ggplot2** \cite{wickham_2016} package was used in R to create stimuli for 
this experiment. This package uses an alpha parameter to set point opacity. Alpha
here refers to the level of linear interpolation \cite{stone_2008} between foreground
and background pixel values; alpha values of 0 (full transparency) and 1 (full opacity)
result in no interpolation and rendering of either the background or foreground
pixel values respectively. Alpha values between 0 and 1 correspond to different
ratios of interpolation, and are illustrated in @fig-alpha-examples.

```{r}
#| label: fig-alpha-examples
#| include: true
#| fig-asp:  0.2
#| fig-cap: The relationship between alpha values and rendered point opacity. Higher alpha values result in greater contrast between the foreground (scatterplot point) and background. When alpha = 0, the foreground is ignored and the background is rendered.

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)
ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 14, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11)) +
rm(x1,y1,df)
```

Definitions of contrast, opacity, and transparency are fuzzy. Often, different 
works will use the terms interchangeably. As mentioned above, I initially
elected to use the term "contrast", given the fundamentality of contrast as a
feature of human visual perception \cite{ginsburg_2003}, however later reviewer
comments prompted the change to "opacity". Nevertheless, we can consider *opacity*
as it is used here to be shorthand for "the contrast between foreground and
background objects", as visually, these concepts are the same. There are numerous
psychophysical definitions of perceived contrast \cite{zuffi_2007} based on what
is being presented, for example, models that take into account visibility
limits (CIELAB lightness), or contrast in periodic patterns such as sinusoidal
gratings (Michelson's contrast). The common thread running through these definitions
is the use of a ratio between target and background luminances. The experiments
described here take place online, with participants completing experiments
on their personal laptop or desktop computers. Due to this, the experimenter
has no control over the exact luminances of stimuli, only over the relative
luminance between targets (scatterplot points) and backgrounds. Given my interest
in relative differences in correlation perception averaged over a series of 
180 single-plot trials, this lack of control over the exact nature of the stimuli
was not problematic. It does mean that reporting the exact luminance values would be
pointless however, so where a value for opacity is referred to in this chapter and beyond, 
it is the alpha value specified by **ggplot2**.

## Effects of Point Opacity on Correlation Estimation

Despite the popularity of adjusting opacity to address overplotting issues,
little investigation had taken place into the effects of reducing point opacity
on people's perceptions of correlation. In 2012, Rensink \cite{rensink_2012} found
correlation perception to be invariant to changes in point opacity, although this
work took place with only small sample sizes (n = 12), and using only bisection/JND
methodologies.

Changing the contrast between a stimulus and its background (lowering its opacity)
effectively reduces the strength of its signal. A likely consequence of this is 
greater levels of uncertainty in aspects of that stimulus, for example, the locations
of points in scatterplots. Consequently, one might anticipate that increased uncertainty
could lead to altered perceptions of correlation and/or the presence of greater
levels of noise in correlation estimates due to effects on the perceived position
of points within a scatterplot point cloud. While there is evidence \cite{wehrhahn_1990}
that perception of stimulus position becomes exponentially worse as contrast
is reduced (as measured by vernier acuity tasks), this is only true for a narrow
range of low contrast stimuli just above the detection threshold. For stimuli
that feature higher contrasts between them and their backgrounds, vernier acuity
appears robust to such changes. Nevertheless, there is evidence that
other perceptual estimates become more uncertain with reduced contrast, such as speed
perception \cite{champion_2017}. With this in mind, I argue that the effects of stimulus opacity
on perceived correlation in scatterplots warrants further investigation.

In 2022 \cite{hong_2022}, Hong et al. used point opacity and size to encode a third
variable in trivariate scatterplots, while asking participants to judge the average
position of all the points displayed. It was found that participants' estimates
of average point position were biased towards areas of larger or darker points; this
was termed the *weighted average illusion*. Together with evidence that darker (more opaque)
and larger points are more salient \cite{healey_2011}, the implication I took from this
work was that I could use point opacity to systematically lower the salience 
of the points representing the widest parts of the probability distribution; if
participants promptly perceived a narrower distribution, I expected this to be able
to (at least partially) correct for the underestimation bias.

One way to correct for an underestimation of correlation in scatterplots would be to simply remove
outer data points until correlation perception is aligned with the actual correlation
value. However, this would necessitate hiding data and thus changing the information
presented to the viewer. An alternative approach is to manipulate the opacity of
only some of the points; it would seem most sensible to do so for the points
that are more extreme relative to the underlying regression line. In the present
study these questions are explored in two online experiments with large sample sizes.
In the first, the effects of point opacity over the entire scatterplot on correlation estimates
is investigated. In the second experiment we examine how changing contrast as a function 
of distance to the regression line affects perceived correlation.
To pre-empt our results we find clear effects of both manipulations.

# General Methods {#shared-methods-adjusting-opacity}

The experiments described in this chapter share multiple aspects of their procedures.
Both experiments were built using PsychoPy \cite{peirce_2019} and are hosted on pavlovia.org.
Both use 1-factor, 4-level designs. Ethical approval for both experiments was granted
by the University of Manchester's Computer Science Departmental Panel (Ref:
2022-14660-24397). In each experiment, participants were shown the respective
Participant Information Sheet (henceforth PIS) and provided consent through key presses
in response to consent statements. Participants were asked to provide their age and 
gender identity, after which they completed the 5-item Subjective Graph Literacy
test described by Garcia-Retamero et al. \cite{garcia_2016} and discussed in 
Section \ref{graph-literacy-lit-review} of the literature review. Early piloting
with a graduate student in humanities suggested the potential for participants to
be unfamiliar with the visual nature of different values of Pearson's *r*. Participants
were therefore shown examples of *r* = 0.2, 0.5, 0.8, and 0.95 (see @fig-training-slide-adjusting-opacity);
a discussion of the effects of this training is provided in Section \ref{training-adjusting-opacity}.
Participants were given two practice trials to familiarise themselves with the
response slider.

```{r}
#| label: fig-training-slide-adjusting-opacity
#| include: true
#| fig-cap: Participants viewed these plots for at least eight seconds before being allowed to continue to the practice trials.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example-plots.png", dpi = NA)
```

```{r}
#| label: fig-mask-adjusting-opacity
#| include: true
#| fig-cap: An example of a visual mask displayed for 2.5 seconds before each experimental trial.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/visual_mask.png")
```

Each trial was preceded by text that either told the participant:

 - Please look at the following plot and use the slider to estimate the correlation (n = 180).
 - Please IGNORE the correlation displayed and set the slider to 1 (n = 3) or 0 (n = 3).
 
 The latter instructions were attention checks, and were formatted with red text 
 to increase their visibility. Each experimental trial was preceded by a visual
 mask (see @fig-mask-adjusting-opacity) that was displayed for 2.5 seconds. Participants
 were instructed to make their judgements as quickly and accurately as possible,
 but there was no time limit per trial. Both experiments described here use a
 fully repeated-measures, within-participants design. Participants saw all 180 experimental
 items, corresponding to ~27,000 individual judgements per experiment, in a fully
 randomised order.
 
 Both experiments were conducted according to principles of open and reproducible
 research. All data and analysis code for the origin paper is available on GitHub [^1].
 Experiment 1 [^2] and 2 [^3] are hosted on Pavlovia.org, while the Open Science
 Framework hosts pre-registrations [^4]. It is important to note at this point
 that experiment 2 was conducted prior to experiment 1; when the original paper
 was written, the order of presentation of the experiments was swapped to make the
 narrative more cohesive. I preserve this order in the present chapter.
 
 [^1]: https://github.com/gjpstrain/contrast_and_scatterplots
 [^2]: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments
 [^3]: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent
 [^4]: Experiment 1 - https://osf.io/tuexh. Experiment 2 - https://osf.io/6f5ev
 
# Experiment 1: Uniform Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e1}

Previous literature had described correlation perception as being resistant
to changes in opacity \cite{rensink_2012, rensink_2014}. Findings from the pre-study
described above provided evidence against this conclusion, so before proceeding
with the fine-tuning of the immature point opacity decay function, I felt that
gaining an understanding of how point opacity and correlation estimation interact
more generally was important. Owing to the robust effects of altering stimulus opacity on perception described 
above \cite{wehrhahn_1990, champion_2017}, it was hypothesised that:

 - H1: A greater spread of estimates of correlation for plots with lower global opacity
compared to higher opacity plots will be observed

## Method {#methods-adjusting-opacity-e1}

### Participants

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E1_uniform_adjustments_tidy_gender$M)`% male, `r printnum(E1_uniform_adjustments_tidy_gender$F)`
% female, and `r printnum(E1_uniform_adjustments_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E1_uniform_adjustments_tidy_age$mean)`
(*SD* = `r printnum(E1_uniform_adjustments_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E1_uniform_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$sd)`). The mean
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design

For each of the 45 *r* values, there were four versions of each plot corresponding
to the four levels of point opacity. Examples of each of these can be seen in
@fig-exp1-examples-chap4, demonstrated with an *r* value of 0.6.

```{r}
#| label: fig-exp1-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 1, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by ggplot.
#| fig-asp: 0.275

ggarrange(plot_example_function(data_with_resid, "Opacity = 0.25",
                                0.25, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.5",
                                0.5, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.75",
                                0.75, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 1",
                                1, 0.2, 7),
          nrow = 1)
```

## Analysis {#analysis-adjusting-opacity-e1}

```{r}
#| label: model-e1
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E1_uniform_adjustments_tidy)
```

```{r}
#| label: model-e1-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e1_lit_model <- add_fixed_effect(e1_model, "literacy", "E1_uniform_adjustments_tidy")
```

```{r}
#| label: model-e1-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model_cmpr <- comparison(e1_model)
```

```{r}
#| label: anova-results-e1

anova_results(e1_model, e1_model_cmpr)

# also do anova for literacy model

anova_results(e1_lit_model, e1_model)
```

To investigate the effects of opacity condition on participants' estimates
of correlation, a linear mixed effects model was built whereby opacity condition
is a predictor for the difference between objective *r* values for each plot
and participants' estimates of *r*. This model has random intercepts for 
items and participants. A likelihood ratio test revealed that the mode including
global opacity as a fixed effect explained significantly more variance than a 
null model ($\chi^2$(`r in_paren(e1_model.df)`) = `r printnum(e1_model.Chisq)`,
*p* `r printp(e1_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity condition, along with 95% confidence intervals.

```{r}
#| label: fig-e1-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 1. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error. The overestimation zone is included to facilitate comparison to later work.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

plotting_df <- as_tibble(emmeans(e1_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D"))) %>%
  mutate(opacity = recode(opacity, !!!labels_e1)) %>%
  rename("Condition" = "opacity")

ggplot(aes(x = Condition, y = emmean*-1), data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e1
#| include: true
#| tbl-cap: Contrasts between different levels of the opacity factor in experiment 1.

# assign slot, if it hasn't been assigned already

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

# make table df

contrasts_extract(e1_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e1)) %>%
  separate(Contrast, c(" ", "  "), sep = "-") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))
```

This effect was driven by significant difference between means of correlation 
estimation error between all conditions bar high and full opacity. Statistical
tests for contrasts were performed using the **emmeans** package \cite{lenth_2024}, 
and are shown in @tbl-contrasts-e1. To test whether the observed results could be
explained by difference in participants' levels of graph literacy, an additional
model was built. This model is identical to the experimental model, but also
includes graph literacy as a fixed effect. Including graph literacy as a fixed effect
explained no additional variance ($\chi^2$(`r in_paren(e1_lit_model.df)`) =
`r printp(e1_lit_model.Chisq)`, *p* `r printp(e1_lit_model.p, add_equals = TRUE)`),
indicating that the differences observed in participants' correlation estimation 
performance were not as a result of differences in levels of graph literacy.

```{r}
#| label: tbl-efs-e1
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (left) and summary statistics (right) for levels of the opacity factor in experiment 1. Each effect size is compared to the reference level, full contrast (alpha = 1).
#| tbl-subcap: ["A", "B"]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

options(knitr.kable.NA = "")

efs <- lme.dscore(e1_model, E1_uniform_adjustments_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Low Opacity (alpha = 0.25)",
                         "opacityB" = "Medium Opacity (alpha = 0.5)",
                         "opacityC" = "High Opacity (alpha = 0.75)",
                         "opacityD" = "Full Opacity (alpha = 1.0)")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

emmeans(e1_model, pairwise ~ "opacity")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Opacity = opacity,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Opacity" = recode(Opacity,
                         "A" = "Low Opacity (alpha = 0.25)",
                         "B" = "Medium Opacity\n(alpha = 0.5)",
                         "C" = "High Opacity\n(alpha = 0.75)",
                         "D" = "Full Opacity\n(alpha = 1.0)")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

A function from the now archived **EMAtools** package \cite{ematools} was used to 
calculate an approximation of Cohen's *d* between the reference level (full contrast, 
alpha = 1.0) and each other level of the opacity factor. These statistics can be
seen in @tbl-efs-e1. The largest effect size observed (*d* ~ 0.16) is between the 
low and full opacity conditions, and is small. This was unsurprising given the lack 
of previously reported effects on correlation perception of global point opacity
\cite{rensink_2012}.

## Discussion {#discussion-adjusting-opacity-e1}

The hypothesis, that there would be a greater spread of correlation estimates
for plots with lower global opacity compared to those with higher global opacity,
was not supported. As seen in @tbl-efs-e1 (right), standard errors for each opacity
factor are identical to 3 decimal places. Participants' errors in correlation 
estimation were significantly greater when the opacity of all scatterplot points
was lower compared when it was higher. This held true up until alpha was set to
0.75, implying a threshold around this value past which there is little variation
in the perception of opacity, at least as far as it is associated with correlation
estimation. This lack of significant difference in correlation estimation between
the two highest global opacity conditions is congruent with the logarithmic nature
of contrast/brightness perception \cite{fechner_1948, varshney_2013}; despite
there being equaal linear distance between the opacity values used, the perceptual
distance between them was clearly non-linear.

As mentioned previously, Rensink \cite{rensink_2012, rensink_2014} present the only
other account of experiments that directly test correlation perception as it pertains
to the opacities of scatterplot points, and report no difference in either bias (error)
or variability (spread) in correlation perception regarding point opacity manipulations.
In comparison, the results observed here do report an effect. This effect may be
explained by differences in experimental power, as it is a small effect, although
I argue that methodological differences may have also played a small role. Given
the small effect size (Cohen's *d* = 0.16), the small sample in Rensink (2014) \cite{rensink_2014}
may have been insufficiently powered. With the large sample size in the present
work (n = 150), evidence for an effect has been found. The experimental methodology 
utilised here is more representative of the use of scatterplots in the wild,
and is therefore more suited to informing design as opposed to investigating the
mathematical relationship between real and perceived correlation. While the effect is small,
it demonstrates definitively that differences in the opacities of scatterplot points
*can* affect estimates of correlation in positively correlated scatterplots.
From the results it is unclear why lowering global opacity causes greater errors in
correlation estimation while causing no difference in spread.

I suggest that correlation perception functions similarly to speed perception
\cite{champion_2017} with regards to changes in the contrast between foreground
targets (in this case scatterplot points) and the background; the greater spatial
uncertainty brought on by reduced point opacity, while not eliciting greater spread
in correlation estimates, might be responsible for the effects observed via an
increase in the perceived width of the probability distribution displayed by the scatterplot

From the results it is clear that a scatterplot optimised for correlation perception
should have contrast between the foreground (points) and background in a range
corresponding to alpha values of between 0.75 and 1. That significant differences
in correlation estimation between data-identical scatterplots with different
global point opacities however, suggests that this effect may be leveraged
to further improve participants' performances on a correlation estimation task.

# Experiment 2: Spatially-Dependent Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e2}

Experiment 1 found that point opacity in positively correlated scatterplots has
an effect on the perception of correlation such that those scatterplots with higher
levels of global point opacity are rated as being more strongly correlated. Given
this finding, the question arises of whether additional changes in correlation 
perception may be observed as a function of the spatial arrangement of point
opacity. With this in mind, it was hypothesised that:

 - H1: the non-linear decay parameter in which point opacity falls with
 residual distance will result in lower mean errors in correlation estimation
 compared to linear decay and full global opacity conditions.
 - H2: the use of the inverted non-linear decay parameter, in which point opacity
 becomes greater with residual distance, will result in higher mean errors in correlation
 estimation than for all other conditions.

## Methods {#methods-adjusting-opacity-e2}

### Participants {#participants-adjusting-opacity-e2}

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 7 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E2_spatially_dependent_tidy_gender$M)`% male, `r printnum(E2_spatially_dependent_tidy_gender$F)`
% female, and `r printnum(E2_spatially_dependent_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E2_spatially_dependent_tidy_age$mean)`
(*SD* = `r printnum(E2_spatially_dependent_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E2_spatially_dependent_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design {#design-adjusting-opacity-e2}

For each of the 45 *r* values in experiment 2, there were four versions 
of each plot corresponding to the three levels of point opacity decay function and 
the baseline global full opacity condition. Examples of each of these can be 
seen in @fig-exp2-examples-chap4, demonstrated with an *r* value of 0.6. Given the
shape of the underestimation curve found in previous work (see related work chap
3 or something), intuition suggested employing a symmetrically opposing curve 
(see the non-linear decay curve in @fig-opposing-curve) to relate point opacity to residuals.

```{r}
#| label: fig-opposing-curve
#| include: true
#| fig-cap: Using an \textit{r} value of 0.2 to demonstrate the relationship between the size of a point's residual and the alpha value (opacity) rendered.
#| fig-asp: 1

my_desired_r = 0.2
  
my_sample_size = 128
mean_variable_1 = 0
sd_variable_1 = 1
mean_variable_2 = 0
sd_variable_2 = 1

mu <- c(mean_variable_1, mean_variable_2) 

myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)

mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 

corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))

corr_model <- lm(V2 ~ V1, data = corr_data)

my_residuals <- abs(residuals(corr_model))

data_with_resid <- round(cbind(corr_data, my_residuals), 2)

slopes_opp <- data_with_resid %>%
  mutate(linear_decay = ((-my_residuals/2.6) + 1)) %>%
  mutate(inverted_decay = 1-(0.25)^my_residuals) %>%
  mutate(non_linear_decay = (1 + (0.25)^ my_residuals)-1) %>%
  pivot_longer(cols = ends_with("ay"),
               names_to = c("decay"),
               values_to = "transformed_residual")

slopes_opp %>%
  mutate(decay = str_replace_all(decay, pattern = "_", replacement = " ")) %>%
  mutate(decay = str_to_title(decay)) %>%
  group_by(decay) %>%
  ggplot(aes(x = my_residuals, y = transformed_residual)) +
  geom_textline(aes(colour = decay,label = decay, size = 6, hjust = 0.65, vjust = -0.3)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits = c(0,2.6)) +
  labs(x = "Residual",
       y = "Alpha (opacity)") +
  theme_ggdist() +
  theme(legend.title = element_blank(),
        legend.position = "none",
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  coord_cartesian(xlim = c(0,2.6), ylim = c(0,1)) +
  scale_colour_manual(values = c("black", "black", "black"))
```


Equation 4.1 was used to non-linearly map residuals to ggplot alpha values, where R is the residual
of the point. 0.25 was chosen as the value of *b*, as it was felt at the time
that this rendered plots that maintained point visibility while also allowing
a large enough point opacity range that, if an effect was present, it was 
likely to be found.

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

```{r}
#| label: fig-exp2-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 2, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by ggplot.
#| fig-asp: 0.275

ggarrange(plot_example_function(slopes, "Non-Linear Decay",
                                (1-slopes$slope_0.25), 0.2, 7),
          plot_example_function(slopes, "Linear Decay",
                                (1-slopes$slope_linear), 0.2, 7),
          plot_example_function(slopes, "Inverted Decay",
                                (1-slopes$slope_inverted), 0.2, 7),
          plot_example_function(slopes, "Full Opacity",
                                1, 0.2, 7),
          nrow = 1)
```

## Analysis {#analysis-adjusting-opacity-e2}

```{r}
#| label: model-e2
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e2_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E2_spatially_dependent_tidy)
```

```{r}
#| label: model-e2-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e2_lit_model <- add_fixed_effect(e2_model, "literacy", "E2_spatially_dependent_tidy")
```

```{r}
#| label: model-e2-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e2_model_cmpr <- comparison(e2_model)
```

```{r}
#| label: anova-results-e2

anova_results(e2_model, e2_model_cmpr)

# also do anova for literacy model

anova_results(e2_lit_model, e2_model)
```

To investigate the effects of the opacity decay functions on participants' estimates
of correlation, a linear mixed effects model was built whereby decay function condition
is a predictor for the difference between objective *r* values for each plot
and participants' estimates of *r*. This model has random intercepts for 
items and participants. A likelihood ratio test revealed that the mode including
global opacity as a fixed effect explained significantly more variance than a 
null model ($\chi^2$(`r in_paren(e2_model.df)`) = `r printnum(e2_model.Chisq)`,
*p* `r printp(e2_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity decay function condition,
along with 95% confidence intervals.

```{r}
#| label: fig-e2-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 2. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

plotting_df <- as_tibble(emmeans(e2_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D"))) %>%
  mutate(opacity = recode(opacity, !!!labels_e2)) %>%
  rename("Condition" = "opacity")

ggplot(aes(x = Condition, y = emmean*-1), data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e2
#| include: true
#| tbl-cap: Contrasts between different levels of opacity decay function in experiment 2.

# assign slot, if it hasn't been assigned already

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

# make table df

contrasts_extract(e2_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e2_regex)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 1), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))

```

The effect seen in experiment 2 was driven by significant differences in means of 
correlation estimation error between all levels of opacity decay function condition
bar full opacity and linear decay. Statistical testing for contrasts was performed
using the **emmeans** package \cite{lenth_2024}, and are shown in @tbl-contrasts-e2.
To test whether the observed results could be explained by differences in graph 
literacy, a model including participants' graph literacy scores as a fixed effect
was built. Including graph literacy as a fixed effect again explained no additional
variance ($\chi^2$(`r in_paren(e2_lit_model.df)`) = `r printp(e2_lit_model.Chisq)`,
*p* `r printp(e2_lit_model.p, add_equals = TRUE)`).

```{r}
#| label: tbl-efs-e2
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (left) and summary statistics (right) for the opacity decay function factor in experiment 2. Each effect size is compared to the reference level, full contrast (alpha = 1).
#| tbl-subcap: ["A", "B"]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

options(knitr.kable.NA = "")

efs <- lme.dscore(e2_model, E2_spatially_dependent_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Non-Linear Decay",
                         "opacityB" = "Linear Decay",
                         "opacityC" = "Inverted Decay",
                         "opacityD" = "Full Opacity")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

emmeans(e2_model, pairwise ~ "opacity")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Opacity = opacity,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Opacity" = recode(Opacity,
                         "A" = "Non-Linear Decay",
                         "B" = "Linear Decay",
                         "C" = "Inverted Decay",
                         "D" = "Full Opacity")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

Approximated Cohen's *d* effect sizes between the baseline (global full opacity)
and each other condition can be seen in @tbl-efs-e2. The largest effect size observed
(~0.23 between full opacity and inverted decay conditions) is small to moderate,
and the effect size between the baseline and the non-linear decay condition (~0.19) is small.
@fig-estimates-by-r illustrates the effects of each manipulation on participants'
correlation estimation performance separately for each value of *r* used. The dotted horizontal
line represents perfect estimation, and standard deviations of estimates are
provided by way of error bars. Participants still underestimated correlation in 
all conditions, although the use of the non-linear decay function biased participants'
estimates upwards to partially correct for the underestimation.

```{r}
#| label: fig-estimates-by-r
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by factor and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring the non-linear opacity decay function. Error bars show standard deviations of estimates.

plot_error_bars_function(E2_spatially_dependent_tidy %>%
                           mutate(grouping_var = fct_relevel(opacity,
                                                             c("A",
                                                               "B",
                                                               "C",
                                                               "D")))
                         , "opacity", "difference", labels_e2) +
  geom_hline(yintercept = 0, linetype = 2)
```

## Discussion {#discussion-adjusting-opacity-e2}

```{r}
#| label: model-e2-low-range
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

E2_low_range <- E2_spatially_dependent_tidy %>%
  filter(contrast == c("B", "D")) %>%
  filter(my_rs < 0.6)

e2_model_low_range <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E2_low_range)
```

```{r}
#| label: model-e2-low-range-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e2_model_comparison_low_range <- comparison(e2_model_low_range)
```

```{r}
#| label: anova-results-e2-low-range
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

anova_results(e2_model_low_range, e2_model_comparison_low_range)
```

Both hypotheses received support from experiment 2. Participants' errors in
correlation estimation were lowest when the non-linear decay function was used,
and were highest when scatterplots employed the non-linear inverted decay function.
There was no significant difference in correlation estimation errors between the 
linear decay function and full contrast conditions. This result was surprising, however
on closer inspection of the scatterplots in the linear decay function condition,
it is clear that the logarithmic nature of contrast perception \cite{fechner_1948,
varshney_2013} means there was little perceptual distance between points with
high opacity values (*alpha* > 0.75). This resulted in no significant perceived
differences between full opacity and linear decay function scatterplots. A similar
threshold for high opacity values was found in experiment 1. Selecting only lower *r*
values, those with naturally higher residuals (arbitrarily *r* < 0.6), still results
in no difference between correlation estimation errors for linear decay parameters
and full opacity conditions ($\chi^2$(`r in_paren(e2_model_low_range.df)`)
= `r printnum(e2_model_low_range.Chisq)`, *p* `r printp(e2_model_low_range.p, add_equals = TRUE)`).
Effect sizes were small, with the largest being between the baseline full opacity
and inverted non-linear decay conditions. This suggests that it is easier to induce
further bias in correlation estimates through a reduction in the salience of a 
point cloud's centre than it is to correct for the underestimation bias.



# General Discussion {#general-discussion-adjusting-opacity}

## Training {#training-adjusting-opacity}
