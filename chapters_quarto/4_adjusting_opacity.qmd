---
title: "adjusting_opacity"
output:
  format: 
    latex:
      
params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false      
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(),
                 dplyr::filter(),
                 lme4::lmer())

# define plotting labels now

labels_pilot_size <- c(S = "Small (52\\\\%)",
                       M = "Medium (100\\\\%)",
                       L = "Large (252\\\\%)")

labels_pilot_present <- c(N = "Absent",
                          Y = "Present")

labels_e1 <- c(A = "Low Opacity\n(alpha = 0.25)",
               B = "Medium Opacity\n(alpha = 0.5)",
               C = "High Opacity\n(alpha = 0.75)",
               D = "Full Opacity\n(alpha = 1.0)")

labels_e2 <- c(A = "Non-Linear\nDecay",
               B = "Linear\nDecay",
               C = "Inverted\nDecay",
               D = "Full\n Opacity")

labels_e2_regex <- c(
  "\\bA\\b" = "Non-Linear Decay",
  "\\bB\\b" = "Linear Decay",
  "\\bC\\b" = "Inverted Decay",
  "\\bD\\b" = "Full Opacity"
)

# read in shared_functions script

source("../shared_functions.R")

# prepare slopes for plotting examples

slopes <- prepare_slopes(0.6)
```

```{r}
#| label: load-data

# load experimental data

exp1_anon <- read_csv("../data/exp_1_data.csv")
exp2_anon <- read_csv("../data/exp_2_data.csv") # all experimental data lives in this folder
```

```{r}
#| label: retrieve-cached-models-chap4

# load in cached models

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data-chap4

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  # extract SGL test scores
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant, literacy)
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images,
                              pattern = "A",
                              replacement = "-A")) %>%
  mutate(images = str_replace(images,
                              pattern = "B",
                              replacement = "-B")) %>%
  mutate(images = str_replace(images,
                              pattern = "C",
                              replacement = "-C")) %>%
  mutate(images = str_replace(images,
                              pattern = "D",
                              replacement = "-D")) %>%
  separate(images, c("item", "contrast"), sep = "-") %>%
  mutate(contrast = str_replace(contrast,
                                pattern = ".png",
                                replacement = "")) %>%
  mutate(item = str_replace(item,
                            pattern = "all_plots/",
                            replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  dplyr::select(c("participant",
                  "item",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "trials.thisN",
                  "session")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  full_join(demographics, by = "participant") %>%
  mutate(across(c("item",
                  "contrast",
                  "half"),
                as_factor)) %>%
  dplyr::select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(contrast,
                                c('D', 'C', 'B','A'))) %>%
  rename("opacity" = "contrast") %>%       # contrast was an earlier term, so is                                                                        
  assign(paste0(unique(anon_file$expName), # changed here for consistency
                "_tidy"),
           value = .,
         envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

walk(list(exp1_anon,
          exp2_anon),
     wrangle)

# experiments were incorrectly named, so rename them

E1_uniform_adjustments_tidy <- E3_full_exp_tidy
E2_spatially_dependent_tidy <- E2_full_exp_tidy

# write wrangled csvs if they do not currently exist

if (!file.exists(csv_wrangled <- "../data/wrangled_data/chap4_exp1_wrangled.csv")) {
  
  E1_uniform_adjustments_tidy %>%
    rename(condition = opacity) %>%
    mutate(condition = paste0(condition,
                              "_exp1")) %>%
    write_csv("../data/wrangled_data/chap4_exp1_wrangled.csv")
  
}

if (!file.exists(csv_wrangled <- "../data/wrangled_data/chap4_exp2_wrangled.csv")) {
  
  E2_spatially_dependent_tidy %>%
    rename(condition = opacity) %>%
    mutate(condition = paste0(condition,
                              "_exp2")) %>%
    write_csv("../data/wrangled_data/chap4_exp2_wrangled.csv")
}

# remove incorrectly named and anon dfs from environment

rm(E3_full_exp_tidy,
   E2_full_exp_tidy,
   exp1_anon,
   exp2_anon,
   wrangle) # wrangle function removed

# extract age and gender information using functions from shared_functions.R

extract_age(E1_uniform_adjustments_tidy)

extract_gender(E1_uniform_adjustments_tidy)

extract_literacy(E1_uniform_adjustments_tidy)

extract_age(E2_spatially_dependent_tidy)

extract_gender(E2_spatially_dependent_tidy)

extract_literacy(E2_spatially_dependent_tidy)
```

# Abstract {#abstract-adjusting-opacity}

Scatterplots are common data visualisations utilised for communication with
experts and lay people alike. Despite being widely studied, people tend to
underestimate the level of correlation displayed in them. The weight of evidence
points toward changes in the opacities of scatterplot points being unable to
change perceptions of correlation, however this was not tested rigorously using
systematic adjustments. Drawing on evidence that the shape of a scatterplot's
point cloud may drive correlation perception, I conducted exploratory work
addressing this underestimation bias. In two experiments (total *N* = 300),
evidence is provided that changing the opacities of scatterplot points *can*
have small effects on participants' performance on a correlation estimation
task. The systematic adjustment of point opacity as a function of residual
magnitude is able to alter estimates sufficiently to correct for the
underestimation bias. In this chapter, I also present an early pilot study that
was ultimately not included in any published works.

# Preface: Learning From an Early Pilot Study {#pilot-study}

The research proposal that kickstarted this project in 2021 set out a plan to
investigate the perception of correlation in scatterplots as a function of
screen size. This proposal is included in the supplemental materials. This was
prompted by recent research demonstrating consistent perceptual biases in
scatterplots due to geometric scaling \cite{wei_2020}, the growing prevalence of
data visualisations in lay people's daily lives due to the COVID-19 pandemic,
and the increasing adoption of wearable devices \cite{shandhi_2024}. The first
experiment conducted therefore examined how perceptions of correlation changed
according to the size of a scatterplot. Additionally, a very early version of
the opacity decay condition from Experiment 2 was included, however the
implementation of this condition was immature. In Experiment 2 onwards, if a
scatterplot point resided in a particular place on a scatterplot, it would
always have the same opacity or size. In the pilot study, the code that set the
opacity of each point always scaled the opacity values such that the point with
the highest residual had the lowest possible opacity, and vice versa, resulting
in the plots seen in @fig-pilot-study-examples.

```{r}
#| label: fig-pilot-study-examples
#| include: true
#| fig-cap: Examples of the experimental stimuli used in the pilot study (opacity decay factor). On the left, the opacity decay function is visible. Note the linear scaling used.

# include pre-rendered graphics

knitr::include_graphics(path = "../supplied_graphics/pilot_examples.png")
```

This provided no consistency between different experimental stimuli, making it
difficult to comment on the effects of changing levels of opacity in various
parts of a plot on correlation estimation. This was later addressed by
hardcoding residual magnitude to a specific value of opacity or size. The pilot
also suffered extensively from poor data quality. Of the 260 participants
tested, data from only 118 was included in the final analyses due to failed
attention checks. It is for this reason that the pre-screen requirements
detailed in Section \ref{recruitment} were implemented.

Participants viewed 180 experimental plots in a 3x2 factorial design. The first
independent variable, plot size, had three levels, 63%, 100%, and 252% scales.
The second was the presence or absence of the opacity decay function (see
@fig-pilot-study-examples). I aimed to recruit 150 participants, but stopped
after 118 due to ongoing data quality issues. Nevertheless, the results provided
were crucial in informing the future direction of the research project. I
present these results in brief below.

## Pilot Study: Results

```{r}
#| label: pilot-study-wrangle

# small wrangling script for pilot study

pilot_data <- read_csv("../data/pilot_data.csv") %>%
  mutate(plots_with_labels = str_replace(plots_with_labels,
                                         pattern = "S",
                                         replacement = "-S-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels,
                                         pattern = "M",
                                         replacement = "-M-")) %>%
  mutate(plots_with_labels = str_replace(plots_with_labels,
                                         pattern = "L",
                                         replacement = "-L-")) %>%
  separate(plots_with_labels, c("item", "size", "present"),
           sep = "-") %>%
  mutate(present = str_replace(present,
                               pattern = ".png",
                               replacement = "")) %>%
  mutate(item = str_replace(item,
                            pattern = "all_plots/",
                            replacement = "")) %>%
  dplyr::select(c("item",
                  "size", 
                  "present",
                  "participant",
                  "unique_item_no",
                  "my_rs",
                  "slider.response")) %>%
  filter(unique_item_no < 181) %>%
  mutate(size = as.factor(size)) %>%
  mutate(present = as.factor(present)) %>%
  mutate(difference = my_rs - slider.response) %>%
  filter(!is.na(difference))

# set contrasts for statistical modelling

contrasts(pilot_data$size) = contr.sum(levels(pilot_data$size))
contrasts(pilot_data$present) = contr.sum(levels(pilot_data$present))

```

```{r}
#| label: pilot-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# use buildmer to create model

pilot_model <- buildmer(difference ~ size * present + 
                          (1 + size * present | participant) +
                          (1 + size * present | item),
                        data = pilot_data)

```

```{r}
#| label: pilot-null-model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# use comparison() function from shared_functions.R to remove fixed effects
# for null model

pilot_null <- comparison(pilot_model)
```

```{r}
#| label: anova-results-pilot

# compare experimental and null using function from shared_functions.R
# this function outputs model statistics to the global environment

anova_results(pilot_model, pilot_null)
```

To investigate the effects of plot size and the presence or absence of an
opacity decay manipulation on participants' estimates of correlation, a linear
mixed effects model was built whereby participants' errors in correlation
estimation were predicted by plot size and the presence or absence of the
opacity decay function. This model features random intercepts for participants
and items, as well as random slopes for both participants and items relevant to
the presence or absence of the opacity decay function. A likelihood ratio test
between the experimental model and a null model with the fixed effects removed
revealed that the experimental model explained significantly more variance than
the null ($\chi^2$(`r in_paren(pilot_model.df)`) =
`r printnum(pilot_model.Chisq)`, *p*
`r printp(pilot_model.p, add_equals = TRUE)`). There was no interaction between
plot size and the presence or absence of the opacity decay function. The
`emmeans` \cite{lenth_2024} package was used to explore estimated marginal means
(see @tbl-emmeans-pilot) and contrasts (see @tbl-contrasts-pilot) separately for
each condition.

```{r}
#| label: tbl-emmeans-pilot
#| include: true
#| tbl-cap: Estimated Marginal Means (EMMs) of correlation estimation error for plot size (a) and the presence of the opacity decay function (b).
#| tbl-subcap: ["EMMs for plot size factor.", "EMMs for presence or absence of opacity decay function."]
#| layout-ncol: 2

# reassign model if needed

if (class(pilot_model) == "buildmer") pilot_model <- pilot_model@model

# use emmeans() to output pairwise comparisons for model

emmeans(pilot_model, pairwise ~ "size")[[1]] %>%
  as_tibble() %>%
  select(-c(df,
            asymp.LCL,
            asymp.UCL)) %>%
  rename(Size = size,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Size" = recode(Size,
                         "L" = "Large (252%)",
                         "M" = "Medium (100%)",
                         "S" = "Small (62%)")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

# as above but for presence factor

emmeans(pilot_model, pairwise ~ "present")[[1]] %>%
  as_tibble() %>%
  select(-c(df,
            asymp.LCL,
            asymp.UCL)) %>%
  rename(Decay = present,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Decay" = recode(Decay,
                         "N" = "Absent",
                         "Y" = "Present")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))

```

```{r}
#| label: tbl-contrasts-pilot
#| include: true
#| tbl-cap: Contrasts between levels of the size factor (a) and opacity decay factor (b).
#| tbl-subcap: ["Contrasts for size decay factor.", "Contrasts for opacity decay factor."]
#| layout-ncol: 2

# use contrasts_extract function from shared_functions.R to get experimental
# contrasts, then use kbl() to create tables for each
# chunk options specify a pair of side-by-side tables

contrasts_extract(pilot_model, "size") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_size)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "),
           sep = " \\| ") %>%
  kbl(booktabs = TRUE,
      digits = c(0, 2, 2),
      escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))

contrasts_extract(pilot_model, "present") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_pilot_present)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "),
           sep = " \\| ") %>%
  kbl(booktabs = TRUE,
      digits = c(0, 2, 2),
      escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

## Pilot Study: Discussion

For the factor of plot size, the effect observed was driven by significant
differences in correlation estimation error between large and small plots and
between medium and small plots. There were no significant differences in
correlation estimation performance between large and medium plots. Participants
estimated more accurately when the plot was large and when the decay function
was present. Participants still underestimated correlation in all conditions.
The finding that estimation error was lower for larger plots is in line with
previous evidence that geometrically scaling a scatterplot up can increase
perceptions of the strength of the correlation displayed \cite{wei_2020}.
Despite the statistical significance of this finding, we elected at this point
to abandon the plot size factor due to the extremely small effect (see
@tbl-emmeans-pilot) and lack of novelty compared to the effects of the opacity
decay function.

The impact of even an immature point opacity decay function on correlation
estimation was a novel finding that I felt deserved further, and more rigorous,
study. Its implementation was based on findings that changing the opacities of
scatterplot points could bias estimates of means \cite{hong_2022}, and on
limited evidence for the perception of correlation being based on the perceived
width of a probability distribution represented by the arrangement of
scatterplot points. I did not foresee the decay function, being novel, having a
greater effect on correlation estimation than the established effect of plot
size. Once evidence had been found that changing the opacity of points in
scatterplots could have effects on correlation estimation, in opposition to
previous research \cite{rensink_2012, rensink_2014}, the door was opened for a
rigorous investigation into how this worked and how it could be used
systematically to correct for the historic underestimation bias.

# Introduction {#introduction-adjusting-opacity}

Findings from the pilot study suggest that changing the opacities of points in
scatterplots is able to change participants' estimates of the correlation being
displayed. The effect found in that study was too small to make a real
difference with regards to correcting for the underestimation bias, and does not
provide information on *how* changing opacity might change the percept (only
that *it does*). Failing to understand the ways in which opacity is able to
change the perception of correlation prevents future work from tuning what was a
small effect in the pilot study into something with real potential for producing
more perceptually optimised scatterplots.

## Overview {#overview-adjusting-opacity}

In two experiments, the opacities of points in scatterplots were manipulated
while participants were asked to make judgements of correlation. In the first,
point opacity is changed in a uniform manner, while in the second, point opacity
is systematically altered as a function of the size of a particular point's
residual. By comparing participants' performance on a correlation estimation
task for data-identical scatterplots that vary only in the opacities of their
points, it is demonstrated that; lower global point opacity results in greater
errors in the estimation of positive correlation (Experiment 1); and lowering
point opacity as a function of the size of a point's residual is able to bias
estimates of positive correlation upwards to partially correct for a historic
underestimation bias (Experiment 2).

# Related Work {#related-work-adjusting-opacity}

## Transparency, Contrast, Opacity, and Formal Definitions

The original paper that this chapter is based on is titled "The Effects of
Contrast on Correlation Perception in Scatterplots". In response to reviewer
comments to the paper that forms \chap{chap:interactions_opacity_size}, the term
"contrast" was changed to "opacity". In order to maintain consistency throughout
this thesis, the more up-to-date wording (opacity) is used, although I discuss
the issue of terminology below.

Adjusting the opacity of points in scatterplots is an established technique used
to address issues of overplotting or clutter \cite{bertini_2004, matejka_2015},
in which scatterplots with large numbers of data points suffer from visibility
issues caused by excessive point density. Lowering the opacity of all
scatterplot points using alpha blending \cite{few_2008} addresses this, and
makes data trends and distributions easier to see and interpret for the reader.
@fig-overplotting-examples demonstrates the impact of lowering global point
opacity in a scatterplot with a very high number of data points.

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.4
#| fig-cap: Adjusting point opacity to address overplotting. Contrast between the points and the background is full (alpha = 1, full opacity points, left) or low (alpha = .1, low opacity points, right). The dataset used has 20,000 points.

# set random seed for reproducibility
 
set.seed(123)

# create overplotting dataframe

data <- data.frame(x = c(rnorm(10000, mean = -1),
                         rnorm(10000, mean = 1)),
                   y = rnorm(20000))

# create plot using ggplot2

ggplot(data,
       aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

Lowering opacity leads to a reduction of the contrast of isolated points with
the background, and for regions with overlapping points, colour intensities are
summed. The stimuli used in the experiments throughout this chapter had 128
small points, meaning the majority of points were clearly visible at all times.
For this reason, the effects of point overlap were not taken into account when
designing and analysing the experiments described here. Due to this, the
approach to opacity described in this chapter would not be useful when dealing
with much larger datasets where clutter becomes an issue.

The `ggplot2` \cite{wickham_2016} package (version 3.4.1) was used in R to
create stimuli for this experiment. This package uses an alpha parameter to set
point opacity. Alpha here refers to the level of linear interpolation
\cite{stone_2008} between foreground and background pixel values; alpha values
of 0 (full transparency) and 1 (full opacity) result in no interpolation and
rendering of either the background or foreground pixel values respectively.
Alpha values between 0 and 1 correspond to different ratios of interpolation,
and are illustrated in @fig-alpha-examples.

```{r}
#| label: fig-alpha-examples
#| include: true
#| fig-asp:  0.2
#| fig-cap: The relationship between alpha values and rendered point opacity. Higher alpha values result in greater contrast between the foreground (scatterplot point) and background. When alpha = 0, the foreground is ignored and the background is rendered.

# create x and y dataframe for plotting

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)

# create plot

ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 14, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11))

# remove extraneous df and vectors

rm(x1,y1,df)
```

Definitions of contrast, opacity, and transparency are fuzzy. Often, different
works will use the terms interchangeably. As mentioned above, I initially
elected to use the term "contrast", given the fundamentality of contrast as a
feature of human visual perception \cite{ginsburg_2003}, however later reviewer
comments prompted the change to "opacity". Nevertheless, we can consider
*opacity* as it is used here when pertaining to scatterplot points on a white
background to be shorthand for "the contrast between foreground and background
objects", as visually, these concepts are the same. There are numerous
psychophysical definitions of perceived contrast \cite{zuffi_2007} based on what
is being presented, for example, models that take into account visibility limits
(CIELAB lightness), or contrast in periodic patterns such as sinusoidal gratings
(Michelson's contrast). The common thread running through these definitions is
the use of a ratio between target and background luminances. The experiments
described here take place online, with participants completing experiments on
their personal laptop or desktop computers. Due to this, the experimenter has no
control over the exact luminances of stimuli, only over the relative luminance
between targets (scatterplot points) and backgrounds. Given my interest in
relative differences in correlation perception averaged over a series of 180
single-plot trials, this lack of control over the exact nature of the stimuli
was not problematic. It does mean that reporting the exact luminance values
would be pointless however, so where a value for opacity is referred to in this
chapter and beyond, it is the alpha value specified by `ggplot2`.

## Effects of Point Opacity on Correlation Estimation {#point-opacity-chap4}

Despite the popularity of adjusting opacity to address overplotting issues,
little investigation had taken place into the effects of reducing point opacity
on people's perceptions of correlation. In 2012, Rensink \cite{rensink_2012}
found correlation perception to be invariant to changes in point opacity,
although this work took place with a small sample (*N* = 12), and using
bisection/JND methodologies (see Section \ref{scatterplots-corr-viz} in
\chap{chap:related_work}).

Changing the contrast between a stimulus and its background (lowering its
opacity) effectively reduces the strength of its signal. A likely consequence of
this is greater levels of uncertainty in aspects of that stimulus, for example,
the locations of points in scatterplots. Consequently, one might anticipate that
increased uncertainty could lead to altered perceptions of correlation and/or
the presence of greater levels of noise in correlation estimates due to effects
on the perceived position of points within a scatterplot point cloud. While
there is evidence \cite{wehrhahn_1990} that perception of stimulus position
becomes exponentially worse as contrast is reduced (as measured by vernier
acuity tasks), this is only true for a narrow range of low contrast stimuli just
above the detection threshold. For stimuli that feature higher contrasts between
them and their backgrounds, vernier acuity appears robust to such changes.
Nevertheless, there is evidence that other perceptual estimates become more
uncertain with reduced contrast, such as speed perception \cite{champion_2017}.
With this in mind, I argue that the effects of stimulus opacity on perceived
correlation in scatterplots warrants further investigation.

In 2022, Hong et al. \cite{hong_2022} used point opacity and size to encode a
third variable in trivariate scatterplots while asking participants to judge the
average position of all the points displayed. It was found that participants'
estimates of average point position were biased towards areas of larger or
darker points; this was termed the *weighted average illusion*. Together with
evidence that darker (more opaque) and larger points are more salient
\cite{healey_2012}, this work suggested that there was the scope to use point
opacity to systematically lower the salience of the points representing the
widest parts of the probability distribution; if participants perceived a
narrower distribution, one might expect this to be able to (at least partially)
correct for the underestimation bias.

One way to correct for an underestimation of correlation in scatterplots would
be to simply remove outer data points until correlation perception is aligned
with the actual correlation value. However, this would necessitate hiding data
and thus changing the information presented to the viewer. An alternative
approach is to manipulate the opacity of only some of the points; it would seem
most sensible to do so for the points that are more extreme relative to the
underlying regression line. In the present study these questions are explored in
two online experiments with large sample sizes. In the first, the effects of
point opacity over the entire scatterplot on correlation estimates is
investigated. The second experiment examines how changing contrast as a function
of distance to the regression line affects perceived correlation. To pre-empt
the results, clear effects of both manipulations are found.

# General Methods {#general-methods-adjusting-opacity}

The experiments described in this chapter share multiple aspects of their
procedures. Both experiments were built using PsychoPy \cite{peirce_2019} and
are hosted on Pavlovia.org. Both use 1-factor, 4-level designs. Ethical approval
for both experiments was granted by the University of Manchester's Computer
Science Departmental Panel (Ref: 2022-14660-24397). In each experiment,
participants were shown the respective Participant Information Sheet (henceforth
PIS) and provided consent through key presses in response to consent statements.
Participants were asked to provide their age and gender identity, after which
they completed the 5-item Subjective Graph Literacy test described by
Garcia-Retamero et al. \cite{garcia_2016} and discussed in Section
\ref{graph-literacy-related-work} of \chap{chap:related_work}. Early piloting
with a graduate student in humanities suggested the potential for participants
to be unfamiliar with the visual nature of different values of Pearson's *r*.
Participants were therefore shown examples of *r* = 0.2, 0.5, 0.8, and 0.95 (see
@fig-training-slide-adjusting-opacity); a discussion of the effects of this
training is provided in Section \ref{training-adjusting-opacity}. Participants
were given two practice trials to familiarise themselves with the response
slider.

```{r}
#| label: fig-training-slide-adjusting-opacity
#| include: true
#| fig-cap: Participants viewed these plots for at least eight seconds before being allowed to continue to the practice trials.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example-plots.png", dpi = NA)
```

```{r}
#| label: fig-mask-adjusting-opacity
#| include: true
#| fig-cap: An example of a visual mask displayed for 2.5 seconds before each experimental trial.

# find visual mask png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/visual_mask.png")
```

Each experimental trial was preceded by text that either told the participant:

-   Please look at the following plot and use the slider to estimate the
    correlation (*n* = 180).
-   Please IGNORE the correlation displayed and set the slider to 1 (*n* = 3) or
    0 (*n* = 3).

The latter instructions were attention checks, and were formatted with red text
to increase their visibility. Each experimental trial was preceded by a visual
mask (see @fig-mask-adjusting-opacity) that was displayed for 2.5 seconds.
Participants were instructed to make their judgements as quickly and accurately
as possible, but there was no time limit per trial. Both experiments described
here use a fully repeated-measures, within-participants design. All 150
participants saw all 180 experimental items, corresponding to \~27,000
individual judgements per experiment, in a fully randomised order.

## Open Research {#open-research-chap4}

Both experiments were conducted according to principles of open and reproducible
research \cite{ayris_2018}. All data and analysis code for the original paper
are available on GitHub [^1]. This repository also includes a Docker
implementation to reproduce the original computational environment the paper was
written in. Experiment 1 [^2] and 2 [^3] are hosted on Pavlovia.org, while the
Open Science Framework hosts pre-registrations [^4]. It is important to note at
this point that Experiment 2 was conducted prior to Experiment 1; when the
original paper was written, the order of presentation of the experiments was
swapped to make the narrative more cohesive. I preserve this order in the
present chapter.

[^1]: https://github.com/gjpstrain/contrast_and_scatterplots

[^2]: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments

[^3]: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent

[^4]: Experiment 1 - https://osf.io/tuexh. Experiment 2 - https://osf.io/6f5ev

# Experiment 1: Uniform Opacity Adjustments

## Introduction {#introduction-e1}

Previous literature had described correlation perception as being resistant to
changes in opacity \cite{rensink_2012, rensink_2014}. Findings from the
pre-study described above provided evidence against this conclusion, so before
proceeding with the fine-tuning of the immature point opacity decay function, I
felt that gaining an understanding of how point opacity and correlation
estimation interact more generally was important. Owing to the robust effects of
altering stimulus opacity on perception described above
\cite{wehrhahn_1990, champion_2017}, it was hypothesised that:

-   H1: A greater spread of estimates of correlation for plots with lower global
    opacity compared to higher opacity plots will be observed.

## Method {#method-e1}

### Participants {#participants-e1}

150 participants were recruited using the Prolific platform \cite{prolific}.
Normal or corrected-to-normal vision and English fluency were required.
Participants who had completed the pilot study were prevented from
participating. Data were collected from 158 participants. 8 failed more than 2
out of 6 attention check questions, and, as per the pre-registration, had their
submissions rejected from the study. The data from the remaining 150
participants were included in the full analysis
(`r printnum(E1_uniform_adjustments_tidy_gender$M)` male,
`r printnum(E1_uniform_adjustments_tidy_gender$F)` female, and
`r printnum(E1_uniform_adjustments_tidy_gender$NB)` non-binary). Participants'
mean age was `r printnum(E1_uniform_adjustments_tidy_age$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_age$sd)`). Mean graph literacy score was
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$sd)`). The mean time
taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design {#design-e1}

For each of the 45 *r* values, there were four versions of each plot
corresponding to the four levels of point opacity. Examples of each of these can
be seen in @fig-exp1-examples-chap4, demonstrated with an *r* value of 0.6.

```{r}
#| label: fig-exp1-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 1, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by `ggplot2`.
#| fig-asp: 0.275

# create example plots, arrange with ggarrange()

ggarrange(plot_example_function(slopes, "Opacity = 0.25",
                                0.25, 0.2, 7),
          plot_example_function(slopes, "Opacity = 0.5",
                                0.5, 0.2, 7),
          plot_example_function(slopes, "Opacity = 0.75",
                                0.75, 0.2, 7),
          plot_example_function(slopes, "Opacity = 1",
                                1, 0.2, 7),
          nrow = 1)
```

## Results {#results-e1}

```{r}
#| label: model-e1
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create experimental model

e1_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E1_uniform_adjustments_tidy)
```

```{r}
#| label: model-e1-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e1_lit_model <- add_fixed_effect(e1_model, "literacy", "E1_uniform_adjustments_tidy")
```

```{r}
#| label: model-e1-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# remove fixed effect to create null model

e1_model_cmpr <- comparison(e1_model)
```

```{r}
#| label: anova-results-e1

# use anova_results() function from shared_functions.R to output
# model stats to global environment

anova_results(e1_model, e1_model_cmpr)

# also do anova for literacy model

anova_results(e1_lit_model, e1_model)
```

To investigate the effects of opacity condition on participants' estimates of
correlation, a linear mixed effects model was built whereby opacity condition is
a predictor for the difference between objective *r* values for each plot and
participants' estimates of *r*. This model has random intercepts for items and
participants. A likelihood ratio test revealed that the model including global
opacity as a fixed effect explained significantly more variance than a null
model ($\chi^2$(`r in_paren(e1_model.df)`) = `r printnum(e1_model.Chisq)`, *p*
`r printp(e1_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity condition, along with 95%
confidence intervals.

```{r}
#| label: fig-e1-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in Experiment 1. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error. The overestimation zone is included to facilitate comparison to later work.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

# create plotting df

plotting_df <- as_tibble(emmeans(e1_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D")))

# output emm df to other_data folder

if (!file.exists(csv_wrangled <- "../data/other_data/exp1_emm_plot.csv")) {
  
  plotting_df %>%
    rename(condition = opacity) %>%
    mutate(condition = paste0(condition,
                              "_exp1")) %>%
    write_csv("../data/other_data/exp1_emm_plot.csv")
  
}

# create plot, including manual annotations

plotting_df %>%
  mutate(opacity = recode(opacity, !!!labels_e1)) %>%
  rename("Condition" = "opacity") %>%
  ggplot(aes(x = Condition,
             y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1,
                      ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e1
#| include: true
#| tbl-cap: Contrasts between different levels of the opacity factor in Experiment 1.

# assign slot, if it hasn't been assigned already

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

# make table df, create table with kbl()

contrasts_extract(e1_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e1)) %>%
  separate(Contrast, c(" ", "  "), sep = "-") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

This effect was driven by significant differences between means of correlation
estimation error between all conditions bar high and full opacity. Statistical
tests for contrasts were performed using the `emmeans` package
\cite{lenth_2024}, and are shown in @tbl-contrasts-e1. To test whether the
observed results could be explained by difference in participants' levels of
graph literacy, an additional model was built. This model is identical to the
experimental model, but also includes graph literacy as a fixed effect.
Including graph literacy as a fixed effect explained no additional variance
($\chi^2$(`r in_paren(e1_lit_model.df)`) = `r printp(e1_lit_model.Chisq)`, *p*
`r printp(e1_lit_model.p, add_equals = TRUE)`), indicating that the differences
observed in participants' correlation estimation performance were not as a
result of differences in levels of graph literacy.

```{r}
#| label: tbl-efs-e1
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (a) and summary statistics (b) for levels of the opacity factor in Experiment 1. Each effect size is compared to the reference level, full contrast (alpha = 1).
#| tbl-subcap: ["Effect sizes for opacity factor.", "Summary statistics."]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

# first set kableExtra options

options(knitr.kable.NA = "")

# lme.dscore from shared_functions.R is reproduced from the now-defunct
# EMAtools R package

# create efs df

efs <- lme.dscore(e1_model,
                  E1_uniform_adjustments_tidy,
                  type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Low Opacity (alpha = 0.25)",
                         "opacityB" = "Medium Opacity (alpha = 0.5)",
                         "opacityC" = "High Opacity (alpha = 0.75)",
                         "opacityD" = "Full Opacity (alpha = 1.0)")) %>%
  rename("Cohen's \\textit{d}" = "d") 

# use kbl() to create table

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

# pairwise comparisons

emmeans(e1_model, pairwise ~ "opacity")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Opacity = opacity,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Opacity" = recode(Opacity,
                         "A" = "Low Opacity (alpha = 0.25)",
                         "B" = "Medium Opacity\n(alpha = 0.5)",
                         "C" = "High Opacity\n(alpha = 0.75)",
                         "D" = "Full Opacity\n(alpha = 1.0)")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

```{r}
#| label: fig-estimates-by-r-e1
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by condition and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring higher global point opacity. Error bars show standard deviations of estimates.

# use plot_error_bars_function from shared_functions.R

plot_error_bars_function(E1_uniform_adjustments_tidy %>%
                           mutate(grouping_var = fct_relevel(opacity,
                                                             c("A",
                                                               "B",
                                                               "C",
                                                               "D")))
                         , "opacity", "difference", labels_e1) +
  geom_hline(yintercept = 0, linetype = 2)
```

A function from the now archived `EMAtools` package \cite{ematools} was used to
calculate an approximation of Cohen's *d* between the reference level (full
contrast, alpha = 1.0) and each other level of the opacity factor. These
statistics can be seen in @tbl-efs-e1 (a) along with means and standard
deviations (b). The largest effect size observed (*d* \~ 0.16) is between the
low and full opacity conditions, and is small. This was unsurprising given the
lack of previously reported effects on correlation perception of global point
opacity \cite{rensink_2012}. @fig-estimates-by-r-e1 shows how participants'
estimates of correlation change with the objective *r* value in the plot. The
line represent mean errors in correlation estimation, and standard deviations of
error are provided as error bars. The dashed horizontal line represents
hypothetical perfect estimation. As reported in previous literature (see Section
\ref{underestimation-related-work} in \chap{chap:related_work}), participants
underestimated *r* in nearly all cases.

## Discussion {#discussion-e1}

The hypothesis, that there would be a greater spread of correlation estimates
for plots with lower global opacity compared to those with higher global
opacity, was not supported. As seen in @tbl-efs-e1 (b), standard errors for each
opacity condition are identical to 3 decimal places. Participants' errors in
correlation estimation were significantly greater when the opacity of all
scatterplot points was lower compared when it was higher. This held true up
until alpha was set to 0.75, implying a threshold around this value past which
there is little variation in the perception of opacity, at least as far as it is
associated with correlation estimation. This lack of significant difference in
correlation estimation between the two highest global opacity conditions is
congruent with the logarithmic nature of contrast/brightness perception
\cite{fechner_1948, varshney_2013}; despite there being equal linear distance
between the opacity values used, the perceptual distance between them is
minimal.

As mentioned previously, Rensink \cite{rensink_2012, rensink_2014} presents the
only other account of experiments that directly test correlation perception as
it pertains to the opacities of scatterplot points, and report no difference in
either bias (error) or variability (spread) in correlation perception regarding
point opacity manipulations. In comparison, the results observed here do report
an effect. This effect may be explained by differences in experimental power, as
it is a small effect, although I argue that methodological differences may have
also played a small role. Given the small effect size (Cohen's *d* = 0.16), the
small sample in Rensink (2014) \cite{rensink_2014} may have caused the
experiment to be insufficiently powered. With the large sample size in the
present work (*N* = 150), evidence for an effect has been found. The
experimental methodology utilised here is more representative of the use of
scatterplots in the wild, and is therefore more suited to informing design as
opposed to investigating the mathematical relationship between real and
perceived correlation. While the effect is small, it demonstrates definitively
that differences in the opacities of scatterplot points *can* affect estimates
of correlation in positively correlated scatterplots. From the results it is
unclear why lowering global opacity causes greater errors in correlation
estimation while causing no difference in spread.

I suggest that correlation perception functions similarly to speed perception
\cite{champion_2017} with regards to changes in the contrast between foreground
targets (in this case scatterplot points) and the background; the greater
spatial uncertainty brought on by reduced point opacity, while not eliciting
greater spread in correlation estimates, might be responsible for the effects
observed via an increase in the perceived width of the probability distribution
displayed by the scatterplot

From the results it is clear that a scatterplot optimised for correlation
perception should have contrast between the foreground (points) and background
in a range corresponding to alpha values of between 0.75 and 1. That there are
significant differences in correlation estimation between data-identical
scatterplots with different global point opacities however, suggests that this
effect may be leveraged to further improve participants' performances on a
correlation estimation task.

# Experiment 2: Spatially-Dependent Opacity Adjustments

## Introduction {#introduction-e2}

Experiment 1 found that point opacity in positively correlated scatterplots has
an effect on the perception of correlation such that those scatterplots with
higher levels of global point opacity are rated as being more strongly
correlated. Given this finding, the question arises of whether additional
changes in correlation perception may be observed as a function of the spatial
arrangement of point opacity. Given the previously reported effects of changing
scatterplot point opacity on other perceptual metrics (see Section
\ref{point-opacity-chap4}), and with the findings from Experiment 1 in mind, it
was hypothesised that:

-   H1: the non-linear decay parameter in which point opacity falls with
    residual magnitude will result in lower mean errors in correlation
    estimation compared to linear decay and full global opacity conditions.
-   H2: the use of the inverted non-linear decay parameter, in which point
    opacity becomes greater with residual magnitude, will result in higher mean
    errors in correlation estimation than for all other conditions.

## Method {#method-e2}

### Participants {#participants-e2}

150 participants were recruited using the Prolific platform \cite{prolific}.
Normal to corrected-to-normal vision and English fluency were required.
Participants who had completed the pilot study or Experiment 1 were prevented
from participating. Data were collected from 158 participants. 8 failed more
than 2 out of 6 attention check questions, and, as per the pre-registration, had
their submissions rejected from the study. The data from the remaining 150
participants were included in the full analysis
(`r printnum(E2_spatially_dependent_tidy_gender$M)` male,
`r printnum(E2_spatially_dependent_tidy_gender$F)` female, and
`r printnum(E2_spatially_dependent_tidy_gender$NB)` non-binary). Participants'
mean age was `r printnum(E2_spatially_dependent_tidy_age$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_age$sd)`). Mean graph literacy score was
`r printnum(E2_spatially_dependent_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_graph_literacy$sd)`). The average time
taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design {#design-e2}

For each of the 45 *r* values in Experiment 2, there were four versions of each
plot corresponding to the three levels of point opacity decay function and the
baseline global full opacity condition. Examples of each of these can be seen in
@fig-exp2-examples-chap4, demonstrated with an *r* value of 0.6. Given the shape
of the underestimation curve found in previous work (see Figure
\ref{fig-underestimation-curves}, \chap{chap:related_work}), intuition suggested
employing a symmetrically opposing curve (see the non-linear decay curve in
@fig-opposing-curve) to relate point opacity to residuals.

```{r}
#| label: fig-opposing-curve
#| include: true
#| fig-cap: Using an \textit{r} value of 0.2 to demonstrate the relationship between the size of a point's residual and the alpha value (opacity) rendered.
#| fig-asp: 1

# create desired r value for opposing curve illustrative plot
# note: methodology identical to that used to create exp stimuli

my_desired_r = 0.2
  
my_sample_size = 128
mean_variable_1 = 0
sd_variable_1 = 1
mean_variable_2 = 0
sd_variable_2 = 1

mu <- c(mean_variable_1, mean_variable_2) 

myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)

mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 

corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))

corr_model <- lm(V2 ~ V1, data = corr_data)

my_residuals <- abs(residuals(corr_model))

data_with_resid <- round(cbind(corr_data, my_residuals), 2)

# create opposing slopes df

slopes_opp <- data_with_resid %>%
  mutate(linear_decay = ((-my_residuals/2.6) + 1)) %>%
  mutate(inverted_decay = 1-(0.25)^my_residuals) %>%
  mutate(non_linear_decay = (1 + (0.25)^ my_residuals)-1) %>%
  pivot_longer(cols = ends_with("ay"),
               names_to = c("decay"),
               values_to = "transformed_residual")

# create slopes plot, including manual annotations

slopes_opp %>%
  mutate(decay = str_replace_all(decay, pattern = "_", replacement = " ")) %>%
  mutate(decay = str_to_title(decay)) %>%
  group_by(decay) %>%
  ggplot(aes(x = my_residuals, y = transformed_residual)) +
  geom_textline(aes(colour = decay,
                    label = decay,
                    size = 6,
                    hjust = 0.65,
                    vjust = -0.3)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10),
                     limits = c(0,2.6)) +
  labs(x = "Residual Magnitude",
       y = "Alpha (opacity)") +
  theme_ggdist() +
  theme(legend.title = element_blank(),
        legend.position = "none",
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  coord_cartesian(xlim = c(0,2.6), ylim = c(0,1)) +
  scale_colour_manual(values = c("black", "black", "black")) +
  annotate("text",
           x = 0.4,
           y = 1,
           label = "Fully Opaque") +
    annotate("text",
           x = 0.4,
           y = 0,
           label = "Fully Transparent")
```

Equation 4.1 was used to non-linearly map residuals to `ggplot2` alpha values.
0.25 was chosen as the value of *b*, as it was felt at the time that this
rendered plots that maintained point visibility while also allowing a large
enough point opacity range that, if an effect was present, it was likely to be
found.

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

```{r}
#| label: fig-exp2-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 2, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by `ggplot2`.
#| fig-asp: 0.275

# example plots for experiment 2, arranged with ggarrange()

ggarrange(plot_example_function(slopes, "Non-Linear Decay",
                                (1-slopes$slope_0.25), 0.2, 7),
          plot_example_function(slopes, "Linear Decay",
                                (1-slopes$slope_linear), 0.2, 7),
          plot_example_function(slopes, "Inverted Decay",
                                (1-slopes$slope_inverted), 0.2, 7),
          plot_example_function(slopes, "Full Opacity",
                                1, 0.2, 7),
          nrow = 1)
```

## Results {#results-e2}

```{r}
#| label: model-e2
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create experimental model

e2_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E2_spatially_dependent_tidy)
```

```{r}
#| label: model-e2-lit
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build model with literacy added for later

e2_lit_model <- add_fixed_effect(e2_model,
                                 "literacy",
                                 "E2_spatially_dependent_tidy")
```

```{r}
#| label: model-e2-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create null model

e2_model_cmpr <- comparison(e2_model)
```

```{r}
#| label: anova-results-e2

# anova between null and experimental model
# function outputs model stats to global env

anova_results(e2_model, e2_model_cmpr)

# also do anova for literacy model

anova_results(e2_lit_model, e2_model)
```

To investigate the effects of the opacity decay functions on participants'
estimates of correlation, a linear mixed effects model was built with decay
function condition as a predictor for the difference between objective *r*
values for each plot and participants' estimates of *r*. This model has random
intercepts for items and participants. A likelihood ratio test revealed that the
model including opacity decay function as a fixed effect explained significantly
more variance than a null model ($\chi^2$(`r in_paren(e2_model.df)`) =
`r printnum(e2_model.Chisq)`, *p* `r printp(e2_model.p, add_equals = TRUE)`).
@fig-e2-estimates shows the mean errors in correlation estimation for each
opacity decay function condition, along with 95% confidence intervals.

```{r}
#| label: fig-e2-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in Experiment 2. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

# create plotting df

plotting_df <- as_tibble(emmeans(e2_model, pairwise ~ "opacity")[[1]]) %>%
    mutate(opacity = fct_relevel(opacity, c("A", "B", "C", "D")))

# write EMM csv if it does not exist already

if (!file.exists(csv_wrangled <- "../data/other_data/exp2_emm_plot.csv")) {
  
  plotting_df %>%
    rename(condition = opacity) %>%
    mutate(condition = paste0(condition, "_exp2")) %>%
    write_csv("../data/other_data/exp2_emm_plot.csv")
  
}

# create emm plot, including manual annotations

 plotting_df %>%
  mutate(opacity = recode(opacity, !!!labels_e2)) %>%
  rename("Condition" = "opacity") %>%
   ggplot(aes(x = Condition, y = emmean*-1)) +
   geom_point(size = 1.75) +
   geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
   theme_ggdist() +
   labs(y = "Estimated Marginal Mean of Error",
        x = "Condition") +
   theme(axis.text = element_text(size = 7),
         axis.title = element_text(size = 8)) +
   scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
   coord_flip() +
   geom_abline(intercept = 0, slope = 0, linetype = 2) +
   annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)
 
# remove extraneous df

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e2
#| include: true
#| tbl-cap: Contrasts between different levels of opacity decay function in Experiment 2.

# assign slot, if it hasn't been assigned already

if (class(e2_model) == "buildmer") e2_model <- e2_model@model

# make table df

contrasts_extract(e2_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e2_regex)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 1), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))

```

The effect seen in Experiment 2 was driven by significant differences in means
of correlation estimation error between all levels of opacity decay function
condition bar full opacity and linear decay. Statistical testing for contrasts
was performed using the `emmeans` package \cite{lenth_2024}, and are shown in
@tbl-contrasts-e2. To test whether the observed results could be explained by
differences in graph literacy, a model including participants' graph literacy
scores as a fixed effect was built. Including graph literacy as a fixed effect
again explained no additional variance ($\chi^2$(`r in_paren(e2_lit_model.df)`)
= `r printp(e2_lit_model.Chisq)`, *p*
`r printp(e2_lit_model.p, add_equals = TRUE)`).

```{r}
#| label: tbl-efs-e2
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (a) and summary statistics (b) for the opacity decay function condition in Experiment 2. Each effect size is compared to the reference level, full contrast (alpha = 1).
#| tbl-subcap: ["Effect sizes for opacity decay factor.", "Summary statistics for opacity decay factor."]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

# first set kableExtra options

options(knitr.kable.NA = "")

# lme.dscore from shared_functions.R is reproduced from the now-defunct
# EMAtools R package

# create efs df

efs <- lme.dscore(e2_model, E2_spatially_dependent_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "opacityD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "opacityA" = "Non-Linear Decay",
                         "opacityB" = "Linear Decay",
                         "opacityC" = "Inverted Decay",
                         "opacityD" = "Full Opacity")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

# pairwise comparisons

emmeans(e2_model, pairwise ~ "opacity")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Opacity = opacity,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Opacity" = recode(Opacity,
                         "A" = "Non-Linear Decay",
                         "B" = "Linear Decay",
                         "C" = "Inverted Decay",
                         "D" = "Full Opacity")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

Approximated Cohen's *d* effect sizes between the baseline (global full opacity)
and each other condition can be seen in @tbl-efs-e2. The largest effect size
observed (\~0.23 between full opacity and inverted decay conditions) is small to
moderate, and the effect size between the baseline and the non-linear decay
condition (\~0.19) is small. @fig-estimates-by-r-e2 illustrates the effects of
each manipulation on participants' correlation estimation performance separately
for each value of *r* used. The dashed horizontal line represents perfect
estimation, and standard deviations of estimation error are provided by way of
error bars. Participants still underestimated correlation in all conditions,
although the use of the non-linear decay function biased participants' estimates
upwards to partially correct for the underestimation.

```{r}
#| label: fig-estimates-by-r-e2
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by condition and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring the non-linear opacity decay function. Error bars show standard deviations of estimates.

# plot error bars using function from shared_functions.R

plot_error_bars_function(E2_spatially_dependent_tidy %>%
                           mutate(grouping_var = fct_relevel(opacity,
                                                             c("A",
                                                               "B",
                                                               "C",
                                                               "D")))
                         , "opacity", "difference", labels_e2) +
  geom_hline(yintercept = 0, linetype = 2)
```

## Discussion {#discussion-e2}

```{r}
#| label: model-e2-low-range
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create new low range df by filtering existing df

E2_low_range <- E2_spatially_dependent_tidy %>%
  filter(opacity == c("B", "D")) %>%
  filter(my_rs < 0.6)

# build low range model

e2_model_low_range <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E2_low_range)
```

```{r}
#| label: model-e2-low-range-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create low range comparison model

e2_model_comparison_low_range <- comparison(e2_model_low_range)
```

```{r}
#| label: anova-results-e2-low-range
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# anova results for low range model vs null
# outputs stats to global environment

anova_results(e2_model_low_range, e2_model_comparison_low_range)
```

Both hypotheses received support in Experiment 2. Participants' errors in
correlation estimation were lowest when the non-linear decay function was used,
and were highest when scatterplots employed the non-linear inverted decay
function. There was no significant difference in correlation estimation errors
between the linear decay function and full contrast conditions. This result was
surprising, however on closer inspection of the scatterplots in the linear decay
function condition, it is clear that the logarithmic nature of contrast
perception \cite{fechner_1948,
varshney_2013} means there was little perceptual distance between points with
high opacity values (*alpha* \> 0.75). This resulted in no significant perceived
differences between full opacity and linear decay function scatterplots. A
similar threshold for high opacity values was found in Experiment 1. Selecting
only lower *r* values, those with naturally higher residuals (arbitrarily *r* \<
0.6), still results in no difference between correlation estimation errors for
linear decay parameters and full opacity conditions
($\chi^2$(`r in_paren(e2_model_low_range.df)`) =
`r printnum(e2_model_low_range.Chisq)`, *p*
`r printp(e2_model_low_range.p, add_equals = TRUE)`). Effect sizes were small,
with the largest being between the baseline full opacity and inverted non-linear
decay conditions. This suggests that it is easier to induce further bias in
correlation estimates through a reduction in the salience of a point cloud's
centre than it is to correct for the underestimation bias.

Looking at the standard deviations of correlation estimates plotted separately
by opacity decay function and *r* value in @fig-estimates-by-r-e2, it can be
observed that as in Experiment 1 (see @fig-estimates-by-r-e1), and aside from
the inverted non-linear decay function condition, precision in *r* estimation
increased with the objective *r* value. This finding corroborates previous work.
In the inverted non-linear decay condition, as *r* approaches 1 (and point
residuals accordingly approach 0), the opacity of points diminishes. Just as the
standard deviation of correlation estimates was higher for the low global
opacity condition in Experiment 1, having lower opacity points at the high *r*
end of the non-linear inverted condition in Experiment 2 resulted in fairly
constant standard deviations across *r* values, as the usual reduction towards
*r* = 1 was not observed.

The non-linear inverted opacity decay condition produced significantly lower
estimates of correlation than all other conditions. This adds perspective to
suggestions \cite{yang_2019} that, among other visual features, the area of a
hypothetical prediction ellipse \cite{cleveland_1982, yang_2019}, a region used
to predict new observations assuming a bivariate normal distribution (see
@fig-prediction-ellipse) is a better predictor of correlation estimation
performance than the objective *r* value itself. In the inverted non-linear
decay condition, the area of this ellipse remained the same, yet estimates of
correlation differed significantly. These results suggest that the apparent
density of the scatterplot point cloud also has effects on estimates of
correlation. Prior work has found that more dense scatterplots are rated as
having higher correlations \cite{lauer_1989, rensink_2014}, although the effect
found was weak. To explore this effect further, work investigating what people
attend to when making correlation judgements must be completed. Eye-tracking
offers an elegant solution to this problem, yet at the time of writing, had only
been used for simpler scatterplot-related tasks, such as those asking
participants to identify the number of, or distance between, points
\cite{netzel_2017}.

```{r}
#| label: fig-prediction-ellipse
#| include: true
#| fig-asp: 1
#| fig-cap: Plot showing a 95\% prediction ellipse over a scatterplot with an \textit{r} value of 0.6.

# use plot_example_function from shared_functions.R to create
# plot that includes prediction ellipse

plot_example_function(slopes ,"", 1, 0.2, 7) +
  stat_ellipse(type = "norm")
```

# General Discussion {#general-discussion-adjusting-opacity}

To summarise the results, evidence was found that changing the opacities of
points in scatterplots can significantly alter participants' estimates of
correlation, that lower global point opacity is associated with greater
correlation underestimation error, that lowering point opacity as a function of
residual magnitude can partially correct for this underestimation, and that
raising opacity as a function of residual magnitude can increase the
underestimation bias. These findings pave the way for using changes in point
opacity to produce perceptually-optimised scatterplots that do not rely on data
removal. As the focus of this work is on designing data visualisations that lay
people are more easily able to interpret and understand, we used a large,
representative sample, including people from a range of nationalities and
educational backgrounds. This chapter demonstrates that a simple framework can
be employed with these groups to gather high quality data, and, by design,
produce conclusions that may generalise better to in-the-wild data visualisation
usage.

In agreement with previous research
\cite{pollack_1960, rensink_2010, rensink_2012,
rensink_2014, rensink_2017}, participants were more accurate and precise in
their estimates of correlation when the *r* value shown in the scatterplot was
higher. @fig-estimates-by-r-e1 and @fig-estimates-by-r-e2 plot objective *r*
value against participants' errors in correlation estimation separately for each
level of the respective independent variable. These plots illustrate, as in much
previous work, the lower levels of precision and accuracy in correlation
estimation that are seen for *r* values further from 0 or 1.

The results here contribute to a body of evidence that suggests participants are
attending to the width of the probability distribution displayed in a
scatterplot \cite{cleveland_1982, meyer_1997, rensink_2017, yang_2019} when
making judgements of correlation. Further evidence is provided for the
systematic underestimation bias, and a potential correction strategy is offered.
This work does not attempt to redesign the scatterplot as a medium, but to
provide a set of recommendations for designers based on the evidence; when
designing positively correlated scatterplots to support correlation perception:

-   Lowering the total opacity in a scatterplot can cause people to
    underestimate correlation compared to when contrast is maximal between the
    points and the plot background (when point opacity is high).
-   The use of a non-linear opacity decay function, in which point opacity falls
    as a function of residual magnitude, can be used to counteract the
    underestimation seen in correlation estimation in positively correlated
    scatterplots.

Scatterplots are widely used, and are often designed with a number of
communicative concepts in mind. When one of these concepts is illustrating the
degree of positive association between two variables, the findings presented
suggest that designers should utilise the techniques described to give viewers
the best chance of interpreting the correlation displayed as accurately as
possible.

## Training {#training-adjusting-opacity}

```{r}
#| label: model-e1-training
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create training model by adding fixed effect of half

e1_training_model <- add_fixed_effect(e1_model, "half", "E1_uniform_adjustments_tidy")

# just use standard experimental model as null

e1_training_cmpr <- e1_model
```

```{r}
#| label: model-e1-training-results
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# anova results for training model vs null (exp)
# outputs stats to global environment

anova_results(e1_training_model, e1_training_cmpr)
```

```{r}
#| label: model-e2-training
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# as above, but for experiment 2

e2_training_model <- add_fixed_effect(e2_model, "half", "E2_spatially_dependent_tidy")

e2_training_cmpr <- e2_model
```

```{r}
#| label: model-e2-training-results
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# anova results for training model vs null (exp)
# outputs stats to global environment

anova_results(e2_training_model, e2_training_cmpr)
```

Both experiments tested lay participants with varying levels of graph literacy.
Due to concerns about participants' familiarity with scatterplots, each saw four
scatterplots depicting correlations of *r* = 0.2, 0.5, 0.8, and 0.95 (see
@fig-training-slide-adjusting-opacity) to familiarise themselves with the
concept. To test if the patterns of results seen in correlation estimation could
be attributed to this training, models were built that included the half of the
session (first or second) as a predictor for participants' judgements of
correlation. Comparing these models to the original revealed a significant
effect in Experiment 1 ($\chi^2$(`r in_paren(e1_training_model.df)`) =
`r printnum(e1_training_model.Chisq)`, *p*
`r printnum(e1_training_model.p, add_equals = TRUE)`), but not Experiment 2
($\chi^2$(`r in_paren(e2_training_model.df)`) =
`r printnum(e2_training_model.Chisq)`, *p*
`r printnum(e2_training_model.p, add_equals = TRUE)`). In Experiment 1,
participants' errors in correlation estimation were higher in the second half of
the experiment, suggesting that having more recently viewed the training
material may have made participants more accurate. That this was not observed in
Experiment 2 implies that the point opacity manipulation had a greater effect on
estimates than any training effects.

## Limitations {#limitations-adjusting-opacity}

The results in Experiment 2 provide evidence that reducing the salience of
points as they move further from the regression line can increase people's
estimates of correlation, at least when plots like these are presented alongside
conventional ones. Testing whether this phenomenon would exist with a plot in
isolation would present a number of difficulties. As can be seen in
@fig-estimates-by-r-e1 and @fig-estimates-by-r-e2, participants' estimates of
correlation, especially between 0.2 and 0.7, suffer from high variance. High
numbers of trials and participants ameliorate this to an extent, but this does
prevent commentary on single plot judgements of correlation.

An important first step in the utilisation of point opacity changes to optimise
perception of correlation in scatterplots has been made, however there remains
much to do. Quantitative determination of whether 0.25 is indeed the most
optimal value for $b$ in equation 1, for example, is impossible from the present
results. It may well be the case that changing the value of $b$ as a function of
the objective Pearson's *r* value could produce more accurate correlation
estimation in participants; findings that participants were more accurate in
correlation estimation when *r* was nearer 0 or 1 would suggest that the use of
a decay parameter for these correlations is unnecessary.

The simplicity of the direct estimation task employed confers some limitations
on the conclusions that can be drawn, although these limitations do not prevent
the data gathered from being practically useful. The methods used do not allow
for the investigation of absolute correlation perception, as JND or bisection
methodologies might. The finding that mean errors in judgements of correlation
for full opacity conditions were different between Experiments 1 (0.149) and 2
(0.123) makes this limitation evident. In the experimental paradigm,
participants indirectly made comparative correlation judgements, which may have
resulted in this discrepancy. Nevertheless, the results found are promising with
regards to design research.

## Future Work {#future-work-adjusting-opacity}

Future work may wish to investigate negative values of *r*, given findings that
people may overestimate the correlation of negatively correlated scatterplots in
a similar way \cite{sher_2017}. The techniques presented here could be adapted
to bias perceptions of correlation down and correct for this overestimation. The
non-linear inverted opacity decay function demonstrated here may be able to
accomplish this.

The opacity decay function conditions in Experiment 2 used the vertical distance
between a particular point and the regression line to set that point's opacity.
Previous work \cite{meyer_1997} has suggested that the perpendicular distance
between a point and the regression line may be a more accurate predictor of
performance on correlation estimation tasks
\cite{cleveland_1982, rensink_2017, yang_2019}. Future work exploring these
manipulations further may wish to investigate whether there are differences
between using perpendicular and vertical residual distance as bases for setting
point opacity with regard to correlation estimation.

Finally, the most exciting avenue for related future work is the possibility of
combining the techniques developed and tested here with other novel scatterplot
visualisation techniques that have a basis in the literature.
\chap{chap:adjusting_size} investigates the use of point size in place of
opacity with the techniques described, while \chap{chap:belief_change} combines
the two to explore how these different modalities work together with regards to
the estimation of positive correlation.

# Conclusion {#conclusion-chap4}

In a pair of experiments, I varied the opacity of point in scatterplots both
uniformly (Experiment 1) and using functions relating point opacity to the size
of a particular point's residual (Experiment 2). In Experiment 1, I showed that,
in contradiction with previous work, changing the opacity of all points in a
scatterplot can effect participants' performance on a correlation estimation
task. In Experiment 2, I showed that varying point opacity as a function of a
point's residual magnitude is able to significantly change participants'
correlation estimates and partially correct for a long-standing underestimation
bias.
