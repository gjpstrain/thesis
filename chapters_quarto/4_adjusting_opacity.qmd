---
title: "adjusting_opacity"
output:
  format: 
    latex:
      
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 

execute: 
  echo: false
  warning: false
  message: false
  include: false      
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_e1 <- c(A = "Low Opacity\n(alpha = 0.25)",
               B = "Medium Opacity\n(alpha = 0.5)",
               C = "High Opacity\n(alpha = 0.75)",
               D = "Full Opacity\n(alpha = 1.0)")
labels_e2 <- c(A = "Non-Linear Decay",
               B = "Linear Decay",
               C = "Inverted Decay",
               D = "Full Opacity")
```

```{r}
#| label: load-data

exp1_anon <- read_csv("../data/exp_1_data.csv")
exp2_anon <- read_csv("../data/exp_2_data.csv")# all experimental data lives in this folder

# load in shared functions

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  dplyr::select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "contrast"), sep = "-") %>%
  mutate(contrast = str_replace(contrast, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  dplyr::select(c("participant",
                  "item",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "trials.thisN",
                  "session")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  full_join(demographics, by = "participant") %>%
  mutate(across(c("item", "contrast", "half"), as_factor)) %>%
  dplyr::select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(contrast, c('A', 'B', 'C', 'D'))) %>%
  rename("opacity" = "contrast") %>% #contrast was an earlier term, so is changed here for consistency                                        

  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

walk(list(exp1_anon,
          exp2_anon),
     wrangle)

# Experiments were incorrectly named, so rename them

E1_uniform_adjustments_tidy <- E3_full_exp_tidy
E2_spatially_dependent_tidy <- E2_full_exp_tidy

# remove incorrectly named and anon dfs from environment

rm(E3_full_exp_tidy, E2_full_exp_tidy, exp1_anon, exp2_anon, wrangle)

# extract age and gender information

extract_age(E1_uniform_adjustments_tidy)

extract_gender(E1_uniform_adjustments_tidy)

extract_literacy(E1_uniform_adjustments_tidy)

extract_age(E2_spatially_dependent_tidy)

extract_gender(E2_spatially_dependent_tidy)

extract_literacy(E2_spatially_dependent_tidy)
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean*-1)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = -1*mean + sd, ymax = -1*mean - sd),
                width = 0.01,
                size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6,
                                  margin = margin(1,0,1,0, "mm")),
        aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title.y = ggtext::element_markdown(size = 8),
        axis.title.x = ggtext::element_markdown(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective *r*",
         y = "Mean *r* Estimation Error") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

# Abstract {#abstract-adjusting-opacity}

Scatterplots are common data visualisations utilised for communication with experts 
and lay people alike. Despite being widely studied, it is common for people to 
underestimate the level of correlation displayed in them. The weight of evidence
points toward changes in the opacities of scatterplot points being unable to change
perceptions of correlation, however this was not tested rigorously using systematic
adjustments. Drawing on evidence that the shape of a scatterplot's point cloud
may drive correlation perception, I conducted exploratory work addressing
this underestimation bias. In two experiments (total *N* = 300), evidence is provided
that changing the opacities of scatterplot points *can* have small effects on participants'
performance on a correlation estimation task. The systematic adjustment of 
point opacity as a function of residual distance is able to alter estimates to a greater 
degree and correct for the underestimation bias. In this chapter, I also present
an early pilot study that was ultimately not included in any published works.

# Preface: Learning From an Early Pilot Study {#pilot-study}

# Introduction {#introduction-adjusting-opacity}

## Overview {#overview-adjusting-opacity}

# Related Work {#related-work-adjusting-opacity}

## Transparency, Contrast, Opacity, and Formal Definitions

- include the "formalising contrast" part of the original papers general methods section here
- also include justification for referring to "opacity" instead of contrast

## Effects of Point Opacity on Correlation Estimation

# General Methods {#shared-methods-adjusting-opacity}

The experiments described in this chapter share multiple aspects of their procedures.
Both experiments were built using PsychoPy \cite{peirce_2019} and are hosted on pavlovia.org.
Both use 1-factor, 4-level designs. Ethical approval for both experiments was granted
by the University of Manchester's Computer Science Departmental Panel (Ref:
2022-14660-24397). In each experiment, participants were shown the respective
Participant Information Sheet (henceforth PIS) and provided consent through key presses
in response to consent statements. Participants were asked to provide their age and 
gender identity, after which they completed the 5-item Subjective Graph Literacy
test described by Garcia-Retamero et al. \cite{garcia_2016} and discussed in 
Section \ref{graph-literacy-lit-review} of the literature review. Early piloting
with a graduate student in humanities suggested the potential for participants to
be unfamiliar with the visual nature of different values of Pearson's *r*. Participants
were therefore shown examples of *r* = 0.2, 0.5, 0.8, and 0.95 (see @fig-training-slide-adjusting-opacity);
a discussion of the effects of this training is provided in Section \ref{training-adjusting-opacity}.
Participants were given two practice trials to familiarise themselves with the
response slider.

```{r}
#| label: fig-training-slide-adjusting-opacity
#| include: true
#| fig-cap: Participants viewed these plots for at least eight seconds before being allowed to continue to the practice trials.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/example-plots.png", dpi = NA)
```

```{r}
#| label: fig-mask-adjusting-opacity
#| include: true
#| fig-cap: An example of a visual mask displayed for 2.5 seconds before each experimental trial.

# find training slide png in supplied graphics folder

knitr::include_graphics(path = "../supplied_graphics/visual_mask.png")
```

Each trial was preceded by text that either told the participant:

 - Please look at the following plot and use the slider to estimate the correlation (n = 180).
 - Please IGNORE the correlation displayed and set the slider to 1 (n = 3) or 0 (n = 3).
 
 The latter instructions were attention checks, and were formatted with red text 
 to increase their visibility. Each experimental trial was preceded by a visual
 mask (see @fig-mask-adjusting-opacity) that was displayed for 2.5 seconds. Participants
 were instructed to make their judgements as quickly and accurately as possible,
 but there was no time limit per trial. Both experiments described here use a
 fully repeated-measures, within-participants design. Participants saw all 180 experimental
 items, corresponding to ~27,000 individual judgements per experiment, in a fully
 randomised order.
 
 Both experiments were conducted according to principles of open and reproducible
 research. All data and analysis code for the origin paper is available on GitHub [^1].
 Experiment 1 [^2] and 2 [^3] are hosted on Pavlovia.org, while the Open Science
 Framework hosts pre-registrations [^4]. It is important to note at this point
 that experiment 2 was conducted prior to experiment 1; when the original paper
 was written, the order of presentation of the experiments was swapped to make the
 narrative more cohesive. I preserve this order in the present chapter.
 
 [^1]: https://github.com/gjpstrain/contrast_and_scatterplots
 [^2]: https://gitlab.pavlovia.org/Strain/exp_uniform_adjustments
 [^3]: https://gitlab.pavlovia.org/Strain/exp_spatially_dependent
 [^4]: Experiment 1 - https://osf.io/tuexh. Experiment 2 - https://osf.io/6f5ev
 
# Experiment 1: Uniform Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e1}

Owing to the robust effects of altering stimulus opacity on perception described 
above \cite{wehrhahn_1990, champion_2017}, it was hypothesised that there would be 
a greater spread of estimates of correlation for plots with lower global opacity
compared to higher opacity plots.

## Method {#methods-adjusting-opacity-e1}

### Participants

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 8 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E1_uniform_adjustments_tidy_gender$M)`% male, `r printnum(E1_uniform_adjustments_tidy_gender$F)`
% female, and `r printnum(E1_uniform_adjustments_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E1_uniform_adjustments_tidy_age$mean)`
(*SD* = `r printnum(E1_uniform_adjustments_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E1_uniform_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E1_uniform_adjustments_tidy_graph_literacy$sd)`). The mean
time taken to complete the experiment was 33 minutes (SD = 10 minutes).

### Design

For each of the 45 *r* values, there were four versions of each plot corresponding
to the four levels of point opacity. Examples of each of these can be seen in
@fig-exp1-examples-chap4, demonstrated with an *r* value of 0.6.

```{r}
#| label: fig-exp1-examples-chap4
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 1, demonstrated with an \textit{r} value of 0.6. Here, "opacity" refers to the alpha value used by ggplot.
#| fig-asp: 0.275

ggarrange(plot_example_function(data_with_resid, "Opacity = 0.25",
                                0.25, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.5",
                                0.5, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 0.75",
                                0.75, 0.2, 7),
          plot_example_function(data_with_resid, "Opacity = 1",
                                1, 0.2, 7),
          nrow = 1)
```

## Analysis {#analysis-adjusting-opacity-e1}

```{r}
#| label: model-e1
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model <- buildmer(difference ~ opacity +
                       (1 + opacity | participant) +
                       (1 + opacity | item),
                     data = E1_uniform_adjustments_tidy)

# build model with literacy added for later

e1_lit_model <- add_fixed_effect(e1_model, "literacy", "E1_uniform_adjustments_tidy")
```

```{r}
#| label: model-e1-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e1_model_cmpr <- comparison(e1_model)
```

```{r}
#| label: anova-results-e1

anova_results(e1_model, e1_model_cmpr)

# also do anova for literacy model

anova_results(e1_lit_model, e1_model)
```

To investigate the effects of opacity condition on participants' estimates
of correlation, a linear mixed effects model was built whereby opacity condition
is a predictor for the difference between objective *r* values for each plot
and participants' estimates of *r*. This model has random intercepts for 
items and participants. A likelihood ratio test revealed that the mode including
global opacity as a fixed effect explained significantly more variance than a 
null model ($\chi^2$(`r in_paren(e1_model.df)`) = `r printnum(e1_model.Chisq)`,
*p* `r printp(e1_model.p, add_equals = TRUE)`). @fig-e1-estimates shows the mean
errors in correlation estimation for each opacity condition, along with 95% confidence intervals.

```{r}
#| label: fig-e1-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 1. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

plotting_df <- as_tibble(emmeans(e1_model, pairwise ~ "opacity")[[1]]) %>%
  mutate(opacity = recode(opacity, !!!labels_e1)) %>%
  rename("Condition" = "opacity")

ggplot(aes(x = reorder(Condition, emmean), y = emmean*-1),data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e1
#| include: true
#| tbl-cap: Contrasts between different levels of the opacity factor in experiment 1.

# assign slot, if it hasn't been assigned already

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

# make table df

contrasts_extract(e1_model, "opacity") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e1)) %>%
  separate(Contrast, c(" ", "  "), sep = "-") %>%
  kbl(booktabs = TRUE, digits = c(0,2,3), escape = FALSE) %>%
  add_header_above(c("Contrasts" = 2, "Statistics" = 2))

```

This effect was driven by significant difference between means of correlation 
estimation error between all conditions bar high and full opacity. Statistical
tests for contrasts were performed using the **emmeans** package \cite{emmeans}, 
and are shown in @tbl-contrasts-e1. To test whether the observed results could be
explained by difference in participants' levels of graph literacy, an additional
model was built. This model is identical to the experimental model, but also
includes graph literacy as a fixed effect. There was no effect of graph literacy,
indicating that the differences observed in participants' correlation estimation 
performance were not as a result of their differing levels of graph literacy
($\chi^2$(`r in_paren(e1_lit_model.df)`) = `r printp(e1_lit_model.Chisq)`,
*p* `r printp(e1_lit_model.p, add_equals = TRUE)`).

```{r}
#| label: tbl-sig-e1
#| include: true
#| tbl-cap: placeholder

if (class(e1_model) == "buildmer") e1_model <- e1_model@model

model_summary <- summary(e1_model)

coef_table <- model_summary$coefficients 

as_tibble(coef_table, rownames = "Effect") %>%
  column_to_rownames(var = "Effect")
```


## Discussion {#discussion-adjusting-opacity-e1}

# Experiment 2: Spatially-Dependent Opacity Adjustments

## Introduction {#introduction-adjusting-opacity-e2}

## Methods {#methods-adjusting-opacity-e2}

150 participants were recruited using the Prolific platform \cite{prolific}. Normal to
corrected-to-normal vision and English fluency were required. Participants who
had completed the pre-study were prevented from participating. Data were collected
from 158 participants. 7 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E2_spatially_dependent_tidy_gender$M)`% male, `r printnum(E2_spatially_dependent_tidy_gender$F)`
% female, and `r printnum(E2_spatially_dependent_tidy_gender$NB)`% non-binary). 
Participants' mean age was `r printnum(E2_spatially_dependent_tidy_age$mean)`
(*SD* = `r printnum(E2_spatially_dependent_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E2_spatially_dependent_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E2_spatially_dependent_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 33 minutes (SD = 10 minutes).


## Analysis {#analysis-adjusting-opacity-e2}

## Discussion {#discussion-adjusting-opacity-e2}

# General Discussion {#general-discussion-adjusting-opacity}

## Training {#training-adjusting-opacity}
