---
title: "adjusting_size"
output:
  format: 
    latex:

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/"
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# define plotting labels now

labels_e3 <- c(A = "Non-Linear\nDecay",
               B = "Linear\nDecay",
               C = "Inverted\nDecay",
               D = "Standard\nSize")

labels_e3_regex <- c(
  "\\bA\\b" = "Non-Linear Decay",
  "\\bB\\b" = "Linear Decay",
  "\\bC\\b" = "Inverted Decay",
  "\\bD\\b" = "Standard Size"
)
```

```{r}
#| label: load-data

exp3_anon <- read_csv("../data/exp_3_data.csv") %>%
  mutate("expName" = recode(expName,
                            "exp_size_only" = "E3_size_adjustments"))

source("../shared_functions.R")
```

```{r}
#| label: retrieve-cached-models-chap4

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data-chap5

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  # first do literacy
  
    literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
    
    # extract and process visual threshold testing
  
  visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels, pattern = "vis_threshold_plots/", replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer, pattern = "_VT.png", replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    select("VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
  monitor_information <- anon_file %>%
    filter(!is.na(height)) %>%
    filter(!is.na(res_width)) %>%
    mutate(res_height = res_width*0.5625,
           width = height*0.5625,
           dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
        select(c("dot_pitch", "participant", "res_width"))
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split images column into item and condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "size"), sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session",
                  "trials.thisN")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "size")), as_factor)) %>%
  select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(size = fct_relevel(size, c('D', 'C', 'B', 'A'))) %>% 
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(exp3_anon)

# remove anon df from environment

rm(exp3_anon)

# extract age and gender data

extract_age(E3_size_adjustments_tidy)

extract_gender(E3_size_adjustments_tidy)

extract_literacy(E3_size_adjustments_tidy)
```

# Abstract {#abstract-adjusting-size}

\chap{chap:adjusting_opacity} provided strong evidence for the effects of systematically varying the opacities
of scatterplot points on participants' estimates of correlation in positively 
correlated scatterplots. Utilising the same function and experimental paradigm,
I show in a single experiment that systematically varying the sizes of scatterplot
points is able to bias participants' estimates of correlation to a greater degree 
than manipulations that only adjust point opacity. In a condition where point size
decreases non-linearly as a function of residual distance, correlation estimation
is significantly biased upwards to correct for a historic underestimation bias
to a greater degree. I discuss the implications of these findings for the mechanisms
behind both opacity and size adjustments in scatterplots in relation to correlation
estimation, and recommend techniques for those who design with the estimation of
positive correlation in mind.

# Introduction {#introduction-adjusting-size}

While I was successful at changing participants' perceptions of correlation in 
positively correlated scatterplots in \chap{chap:adjusting_opacity}, the extent to which these perceptions
were changed was minimal. Figure \ref{fig-estimates-by-r-e2} illustrates how participants'
mean errors in *r* estimation changed as a function of the subjective *r* value.
Scatterplots employing non-linear opacity decay produced the most drastic changes
in correlation estimation, however this was still small, with an effect size of 
Cohen's *d* = 0.19. Recent evidence suggests that with regards to altering percepts
in scatterplots, changes in point size may be more effective than changes in 
opacity. In a fully-reproducible, large sample (N = 150) study, I show that 
systematically altering point size using the same function is not only able to
more effectively correct for the correlation underestimation bias, but is also
able to alter the shape of the correlation estimation curve.

# Related Work {#related-work-adjusting-size}

## Size and Perception 

## Scatterplot Point Size and Correlation Perception

# Methods {#methods-e3}

## Stimuli {#stimuli-e3}

The creation of stimuli in this experiment follows the same general principles
outlined in Section \chap{creating-stimuli}, \chap{chap:gen_methods}. As in \chap{chap:adjusting_opacity}, equation
1 was used to map point residuals to size values in the two non-linear decay conditions:

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

As in \chap{chap:adjusting_opacity}, the use of this equation produces a curve around
the identity line symmetrically opposing the underestimation curve found in previous
work. Additionally, a constant of 0.2 was added to each raw size value, and a scaling
factor of 4 was utilised; these adjustments resulted in the smallest points in
the present experiment having a width of 12 pixels on a 1920x1080 pixel monitor,
which is consistent with the point size used in the experiments described in 
\chap{chap:adjusting_opacity}. Examples of the stimuli used in this experiment
can be see in @fig-exp3-examples-chap5.

```{r}
#| label: fig-exp3-examples-chap5
#| include: true
#| fig-cap: Examples of the stimuli used in experiment 3, demonstrated with an \textit{r} value of 0.6.
#| out-width: NA
#| out-height: NA
#| fig-asp: 0.275

ggarrange(plot_example_function(data_with_resid, "Standard Size",
                                1, 0.2, 8),
          plot_example_function(data_with_resid, "Linear Decay",
                                1, (1-slopes$slope_linear), 7),
          plot_example_function(data_with_resid, "Non-linear Decay",
                                1, (1-slopes$slope_0.25), 7),
          plot_example_function(data_with_resid, "Inverted\nNon-linear Decay",
                                1, (1-slopes$slope_inverted), 7),
          nrow = 1)
```

## Dot Pitch in Crowdsourced Experiments {#dot-pitch-chap5}

```{r}
#| label: dot-pitch-e3

mean_dot_pitch <- mean(E3_size_adjustments_tidy$dot_pitch)

sd_dot_pitch <- sd(E3_size_adjustments_tidy$dot_pitch)

range_dot_pitch <- range(E3_size_adjustments_tidy$dot_pitch)
```

When the experiments described in \chap{chap:adjusting_opacity} took place, no method
of obtaining dot pitch was implemented. Dot
pitch is defined as the distance between the dots (sub-pixels) \cite{castellano_1992}
that make up each pixel. Calculating dot pitch is a requirement for the subsequent
calculation of the physical on-screen sizes of the scatterplot points that participants
saw. In the preamble to the current experiment, participants were asked to hold
a standard size credit/debit/ID card up to the monitor, and then to resize a
corresponding on-screen image until it matched the size of their physical card \cite{screenscale}.
These cards have a universal standard size (ISO/IEC 7810 ID-1), which when combined
with the monitor resolution information recorded by Psychopy, and assuming a widescreen
16:9 aspect ratio, allows for the inference of dot pitch and therefore the
physical size of the points in the experience. Mean dot pitch was
`r printnum(mean_dot_pitch)`mm ($SD = `r printnum(sd_dot_pitch)`$),
corresponding to a physical size on the screen of `r printnum(mean_dot_pitch*13)`mm
for the smallest points displayed. Section \ref{results-e3} includes analysis
that takes into account the physical on-screen sizes of scatterplot points.

## Point Visibility Testing

```{r}
# extract percentage correct on visual thresholds

vis_df <- E3_size_adjustments_tidy %>%
  group_by(VT_no_correct) %>%
  count()
```

It is key that the manipulations used do not remove (or appear to remove) data
from scatterplots. Therefore, point visibility testing is included in this experiment
prior to the experimental items. Participants were shown 6 scatterplots and were 
asked to enter in a text box how many points were being displayed. These points were
the same size as the smallest points displayed in the experimental items.
`r printnum(vis_df$n[1]/270, digits = 0)`% of participants were correct on
`r printnum(vis_df$VT_no_correct[1])` out of 6 point visibility tests, while
`r printnum(vis_df$n[2]/270, digits = 0)`% were correct on `r printnum(vis_df$VT_no_correct[2])` out of 6.
It should be noted that those participants scoring 5/6 did not answer
incorrectly, rather they did not answer at all for a particular question,
which is suggestive of a mis-click or an initial misunderstanding of
the task. Regardless, the results of this test indicate a sufficient level of 
point visibility.

## Design {#design-e3}

Again, a fully repeated-measures, within-participants design was employed. Each 
participant saw and responding to each of the 180 scatterplots in a fully
randomised order. There were four scatterplots for each of the 45 *r* values
corresponding to the four levels of the size decay condition, examples of which are 
shown in @fig-exp3-examples-chap5. The experiment itself is hosted on Pavlovia [^1].

[^1]:https://gitlab.pavlovia.org/Strain/exp_size_only

## Procedure {#procedure-e3}

Each participant viewed the PIS and provided consent through key presses in response
to consent statements. Participants were asked to provide their age in a free text
box, followed by their gender identity. Participants then completed the 5-item
Subjective Graph Literacy test \cite{garcia_2016}, followed by the screen scale
and point visibility tasks described above. Participants were shown example of 
scatterplots depicting *r* values of 0.2, 0.5, 0.8, and 0.95. Section \ref{results-e3}
contains discussion of the potential effects of this training. Following two
practice trials, participants worked through the series of 180 experimental and
six attention check trials in a randomised order. Visual masks preceded each plot.

## Participants {#participants-e3}

Normal to corrected-to-normal vision and English fluency were required. Participants who
had completed any of the experiments described in \chap{chap:adjusting_opacity}
were prevented from participating. Data were collected from 164 participants.
14 failed more than 2 out of 6 attention check questions,
and, as per the pre-registration, had their submissions rejected from the study.
The data from the remaining 150 participants were included in the full analysis
(`r printnum(E3_size_adjustments_tidy_gender$M, digits = 0)`% male, `r printnum(E3_size_adjustments_tidy_gender$F, digits = 0)`
% female, and `r printnum(E3_size_adjustments_tidy_gender$NB, digits = 0)`% non-binary). 
Participants' mean age was `r printnum(E3_size_adjustments_tidy_age$mean)`
(*SD* = `r printnum(E3_size_adjustments_tidy_age$sd)`). Mean graph literacy score
was `r printnum(E3_size_adjustments_tidy_graph_literacy$mean)` (*SD* =
`r printnum(E3_size_adjustments_tidy_graph_literacy$sd)`). The average
time taken to complete the experiment was 39 minutes (SD = 14 minutes). 

# Results {#results-e3}

```{r}
#| label: model-e3
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e3_model <- buildmer(difference ~ size +
                       (1 + size | participant) +
                       (1 + size | item),
                     data = E3_size_adjustments_tidy)
```

```{r}
#| label: model-e3-lit-dot-pitch
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build additional models (literacy, dot pitch, and training)

e3_lit_model <- add_fixed_effect(e3_model, "literacy", "E3_size_adjustments_tidy")

e3_dot_pitch_model <- add_fixed_effect(e3_model, "dot_pitch", "E3_size_adjustments_tidy")

e3_training_model <- add_fixed_effect(e3_model, "half", "E3_size_adjustments_tidy")
```

```{r}
#| label: model-e3-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

e3_model_cmpr <- comparison(e3_model)
```

```{r}
#| label: anova-results-e3

# do all anovas now

anova_results(e3_model, e3_model_cmpr)

anova_results(e3_lit_model, e3_model)

anova_results(e3_dot_pitch_model, e3_model)

anova_results(e3_training_model, e3_model)
```

To investigate the effects of point size adjustments on participants' estimates of
correlation, a linear mixed effects model was built whereby the point size condition
is a predictor for the difference between objective *r* values for each plot and 
participants' estimates of *r*. This model has random intercepts for participants
and items. A likelihood ratio test revealed that the model including size decay function
as a fixed effect explained significantly more variance than the null model
($\chi^2$(`r in_paren(e3_model.df)`) = `r printnum(e3_model.Chisq)`,
*p* `r printp(e3_model.p, add_equals = TRUE)`). @fig-e3-estimates shows the mean
errors in correlation estimation for each size decay condition, along with 95% confidence intervals.

```{r}
#| label: fig-e3-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in experiment 3. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error. The overestimation zone is included to facilitate comparison to later work.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e3_model) == "buildmer") e3_model <- e3_model@model

plotting_df <- as_tibble(emmeans(e3_model, pairwise ~ "size")[[1]]) %>%
  mutate(size = fct_relevel(size, c("A", "B", "C", "D"))) %>%
  mutate(size = recode(size, !!!labels_e3)) %>%
  rename("Condition" = "size")

ggplot(aes(x = Condition, y = emmean*-1), data = plotting_df) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

rm(plotting_df)
```
```{r}
#| label: tbl-contrasts-e3
#| include: true
#| tbl-cap: Contrasts between different levels of the size decay factor in experiment 3.

# assign slot, if it hasn't been assigned already

if (class(e3_model) == "buildmer") e3_model <- e3_model@model

# make table df

contrasts_extract(e3_model, "size") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e3_regex)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 1), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

This effect was driven by significant differences between means of correlation 
estimation error between all conditions. Statistical
tests for contrasts were performed using the `emmeans` package \cite{lenth_2024}, 
and are shown in @tbl-contrasts-e3. To test whether the observed results could be
explained by difference in participants' levels of graph literacy, an additional
model was built. This model is identical to the experimental model, but also
includes graph literacy as a fixed effect. Including graph literacy as a fixed effect
explained no additional variance ($\chi^2$(`r in_paren(e3_lit_model.df)`) =
`r printnum(e3_lit_model.Chisq)`, *p* `r printp(e3_lit_model.p, add_equals = TRUE)`),
indicating that the differences observed in participants' correlation estimation 
performance were not as a result of differences in levels of graph literacy.

While participants performed well on the point visibility task, another facet
of using a larger or smaller monitor with a lower or higher resolution could have
affected estimates of correlation. Comparing a model including the dot pitch of
participants' monitors to the experimental model revealed a significant main effect
($\chi^2$(`r in_paren(e3_dot_pitch_model.df)`) = `r printnum(e3_dot_pitch_model.Chisq)`,
*p* `r printp(e3_dot_pitch_model.p, add_equals = TRUE)`). There was no
interaction between size decay condition and dot pitch; a 0.1mm decrease in dot pitch
resulted in correlation estimates decreasing by .03. Given the low range of dot pitches
gathered from participants `r printnum(range_dot_pitch[1])`mm to 
`r printnum(range_dot_pitch[2])`mm, the effect is not substantial enough to warrant
further discussion.

```{r}
#| label: tbl-efs-e3
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (left) and summary statistics (right) for levels of the size decay factor in experiment 3. Each effect size is compared to the reference level, termed, "Standard Size". 
#| tbl-subcap: ["A", "B"]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

options(knitr.kable.NA = "")

efs <- lme.dscore(e3_model, E3_size_adjustments_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "sizeD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "sizeA" = "Non-Linear Decay",
                         "sizeB" = "Linear Decay",
                         "sizeC" = "Inverted Decay",
                         "sizeD" = "Standard Size")) %>%
  rename("Cohen's \\textit{d}" = "d") 

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

emmeans(e3_model, pairwise ~ "size")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Size = size,
         Mean = emmean,
         "Standard\nError" = SE) %>%
  mutate("Size" = recode(Size,
                         "A" = "Non-Linear Decay",
                         "B" = "Linear Decay",
                         "C" = "Inverted Decay",
                         "D" = "Standard Size")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

```{r}
#| label: fig-estimates-by-r-e3
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by factor and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring the non-linear size decay function. Error bars show standard deviations of estimates.

plot_error_bars_function(E3_size_adjustments_tidy %>%
                           mutate(grouping_var = fct_relevel(size,
                                                             c("A",
                                                               "B",
                                                               "C",
                                                               "D")))
                         , "size", "difference", labels_e3) +
  geom_hline(yintercept = 0, linetype = 2)
```

# Discussion {#discussion-e3}

Participants' errors in correlation estimation were significantly lower in the non-linear
size decay condition (see @fig-estimates-by-r-e3) compared to all other conditions.
This finding provides support for the first hypothesis. Conversely, no support
was found for the second hypothesis, that estimates would be least accurate
in the inverted non-linear size decay condition. Errors in this condition were
significantly higher than for the other two decay conditions, but were significantly
lower than the errors that were observed for the standard size condition.

## Increased Correlation Estimation Accuracy

The mean error in correlation estimation for the non-linear size decay condition
in this experiment was .025, while for the equivalent opacity decay condition described
in experiment 2 (see \chap{chap:adjusting_opacity}) was .086. This finding 
provides evidence that point size is a stronger encoding channel for the manipulation
of perceived contrast than point opacity. If these effects are being driven by
the lower salience of more external points, the fact that a larger effect of point
size has been reported is congruent with research showing clear influences of stimulus
size on object salience and perceptual weighting \cite{grice_1983, hong_2022, healey_2011}.
The present results therefore provide support for point salience/perceptual weighting
being a key driver of the effects observed; lower point salience brought on by 
reduced point size in the exterior of the scatterplot reduces the perceived width
of the distribution of data points around the regression line, biasing estimates
of correlation upwards and leading to a higher degree of accuracy. Other candidate
mechanisms do exist; similar results would be expected if a feature-based attentional
bias was responsible \cite{sun_2016, hong_2022}; the current methodology does not
allow for distinguishing between these explanations, and it may be that both are 
partially responsible.

The lack of support for the second hypothesis is surprising, and suggests that 
point salience and perceived distributional width do not form the whole story. There
is evidence that larger stimuli exhibit greater levels of spatial uncertainty \cite{alais_2004},
and it is possible that this uncertainty results in a perceptual under-weighting of 
contribution of these points during correlation estimation. This is consistent 
with previous work \cite{warren_2002, warren_2004} suggesting that the brain
may make robust statistical use of visuo-spatial information. These mechanisms
act to downweight the influence of less reliable information (in this case the higher
spatial uncertainty of larger exterior points) on subsequent perceptual estimates.
In the present experiment, this resulted in participants making more accurate 
estimates of correlation in the inverted decay condition compared to the 
standard size condition. It was suggested in \chap{chap:adjusting_opacity} that
inverted opacity manipulations may be employed to correct for the *overestimation*
of correlation observed with negatively correlation scatterplots \cite{sher_2017};
findings here indicate that using an inverted size decay function in this way may
not be appropriate.

## Constant Correlation Estimation Precision

Unlike the experiments described in \chap{chap:adjusting_opacity}, in which standard
deviations of correlation estimation errors generally became smaller as the actual
*r* value increased, distributions of standard deviations here remained mostly constant.
This was unexpected, as previous work \cite{rensink_2010, rensink_2012, rensink_2014, rensink_2017}
routinely finds precision in *r* estimation to increase with the objective *r* value.
This finding may be related to the nature of the stimuli in the current study.
At high values of *r* there is a large amount
of overlap between points in the non-linear, non-linear inverted,
and linear size decay conditions. This overlap may blur the percept and
account for the absence of this effect, however cannot account for these findings 
with regards to the standard size condition. Aside from the inverted non-linear decay condition in
\chap{chap:adjusting_opacity}, the finding that precision increased with *r*
was robust. Its absence here is curious given that the standard size decay condition
here is identical to the full opacity condition in that chapter.
Relying on relative judgements means the interplay between scatterplots with 
different visual features must be accounted for within a particular experiment.
The stimuli as *r* approaches 1 in the current study exhibit greater levels of
visual variance than the stimuli in experiments 1 and 2 (see \chap{chap:adjusting_opacity}),
which may explain the lack of increased precision here. Further testing is required
for a more concrete explanation.

Ultimately, my aim is to provide tools for the design of visualisations
more suited for the tasks they are intended to support. When that task is 
the perception of positive correlation, the use of the non-linear size
decay condition described here is recommended. For other scatterplot tasks,
such as cluster separation or numerosity perception, or
other chart types, the use of the size manipulation may in fact be a hindrance.

## Training {#training-e3}

Before beginning the experimental trials, participants viewed plots depicting
*r* = 0.2, 0.5, 0.8, and 0.95 for a minimum of 8 seconds. Comparing a model
including session half as a fixed effect with the origin experimental model
revealed no significant effect ($\chi^2$(`r in_paren(e3_training_model.df)`)
= `r printnum(e3_training_model.Chisq)`, *p* `r printnum(e3_training_model.p, add_equals = TRUE)`),
suggesting that having more recently viewed the example plots did not have an effect
on participants' performance.

## Limitations {#limitations-e3}

Despite confirming a method of obtaining dot pitch, no method of obtaining
head-to-monitor distances is available. This, along with the comparative correlation
judgements collected, prevents concrete psychophysical conclusions from being made.
Instead, the experimental paradigm allows for findings that are rigorous to 
different viewing contexts and are of particular importance for the HCI and visualisation
design audiences. It may be that a high level perceptual phenomenon is responsible for the effects 
have seen here; investigating this is beyond the scope of the current study and
does not negate the findings. There is also the potential for misinterpretation
of the scatterplots presented in the current study, especially given their similarity in
form, but not purpose to bubble charts.

## Future Work {#future-work-e3}

Evidence has been found for robust effects of varying point opacity and point
size on correlation estimation in positively correlated scatterplots. There are 
different effects of changing these visual features, and these effects are also
partially dependent on the objective *r* value in the scatterplot. Future work
should investigate the combination of these visual manipulations as they pertain
to correlation estimation.

Additionally, future qualitative work is required to investigate whether the modified
scatterplots presented result in misinterpretation or a decrease in the levels of
trust people place in data visualisations.

# Conclusion

I conducted a single experiment, in which points on scatterplots had their sizes
systematically changed according to their distance from the regression line. The
equation relating point size to residual distance was identical to that used in
\chap{chap:adjusting_opacity}, with the addition of a scaling factor and constant
to provide parity between the smallest points in the previous experiment and those
in the current. Additionally, I tested the visibility of the smallest points used
to ensure that no data were functionally removed, as well as collecting the sizes and
resolutions of participants' monitors. I found evidence that reducing the size
of scatterplot points as a function of residual error using a non-linear equation
not only producing significantly more accurate estimates of positive correlation,
but also subtly changed the fundamental shape of the estimation curve in a way not previously
reported. Unlike the inverted opacity decay function used in \chap{chap:adjusting_opacity},
the inverted size decay function here did not produce the least accurate estimates
of correlation; these were found when participants viewed the "standard size" plot
devoid of a size manipulation. Taken together, these results suggest a greater potential
for manipulating correlation estimates when using point size compared to point
opacity.
