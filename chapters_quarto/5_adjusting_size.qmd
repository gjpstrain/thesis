---
title: "adjusting_size"
output:
  format: 
    latex:

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/"
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

```{r}
#| label: setup

# load packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(geomtextpath)
library(conflicted)
library(formattable)
library(effectsize)

# fix conflicts

conflicts_prefer(dplyr::select(),
                 dplyr::filter(),
                 lme4::lmer())

# define plotting labels now

labels_e3 <- c(A = "Non-Linear\nDecay",
               B = "Linear\nDecay",
               C = "Inverted\nDecay",
               D = "Standard\nSize")

labels_e3_regex <- c(
  "\\bA\\b" = "Non-Linear Decay",
  "\\bB\\b" = "Linear Decay",
  "\\bC\\b" = "Inverted Decay",
  "\\bD\\b" = "Standard Size"
)

# read in shared_functions

source("../shared_functions.R")

# prepare slopes for plotting examples

slopes_0.2 <- prepare_slopes(0.2)
slopes_0.6 <- prepare_slopes(0.6)
slopes_0.99 <- prepare_slopes(0.99)
```

```{r}
#| label: load-data

# load in data, rename experiment

exp3_anon <- read_csv("../data/exp_3_data.csv") %>%
  mutate("expName" = recode(expName,
                            "exp_size_only" = "E3_size_adjustments"))
```

```{r}
#| label: retrieve-cached-models-chap5

# load cached models

if (!params$eval_models){ lazyload_cache_dir("../thesis_cache/") }
```

```{r}
#| label: wrangle-data-chap5

# NB: With the exception of anonymisation, data are provided as-is from pavlovia.
# Wrangling functions must be run first to make the dataset usable.

wrangle <- function(anon_file) {
  
  # first do literacy
  
    literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
    
    # extract and process visual threshold testing
  
  visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    select("VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
  monitor_information <- anon_file %>%
    filter(!is.na(height)) %>%
    filter(!is.na(res_width)) %>%
    mutate(res_height = res_width*0.5625,
           width = height*0.5625,
           dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
        select(c("dot_pitch", "participant", "res_width"))
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images,
                              pattern = "A",
                              replacement = "-A")) %>%
  mutate(images = str_replace(images,
                              pattern = "B",
                              replacement = "-B")) %>%
  mutate(images = str_replace(images,
                              pattern = "C",
                              replacement = "-C")) %>%
  mutate(images = str_replace(images,
                              pattern = "D",
                              replacement = "-D")) %>%
  separate(images, c("item", "size"), sep = "-") %>%
  mutate(size = str_replace(size,
                            pattern = ".png",
                            replacement = "")) %>%
  mutate(item = str_replace(item,
                            pattern = "all_plots/",
                            replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session",
                  "trials.thisN")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item",
                          "size")),
                as_factor)) %>%
  select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(size = fct_relevel(size, c('D', 'C', 'B', 'A'))) %>% 
  assign(paste0(unique(anon_file$expName),
                "_tidy"),
           value = .,
         envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(exp3_anon)

# write wrangled csv if it does not exist

if (!file.exists(csv_wrangled <- "../data/wrangled_data/chap5_exp1_wrangled.csv")) {
  
  E3_size_adjustments_tidy %>%
    rename(condition = size) %>%
    mutate(condition = paste0(condition, "_exp3")) %>%
    write_csv("../data/wrangled_data/chap5_exp1_wrangled.csv")
  
}

# remove anon df from environment

rm(exp3_anon)

# extract age and gender data

extract_age(E3_size_adjustments_tidy)

extract_gender(E3_size_adjustments_tidy)

extract_literacy(E3_size_adjustments_tidy)
```

# Abstract {#abstract-adjusting-size}

\chap{chap:adjusting_opacity} provided strong evidence for the effects of
systematically varying the opacities of scatterplot points on participants'
estimates of correlation in positively correlated scatterplots. Utilising the
same function and experimental paradigm, I show in a single experiment that
systematically varying the sizes of scatterplot points is able to bias
participants' estimates of correlation to a greater degree than manipulations
that only adjust point opacity. In a condition where point size decreases
non-linearly as a function of residual magnitude, correlation estimation is
significantly biased upwards to correct for the underestimation bias to a
greater degree. I discuss the implications of these findings for the mechanisms
behind both opacity and size adjustments in scatterplots in relation to
correlation estimation, and recommend techniques for those who design with the
estimation of positive correlation in mind.

# Introduction {#introduction-adjusting-size}

While I was successful at changing participants' perceptions of correlation in
positively correlated scatterplots in \chap{chap:adjusting_opacity}, the extent
to which these perceptions were changed was minimal. Figure
\ref{fig-estimates-by-r-e2} illustrates how participants' mean errors in *r*
estimation changed as a function of the subjective *r* value. Scatterplots
employing non-linear inverted opacity decay produced the most drastic changes in
correlation estimation, however these changes were still small, with an effect
size of Cohen's *d* = 0.23. Recent evidence suggests that with regards to
altering percepts in scatterplots, changes in point size may be more effective
than changes in opacity. In a fully-reproducible, large sample (*N* = 150)
study, I show that systematically altering point size using the same function is
not only able to more effectively correct for the correlation underestimation
bias, but is also able to alter the shape of the correlation estimation curve.

# Related Work {#related-work-adjusting-size}

## Point Size and the Perception of Correlation in Scatterplots {#point-size-chap5}

```{r}
#| label: fig-bubble-chart
#| include: true
#| fig-cap: An example of a bubble chart. This plot compares car engine displacement (cubic inches) with fuel efficiency (miles per gallon). Additionally, the number of cylinders in the car's engine are encoded with point size. One can see from this plot that vehicles with higher engine displacement and lower fuel efficiency tend to have a greater number of engine cylinders.
#| fig-asp: 0.6

# create bubble chart plot by setting size as a function of "cyl"

ggplot(aes(x = mpg, y = disp), data = mtcars) +
  geom_point(aes(size = cyl), shape = 16) +
  theme_ggdist() +
  labs(x = "Fuel Efficiency (miles per gallon)",
       y = "Engine Displacement (cubic inches)",
       size = "Number of\nCylinders") +
  theme(legend.position = c(0.9, 0.7),
        legend.background = element_rect(fill = "white", colour = "black"))
```

Opacity adjustments have been used extensively to solve issues of overplotting
and clutter in scatterplots \cite{bertini_2004, matejka_2015}. Figure
\ref{fig-overplotting-examples} in \chap{chap:adjusting_opacity} demonstrates
this usage. Practicality also dictates that scatterplots visualising
sufficiently large datasets inherently require their points to be smaller to
minimise obfuscation of the data they portray. Aside from being used to increase
point visibility, point size changes in scatterplots have also been employed in
the creation of bubble charts; here, the size of a scatterplot point is mapped
to a third variable. @fig-bubble-chart demonstrates this class of scatterplot
using the included `mtcars` dataset in `ggplot2`.

Despite the popularity of such charts, at the time of writing, very little
investigation into how point size affects correlation perception in scatterplots
had taken place. That which had been completed found bias and variability in
correlation perception performance to be invariant to changes in point size
\cite{rensink_2012, rensink_2014}, however these studies were low-powered (*N* = 12)
considering the small effect sizes found in the experiments described in
\chap{chap:adjusting_opacity}. From the wider literature, there is evidence that
larger points in scatterplots can bias judgements of mean point position to a
greater degree than point opacity can \cite{hong_2022}, in what is termed the
*weighted average illusion*. Outside of scatterplots and scatterplot-adjacent
chart types, there is evidence that increased size can result in faster reaction
times to peripherally presented stimuli \cite{grice_1983}, and that larger
stimuli are associated with lower levels of spatial certainty \cite{alais_2004},
but higher levels of salience \cite{healey_2012}. The current experiment should
therefore facilitate discrimination between these candidate drivers of the
effects observed in a way that was not possible when manipulating opacity, as in
that case effects of salience or spatial certainty would operate in the same
direction.

# Hypotheses

Based on the findings from \chap{chap:adjusting_opacity}, and on evidence from
the wider literature described above, two hypotheses are made:

-   H1: The non-linear decay function, in which scatterplot points become
    smaller when they reside further from the regression line, will result in
    lower mean errors in correlation estimation than for linear decay and
    standard size conditions.
-   H2: The use of an inverted non-linear decay function, in which scatterplot
    points become larger when they reside further away from the regression line,
    will result in higher mean errors in correlation estimation than for all
    other conditions.

# Method {#method-e3}

## Open Research {#open-research-chap5}

The experiment was conducted according to the principles of open and
reproducible research \cite{ayris_2018}. All data and code for the original
paper are maintained in a GitHub repository[^1]. This repository also features
an implementation of a Docker container that enables the full recreation of the
computational environment the original paper was written in. The experiment
itself is hosted on GitLab[^2]. The hypotheses and analysis plans were
pre-registered with the Open Science Framework (OSF)[^3], and there were no
deviations from them.

[^1]: https://github.com/gjpstrain/size_and_scatterplots

[^2]: https://gitlab.pavlovia.org/Strain/exp_size_only

[^3]: https://osf.io/k4gd8

## Stimuli {#stimuli-e3}

The creation of stimuli in this experiment follows the same general principles 
as outlined in Section \ref{creating-stimuli}, \chap{chap:gen_methods}, and was
performed using `ggplot2` (version 3.4.4). As in \chap{chap:adjusting_opacity},
equation 5.1 was used to map point residuals to size values in the two
non-linear decay conditions:

\begin{equation}
  point_{size} = 1 - b^{residual}
\end{equation}

Again, a value of $b = 0.25$ was used. As in \chap{chap:adjusting_opacity}, the
use of this equation produces a curve around the identity line symmetrically
opposing the underestimation curve found in previous work. Additionally, a
constant of 0.2 was added to each raw size value and a scaling factor of 4 was
utilised; these adjustments resulted in the smallest points in the present
experiment having a width of 12 pixels, which is consistent with the point size
used in the experiments described in \chap{chap:adjusting_opacity}. Examples of 
the stimuli used in this experiment can be see in @fig-exp3-examples-chap5.

```{r}
#| label: fig-exp3-examples-chap5
#| include: true
#| fig-cap: Examples of the stimuli used in Experiment 3, demonstrated with \textit{r} values of 0.2, 0.6, and 0.99.
#| fig-asp: 0.8

# example plots for experiment 3, arranged with ggarrange()

row_1 <- ggarrange(plot_example_function(slopes_0.2, "Standard Size",
                                         1, 0.2, 8),
                   plot_example_function(slopes_0.2, "Linear Decay",
                                         1, (1-slopes_0.2$slope_linear), 8),
                   plot_example_function(slopes_0.2, "Non-linear Decay",
                                         1, (1-slopes_0.2$slope_0.25), 8),
                   plot_example_function(slopes_0.2, "Inverted Non-linear Decay",
                                         1, (1-slopes_0.2$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.2", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_2 <- ggarrange(plot_example_function(slopes_0.6, NULL,
                                         1, 0.2, 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_linear), 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_0.25), 7),
                   plot_example_function(slopes_0.6, NULL,
                                         1, (1-slopes_0.6$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.6", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

row_3 <- ggarrange(plot_example_function(slopes_0.99, NULL,
                                         1, 0.2, 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_linear), 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_0.25), 7),
                   plot_example_function(slopes_0.99, NULL,
                                         1, (1-slopes_0.99$slope_inverted), 7),
                   nrow = 1) %>%
  annotate_figure(left = gridtext::richtext_grob("<i>r</i> = 0.99", rot = 90,
                                                 gp = grid::gpar(fontsize = 10)))

ggarrange(row_1, row_2, row_3, ncol = 1, heights = c(1.2, 1, 1))
```

## Dot Pitch in Crowdsourced Experiments {#dot-pitch-chap5}

```{r}
#| label: dot-pitch-e3

# get mean, sd, and range of dot pitch measurements
# save these in the global environment so they can referenced in text

mean_dot_pitch <- mean(E3_size_adjustments_tidy$dot_pitch)

sd_dot_pitch <- sd(E3_size_adjustments_tidy$dot_pitch)

range_dot_pitch <- range(E3_size_adjustments_tidy$dot_pitch)
```

```{r}
#| label: fig-screen-scale-illustration
#| include: true
#| fig-cap: A mock-up of the screen scale \cite{screenscale} task used to infer the sizes of participants' monitors.

# find screen scale illustration png in supplied graphics folder
# this was handmade using royalty-free graphics found online

knitr::include_graphics(path = "../supplied_graphics/screen_scale_illustration.png",
                        dpi = NA)
```

When the experiments described in \chap{chap:adjusting_opacity} took place, no
method of obtaining dot pitch was implemented. Dot pitch is defined as the
distance between the dots (sub-pixels) \cite{castellano_1992} that make up each 
individual pixel. Calculating dot pitch is a requirement for the subsequent calculation of
the physical on-screen sizes of the scatterplot points that participants saw. In
the preamble to the current experiment, participants were asked to hold a
standard size credit/debit/ID card up to the monitor, and then to resize a
corresponding on-screen image using their keyboard arrows until it matched the
size of their physical card \cite{screenscale}. These cards have a universal
standard size (ISO/IEC 7810 ID-1), which when combined with the monitor
resolution information recorded by PsychoPy, and assuming a widescreen 16:9
aspect ratio, allows for the inference of dot pitch and therefore the physical
size of the points in the experiment. Mean dot pitch was
`r printnum(mean_dot_pitch)`mm ($SD = `r printnum(sd_dot_pitch)`$),
corresponding to a physical size on the screen of
`r printnum(mean_dot_pitch*13)`mm for the smallest points displayed. Section
\ref{results-e3} includes analysis that takes into account the physical
on-screen sizes of scatterplot points.

## Point Visibility Testing {#point-visibility-testing-e3}

```{r}
#| label: visibility-e3

# extract a dataframe of participants' 
# performance on the visibility threshold testing

vis_df_e3 <- E3_size_adjustments_tidy %>%
  group_by(VT_no_correct) %>%
  count()
```

It is key that the manipulations used do not remove (or appear to remove) data
from scatterplots. Therefore, point visibility testing is included in this
experiment prior to the experimental items. Participants were shown 6
scatterplots and were asked to enter in a text box how many points were being
displayed. These points were the same size as the smallest points displayed in
the experimental items. `r printnum(vis_df_e3$n[1]/270, digits = 0)`% of
participants were correct on `r printnum(vis_df_e3$VT_no_correct[1])` out of 6
point visibility tests, while `r printnum(vis_df_e3$n[2]/270, digits = 0)`% were
correct on `r printnum(vis_df_e3$VT_no_correct[2])` out of 6. It should be noted
that those participants scoring 5/6 did not answer incorrectly, rather they did
not answer at all for a particular question, which is suggestive of a mis-click
or an initial misunderstanding of the task. Regardless, the results of this test
indicate a sufficient level of point visibility.

## Design {#design-e3}

A fully repeated-measures, within-participants design was employed. Each
participant saw and responded to each of the 180 scatterplots in a fully
randomised order. There were four scatterplots for each of the 45 *r* values
corresponding to the four levels of the size decay condition, examples of which
are shown in @fig-exp3-examples-chap5.

## Procedure {#procedure-e3}

Ethical approval for this experiment was granted by the University of 
Manchesterâ€™s Computer Science Departmental Panel (Ref: 2022-14660-24397).
Each participant viewed the PIS and provided consent through key presses in
response to consent statements. Participants were asked to provide their age in
a free text box, followed by their gender identity. Participants then completed
the 5-item Subjective Graph Literacy test \cite{garcia_2016}, followed by the
screen scale and point visibility tasks described above. Participants were shown
examples of scatterplots depicting *r* values of 0.2, 0.5, 0.8, and 0.95. Section
\ref{results-e3} contains a discussion of the potential effects of this
training. Following two practice trials, participants worked through the series
of 180 experimental and six attention check trials in a randomised order. Visual
masks preceded each plot.

## Participants {#participants-e3}

150 participants were recruited using the Prolific platform \cite{prolific}.
Normal or corrected-to-normal vision and English fluency were required.
Participants who had completed any of the experiments described in
\chap{chap:adjusting_opacity} were prevented from participating. Data were
collected from 164 participants. 14 failed more than 2 out of 6 attention check
questions, and, as per the pre-registration, had their submissions rejected from
the study. The data from the remaining 150 participants were included in the
full analysis (`r printnum(E3_size_adjustments_tidy_gender$M)` male,
`r printnum(E3_size_adjustments_tidy_gender$F)` female, and
`r printnum(E3_size_adjustments_tidy_gender$NB)` non-binary). Participants' mean
age was `r printnum(E3_size_adjustments_tidy_age$mean, digits = 1)` (*SD* =
`r printnum(E3_size_adjustments_tidy_age$sd, digits = 1)`). Mean graph literacy score was
`r printnum(E3_size_adjustments_tidy_graph_literacy$mean, digits = 1)` (*SD* =
`r printnum(E3_size_adjustments_tidy_graph_literacy$sd, digits = 1)`) out of 30. The mean time
taken to complete the experiment was 39 minutes (SD = 14 minutes).

# Results {#results-e3}

```{r}
#| label: model-e3
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create experiment 3 model

e3_model <- buildmer(difference ~ size +
                       (1 + size | participant) +
                       (1 + size | item),
                     data = E3_size_adjustments_tidy)
```

```{r}
#| label: model-e3-lit-dot-pitch
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build additional models (literacy, dot pitch, and training)

e3_lit_model <- add_fixed_effect(e3_model,
                                 "literacy",
                                 "E3_size_adjustments_tidy")

e3_dot_pitch_model <- add_fixed_effect(e3_model,
                                       "dot_pitch",
                                       "E3_size_adjustments_tidy")

e3_training_model <- add_fixed_effect(e3_model,
                                      "half",
                                      "E3_size_adjustments_tidy")
```

```{r}
#| label: model-e3-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create null model for comparison purposes

e3_model_cmpr <- comparison(e3_model)
```

```{r}
#| label: anova-results-e3

# do all anovas now
# anova_results function from shared_functions.R outputs model statistics 
# to global environment

anova_results(e3_model, e3_model_cmpr)

anova_results(e3_lit_model, e3_model)

anova_results(e3_dot_pitch_model, e3_model)

anova_results(e3_training_model, e3_model)
```

To investigate the effects of point size adjustments on participants' estimates
of correlation, a linear mixed effects model with point size
condition as a predictor for the difference between objective *r* values for
each plot and participants' estimates of *r* was built. This model has random intercepts
for participants and items. A likelihood ratio test revealed that the model
including size decay function as a fixed effect explained significantly more
variance than the null model ($\chi^2$(`r in_paren(e3_model.df)`) =
`r printnum(e3_model.Chisq)`, *p* `r printp(e3_model.p, add_equals = TRUE)`).
@fig-e3-estimates shows the mean errors in correlation estimation for each size
decay condition, along with 95% confidence intervals.

```{r}
#| label: fig-e3-estimates
#| include: true
#| fig-cap: Estimated marginal means for the four conditions tested in Experiment 3. 95\% confidence intervals are shown. The vertical dashed line represents no estimation error. The overestimation zone is included to facilitate comparison to later work.
#| fig-asp: 0.4

# assign buildmer model to lmer slot

if (class(e3_model) == "buildmer") e3_model <- e3_model@model

plotting_df <- as_tibble(emmeans(e3_model, pairwise ~ "size")[[1]]) %>%
  mutate(size = fct_relevel(size, c("A", "B", "C", "D")))

# if it doesn't exist, output emmeans dataframe for use in Chapter 8

if (!file.exists(csv_wrangled <- "../data/other_data/exp3_emm_plot.csv")) {
  
  plotting_df %>%
    rename(condition = size) %>%
    mutate(condition = paste0(condition, "_exp3")) %>%
    write_csv("../data/other_data/exp3_emm_plot.csv")
  
}

# plot EMMs

plotting_df %>%
  mutate(size = recode(size, !!!labels_e3)) %>%
  rename("Condition" = "size") %>%
  ggplot(aes(x = Condition, y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  theme_ggdist() +
  labs(y = "Estimated Marginal Mean of Error",
       x = "Condition") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(type = "closed", length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3) +
  annotate(geom = "text",
           x = 2.5,
           y = 0.008,
           label = "No Estimation Error",
           size = 2.5,
           angle = 270)

# remove extraneous plotting dataframe 

rm(plotting_df)
```

```{r}
#| label: tbl-contrasts-e3
#| include: true
#| tbl-cap: Contrasts between different levels of the size decay factor in Experiment 3.

# assign slot, if it hasn't been assigned already

if (class(e3_model) == "buildmer") e3_model <- e3_model@model

# make table df, then use kblExtra to create table

contrasts_extract(e3_model, "size") %>%
  mutate(Contrast = str_replace_all(Contrast, labels_e3_regex)) %>%
  mutate(Contrast = str_replace(Contrast, " - ", " | ")) %>%
  separate(Contrast, into = c(" ", "  "), sep = " \\| ") %>%
  kbl(booktabs = TRUE, digits = c(0, 2, 1), escape = FALSE) %>%
  add_header_above(c("Contrast" = 2, "Statistics" = 2))
```

This effect was driven by significant differences between means of correlation
estimation error between all conditions. Statistical tests for contrasts were
performed using the `emmeans` package \cite{lenth_2024}, and are shown in
@tbl-contrasts-e3. To test whether the observed results could be explained by
differences in participants' levels of graph literacy, an additional model was
built. This model is identical to the experimental model, but also includes
graph literacy as a fixed effect. Including graph literacy as a fixed effect
explained no additional variance ($\chi^2$(`r in_paren(e3_lit_model.df)`) =
`r printnum(e3_lit_model.Chisq)`, *p*
`r printp(e3_lit_model.p, add_equals = TRUE)`), indicating that the differences
observed in participants' correlation estimation performance were not as a
result of differences in levels of graph literacy.

While participants performed well on the point visibility task, another facet of
using a larger or smaller monitor with a lower or higher resolution could have
affected estimates of correlation. Comparing a model including the dot pitch of
participants' monitors to the experimental model revealed a significant main
effect ($\chi^2$(`r in_paren(e3_dot_pitch_model.df)`) =
`r printnum(e3_dot_pitch_model.Chisq)`, *p*
`r printp(e3_dot_pitch_model.p, add_equals = TRUE)`). There was no interaction
between size decay condition and dot pitch; a 0.1mm decrease in dot pitch
resulted in correlation estimates decreasing by .03. Given the low range of dot
pitches gathered from participants (`r printnum(range_dot_pitch[1])`mm to
`r printnum(range_dot_pitch[2])`mm), the effect is not substantial enough to
warrant further discussion.

```{r}
#| label: tbl-efs-e3
#| include: true
#| tbl-cap: Cohen's \textit{d} effect sizes (a) and summary statistics (b) for levels of the size decay factor in Experiment 3. Each effect size is compared to the reference level, termed "Standard Size". 
#| tbl-subcap: ["Effect sizes for size decay factor.", "Summary statistics for size decay factor."]
#| layout-ncol: 2

# this approximation uses t values and the pooled standard deviation of the differences

# first set kableExtra options

options(knitr.kable.NA = "")

# lme.dscore from shared_functions.R is reproduced from the now-defunct
# EMAtools R package

# create efs df

efs <- lme.dscore(e3_model, E3_size_adjustments_tidy, type = "lme4") %>%
  as_tibble(rownames = "Effect") %>%
  select(-c(t, df)) %>%
  add_row(Effect = "sizeD", d = NA_real_, .before = 1) %>%
  mutate(Effect = recode(Effect,
                         "sizeA" = "Non-Linear Decay",
                         "sizeB" = "Linear Decay",
                         "sizeC" = "Inverted Decay",
                         "sizeD" = "Standard Size")) %>%
  rename("Cohen's \\textit{d}" = "d") 

# use kbl() to create table

kbl(efs, booktabs = TRUE, digits = c(0, 2), escape = FALSE)

# pairwise comparisons

emmeans(e3_model, pairwise ~ "size")[[1]] %>%
  as_tibble() %>%
  select(-c(df, asymp.LCL, asymp.UCL)) %>%
  rename(Size = size,
         "Mean of Error" = emmean,
         "Standard Error" = SE) %>%
  mutate("Size" = recode(Size,
                         "A" = "Non-Linear Decay",
                         "B" = "Linear Decay",
                         "C" = "Inverted Decay",
                         "D" = "Standard Size")) %>%
  kbl(booktabs = T, digits = c(1, 2, 3))
```

```{r}
#| label: fig-estimates-by-r-e3
#| include: true
#| fig-cap: Participants' mean errors in correlation estimates grouped by condition and by \textit{r} value. The dashed horizontal line represents perfect estimation. Participants were most accurate when presented with the plots featuring the non-linear size decay function. Error bars show standard deviations of estimates.

# plot error bars for experiment 3

plot_error_bars_function(E3_size_adjustments_tidy %>%
                           mutate(grouping_var = fct_relevel(size,
                                                             c("A",
                                                               "B",
                                                               "C",
                                                               "D")))
                         , "size", "difference", labels_e3) +
  geom_hline(yintercept = 0, linetype = 2)
```

As in \chap{chap:adjusting_opacity}, an approximation of Cohen's *d* was
calculated between the reference level (here labelled "Standard Size") and all
other levels of the size decay factor. These statistics can be seen in
@tbl-efs-e3 along with descriptive statistics for all levels. The largest
observed effect size is between the standard size and non-linear decay
conditions (*d* = 0.64), and is medium. This is a marked improvement on the
non-linear opacity decay condition from Experiment 2,
\chap{chap:adjusting_opacity}, where the effect size observed was *d* = 0.19.
@fig-estimates-by-r-e3 plots the effects of each size decay manipulation
separately for each *r* value used. Points represent means of estimation error
for each *r* value, with standard deviations of error shown as error bars. The
dashed horizontal line represents hypothetical perfect estimation. Expectedly,
while participants still underestimated *r* in all conditions, the non-linear
size decay condition provided the most promising correction to date; the average
estimation error is approaching a flat line (see the left-most plot in
@fig-estimates-by-r-e3). The change in the shape of the correlation estimation
is a novel finding seen with the non-linear and linear decay conditions.


# Discussion {#discussion-e3}

Participants' errors in correlation estimation were significantly lower in the
non-linear size decay condition (see @fig-estimates-by-r-e3) compared to all
other conditions. This finding provides support for the first hypothesis.
Conversely, no support was found for the second hypothesis, that estimates would
be least accurate in the inverted non-linear size decay condition. Errors in
this condition were significantly higher than for the other two decay
conditions, but were significantly lower than the errors that were observed for
the standard size condition.

## Increased Correlation Estimation Accuracy

The mean error in correlation estimation for the non-linear size decay condition
in this experiment was .025, while the error associated with the equivalent opacity decay condition
described in Experiment 2 (see \chap{chap:adjusting_opacity}) was .086. This
finding, along with the significantly higher Cohen's *d* effect size reported in
this experiment compared to Experiment 2 for the non-linear decay conditions
(0.86 vs. 0.19), provides evidence that point size is a stronger encoding
channel for the manipulation of perceived correlation than point opacity. If
these effects are being driven by the lower salience of more exterior points,
the fact that a larger effect of point size has been reported is congruent with
research showing clear influences of stimulus size on object salience and
perceptual weighting \cite{grice_1983, hong_2022, healey_2012}. The present
results therefore provide support for point salience/perceptual weighting being
a key driver of the effects observed; lower point salience brought on by reduced
point size in the exterior of the scatterplot reduces the perceived width of the
distribution of data points around the regression line, biasing estimates of
correlation upwards and leading to a higher degree of accuracy. Other candidate
mechanisms do exist; similar results would be expected if a feature-based
attentional bias was responsible \cite{sun_2016, hong_2022}. The current
methodology does not allow for distinguishing between these explanations, and it
may be that both are partially responsible.

The lack of support for the second hypothesis is surprising, and suggests that
point salience and perceived distributional width do not form the whole story.
There is evidence that larger stimuli exhibit greater levels of spatial
uncertainty \cite{alais_2004}, and it is possible that this uncertainty results
in a perceptual under-weighting of the contribution of these points during
correlation estimation. This is consistent with previous work
\cite{warren_2002, warren_2004} suggesting that the brain may make robust
statistical use of visuo-spatial information. These mechanisms act to downweight
the influence of less reliable information (in this case the higher spatial
uncertainty of larger exterior points) on subsequent perceptual estimates. In
the present experiment, this resulted in participants making more accurate
estimates of correlation in the inverted decay condition compared to the
standard size condition. It was suggested in \chap{chap:adjusting_opacity} that
inverted opacity manipulations may be employed to correct for the
*overestimation* of correlation observed with negatively correlated scatterplots
\cite{sher_2017}; findings here indicate that using an inverted size decay
function in this way may not be appropriate.

## Constant Correlation Estimation Precision

Unlike the experiments described in \chap{chap:adjusting_opacity}, in which
standard deviations of correlation estimation errors generally became smaller as
the actual *r* value increased, distributions of standard deviations here
remained mostly constant. This was unexpected, as previous work
\cite{rensink_2010, rensink_2012, rensink_2014, rensink_2017} routinely finds
precision in *r* estimation to increase with the objective *r* value. This
finding may be related to the nature of the stimuli in the current study. At
high values of *r* there is a large amount of overlap between points in the
non-linear, non-linear inverted, and linear size decay conditions. This overlap
may blur the percept and account for the absence of this effect, however cannot
account for these findings with regards to the standard size condition. Aside
from the inverted non-linear decay condition in \chap{chap:adjusting_opacity},
the finding that precision increased with *r* was robust. Its absence here is
curious given that the standard size decay condition here is identical to the
full opacity condition in that chapter. Relying on relative judgements means that the
interplay between scatterplots with different visual features must be accounted
for within a particular experiment. The stimuli as *r* approaches 1 in the
current study exhibit greater levels of visual variance than the stimuli in
Experiments 1 and 2 (see \chap{chap:adjusting_opacity}), which may explain the
lack of increased precision here. Further testing is required for a more
concrete explanation.

Ultimately, my aim is to provide tools for the design of visualisations more
suited for the tasks they are intended to support. When that task is the
perception of positive correlation, the use of the non-linear size decay
condition described here is recommended. For other scatterplot tasks, such as
cluster separation or numerosity perception, or other chart types, the use of
the size manipulation may in fact be a hindrance.

## Training {#training-e3}

Before beginning the experimental trials, participants viewed plots depicting
*r* = 0.2, 0.5, 0.8, and 0.95 for a minimum of 8 seconds. Comparing a model
including session half as a fixed effect with the original experimental model
revealed no significant effect ($\chi^2$(`r in_paren(e3_training_model.df)`) =
`r printnum(e3_training_model.Chisq)`, *p*
`r printnum(e3_training_model.p, add_equals = TRUE)`), suggesting that having
more recently viewed the example plots did not have an effect on participants'
performance.

## Limitations {#limitations-e3}

Despite confirming a method of obtaining dot pitch, no method of obtaining
head-to-monitor distances is available. This, along with the comparative
correlation judgements collected, prevents concrete psychophysical conclusions
from being made. Instead, the experimental paradigm allows for findings that are
rigorous to different viewing contexts and are of particular importance for the
HCI and visualisation design audiences. It may be that a high level perceptual
phenomenon is responsible for the effects seen here; investigating this is
beyond the scope of the current study and does not negate the findings. There is
also the potential for misinterpretation of the scatterplots presented in the
current study, especially given their similarity in form, but not purpose, to
bubble charts.

## Future Work {#future-work-e3}

Evidence has been found for robust effects of varying point opacity and point
size on correlation estimation in positively correlated scatterplots. There are
different effects of changing these visual features, and these effects are also
partially dependent on the objective *r* value in the scatterplot. Future work
should investigate the combination of these visual manipulations as they pertain
to correlation estimation.

Additionally, future qualitative work is required to investigate whether the
modified scatterplots presented result in misinterpretation or a decrease in the
levels of trust people place in data visualisations.

# Conclusion {#conclusion-chap5}

I conducted a single experiment, in which points on scatterplots had their sizes
systematically changed according to their distance from the regression line. The
equation relating point size to residual magnitude was identical to that used in
\chap{chap:adjusting_opacity}, with the addition of a scaling factor and
constant to provide parity between the smallest points in the previous
experiment and those in the current. Additionally, I tested the visibility of
the smallest points used to ensure that no data were functionally removed, as
well as collecting the sizes and resolutions of participants' monitors. I found
evidence that reducing the size of scatterplot points as a function of residual
magnitude using a non-linear equation not only produced significantly more
accurate estimates of positive correlation, but also subtly changed the
fundamental shape of the estimation curve in a way not previously reported.
Unlike the inverted opacity decay function used in
\chap{chap:adjusting_opacity}, the inverted size decay function here did not
produce the least accurate estimates of correlation; these were found when
participants viewed the "standard size" plot devoid of a size manipulation.
Taken together, these results suggest a greater potential for manipulating
correlation estimates when using point size compared to point opacity.
