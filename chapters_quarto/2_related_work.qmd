---
title: "related_work"
output:
  format: 
    latex:
params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    out-width: NA
    out-height: NA
    cache.path: "../thesis_cache/" 
    
execute: 
  echo: false
  warning: false
  message: false
  include: false     
---

```{r}
#| label: setup

library(tidyverse)
library(ggdist)
library(ggpubr)
library(png)
library(grid)
library(cowplot)
library(MASS)
library(GGally)
library(hexbin)

source("../shared_functions.R")

slopes <- prepare_slopes(0.6)
```

# Data Visualisation: A Brief History {#brief-history}

Data visualisation, which can be thought of as the practice of representing information 
in a visual modality \cite{hinterberger_2009}, is difficult to concretely define,
classify, and categorise. With the primacy of vision with regards to our interactions with
and interpretations of the world around us, data visualisation may be thought of
as an extension of art and the written word. Both art and writing are ancient 
phenomena, with evidence for the former being found in the prehistoric
period some 66,000 years ago \cite{standish_2025}, and evidence for the latter
emerging as Mesopotamian cuneiform around 3200 B.C.E \cite{schmandt_2014}.
Broadly, the literature agrees that art emerged prior to the written word; 
this speaks volumes of the human instinct to represent our thoughts, feelings,
emotions, and the ways that we interact with the world pictorially.
This instinct has not waned, and modern computing makes it easier than ever for
those of us with no technical or artistic skills to create graphics and visualisations
that *tell stories* about our data in ways which are both beautiful and practical.

<!-- When, then, should we consider to be the emergence of data visualisation as a  -->
<!-- human practice? Of course, answering this question requires the provision of a  -->
<!-- definition for the practice itself first. -->

When, then, should we consider to be the emergence of data visualisation as a 
human practice? Schmandt-Besserat \cite{schmandt_1978, schmandt_2014}
considers clay counting tokens to be the direct precursor of the written word;
while the evidence for this link is controversial \cite{robson_2007}, the existence of such
tokens is not. With each shape of token representing a certain amount of a 
certain good (measures of grain, jars of oil, etc.), this system could be considered
a very early, very simple form of data visualisation (or physicalisation
\cite{jansen_2015}). Similarly, there is limited evidence of prehistoric cartographic
drawings \cite{muhly_1978}, which may also be considered
a form of, or related to, data visualisation. While I am not asserting
that data visualisation is older than writing, or that ancient map drawings are
equivalent to modern graphics, the existence of these representations emphasises the 
attractive convenience that symbols and signs represent for humans; making sense
of our world and the relationships therein is often easier through pictures as
opposed to words and numbers, a principle which I consider key for this thesis.

Note: much of the rest of this section is heavily inspired by Michael Friendly's
*A Brief History of Data Visualization* \cite{friendly_2008}.

Moving on, then, to the kind of pictorial representation that modern students and scientists would 
firmly recognise as a "data visualisation". Tufte and Graves-Morris, in 1983's
seminal *The Visual Display of Quantitative Information* \cite{tufte_1983}, describe
an unattributed time series illustration from the 10th or 11th century, itself
described by Funkhouser in 1936 \cite{funkhouser_1936} as being discovered by
Sigmund Günther in 1877. This illustration is included here in @fig-early-time-series.

```{r}
#| label: fig-early-time-series
#| include: true
#| fig-cap: Reproduced in Tufte and Graves-Morris, 1983 \cite{tufte_1983} from Funkhouser, 1936 \cite{funkhouser_1936}.
#| out-width: NA
#| out-height: NA

knitr::include_graphics(path = "../supplied_graphics/tufte_1983.png", dpi = NA)
```

This illustration purports to show the movements of planetary bodies as a function of time,
although Funkhouser considered it little more than a "schematic diagram...for
illustrative purposes" \cite{funkhouser_1936}. Regardless, the recognisable
grid lines and sinusoidal variation in the curves are ideas that would not appear
again for another 600-700 years, after which they would become mainstream 
visualisation techniques. In the mid-14th century, French philosopher Nicole
Oresme demonstrated an understanding of graphing by plotting proto-bar charts,
and by the 16th century, advances in cartography, photography, and mathematics
laid the ground for an explosion in data visualisation.

The 17th century saw the birth of geometry and coordinate systems, error measurement,
probability, and demographic statistics. With these scientific advancements
came the advancements in data visualisation needed to communicate these concepts.
For example, in 1626, Scheiner used what Tufte would later term the "principle of small multiples"
\cite{tufte_1983} to illustrate how configurations of sunspots change over time
(see @fig-sunspots).

```{r}
#| label: fig-sunspots
#| include: true
#| fig-cap: Scheiner's (1626) plot detailing how configurations of sunspots change over time. This technique would later be referred to as the "principle of small multiples" by Tufte.

knitr::include_graphics(path = "../supplied_graphics/sunspots.png", dpi = NA)
```

```{r}
#| label: fig-cholera
#| include: true
#| fig-cap: John Snow's (1854) map of cholera cases in Soho, London. Using this data visualisation, Snow was able to demonstrate a link between cholera cases and a contaminated water supply.

knitr::include_graphics(path = "../supplied_graphics/snow_cholera.jpg", dpi = NA)
```

```{r}
#| label: fig-napoleon
#| include: true
#| fig-cap: Charles Joseph Minard's (1869) flow diagram of Napoleon's botched invasion of Russia in 1812-1813. This diagram shows Napoleon's advance and retreat on Moscow. The width of the orange and black columns encodes the size of the Grande Armée. The temperature scale on the lower portion of the graph illustrates the weather conditions during the retreat, with a freezing Russian winter causing high rates of attrition.

knitr::include_graphics(path = "../supplied_graphics/minard_napoleon.jpg", dpi = NA)
```

```{r}
#| label: fig-nightingale
#| include: true
#| fig-cap: Florence Nightingale's (1858) polar area chart illustrates the causes for mortality among British soldiers during the Crimean War. Data visualisations of this type were used to illustrate that in reality, more British soldiers died from preventable disease than were killed by the enemy, and were used as part of a campaign to improve sanitation among soldiers.

knitr::include_graphics(path = "../supplied_graphics/nightingale-rose.jpg", dpi = NA)
```

The latter half of the 19th century, the so-called "Golden Age of Statistical Graphics"
\cite{friendly_2008} saw the rise of forms of data visualisation that begin 
to look remarkably similar to the graphs and informatics seen in mass media and
scientific publication today. The most notable examples of these are John Snow's cholera map, which
was able to link the incidence of cholera to a contaminated water pump in London
(@fig-cholera), Charles Joseph Minard's flow chart of the Napoleonic invasion of Russia
(@fig-napoleon), and Florence Nightingale's rose diagrams (polar area charts in
the modern parlance, see @fig-nightingale). In each of these graphs, visualisation
is used with different intent. In John Snow's cholera map, visualisation was used
to track cases of a deadly disease, and facilitated a novel linkage between cholera
and contaminated drinking water. In Charles Joseph Minard's flow chart of Napoleon's
failed 1812 invasion of Russia, a total of six variables are displayed to tell the
data story, allowing the viewer to appreciate the movements of the Grande Armée,
it's diminishing size owing to attrition, and the freezing temperatures that
largely caused that attrition. Florence Nightingale's polar area chart depicts
the causes of mortality amongst British troops in the Crimean War; charts such as
this were used to successfully campaign for better sanitation in hospitals and
the front lines.

In all of these visualisations, data is used to accentuate storytelling. In some
cases, this may lead to critical discoveries that save lives, and in others, it
may simply facilitate a greater understanding and appreciation of the data. In either
case, visualisation is used effectively to appeal to our affinity for visual
storytelling.

An appetite for precision defined the approach to statistical thinking, and by
extension, data visualisation, in the first half of the 20th century. Statistical
graphics finally became mainstream, and were implemented in curricula and used
in government, commerce, and finance. This period also marks the beginning of 
graphical methods being used to generate new scientific insights, a trend which
would only accelerate throughout the next century. 

<!-- The most prominent example of this -->
<!-- is Henry Moseley's (1913) discovery of the concept of atomic number \cite{moseley_1913}, -->
<!-- which I will discuss in greater detail in Section \ref{history-corr-viz}. -->

Significant developments in the latter half of the 20th century laid the final
brick in the foundations of what would become the modern data visualisation 
landscape. John W. Tukey's *The Future of Data Analysis* \cite{tukey_1962} proposed
a separation between data analysis and mathematical statistics. This seminal work
would become hugely influential, and Tukey would go on to invent a great number of
analysis-driven data visualisations, including stem-leaf plots and box plots, both
of which are now commonplace in software packages and statistics education. In 1967,
Jacques Bertin \cite{bertin_1967} published *Sémiologie graphique* 
(*Semiology of Graphics*), organising the perceptual elements of data visualisations
according to their features and their relationships to the underlying data; this work
would be influential for Leland Wilkinson's *Grammar of Graphics* \cite{wilkinson_1999},
which in turn influenced the `ggplot2` package \cite{wickham_2016} that is used
extensively in this thesis.

Since then, both the practice and study of data visualisation have become thoroughly
mainstream. Students are taught to visualise data early on, and the propagation of
both software and powerful computing hardware have brought advanced techniques,
such as high dimensional visualisation and massive datasets, into the home. To summarise
the timeline of developments in data visualisation over the last 500 years, I include
a rug and density plot from Friendly (2008) \cite{friendly_2008} in @fig-rug-density.

```{r}
#| label: fig-rug-density
#| include: true
#| fig-cap: Major milestones in the development of data visualisation illustrated using a rug plot and density estimate. This figure is taken from Friendly (2008) \cite{friendly_2008}.

knitr::include_graphics(path = "../supplied_graphics/rug_density.png", dpi = NA)
```

Recounting a full and detailed history of the practice and study of data visualisation
would require much more than a single thesis, and has been done to a very high standard
elsewhere \cite{friendly_2008, friendly_2021}. Given that this thesis is focused on
the perception of correlation in scatterplots, the remainder of the chapter is primarily
limited to discussions of relatedness, correlation, and scatterplot visualisations.

# Relatedness & Correlation {#relatedness-and-correlation}

Very early piloting (detailed in Section \ref{general-methods-adjusting-opacity},
\chap{chap:adjusting_opacity}) revealed that while people understood the concept
of correlation, they were unsure as to what different degrees of correlation looked
like. To address this, training was included in experiments 1 to 4. For the same reason,
correlation was operationalised as *Strength of Relatedness* in experiment 5.
To make my research accessible, I start from here; two (or more) things are related
if a change in one is associated with a change in the other(s). In reality, however,
the data visualisation under investigation, scatterplots, do not visualise
relatedness, but correlation. Correlation refers to a specific statistical
relationship, which I explore from first principles below.

Francis Galton was the first to formally introduce the concept of *co-relation*
in 1888 \cite{galton_1888}. He derived a definition of correlation as a by-product
of his invention of statistical regression, first describing this new property
by way of anthropometric data relating to the measurement of different parts of the 
body. Early in this brief, but significant, paper, Galton posits a basic definition
of correlation:

\begin{quotation}
  ``Two variable organs [of the body] are said to be co-related when the variation of the one is accompanied on the average by more or less variation of the other, and in the same direction.''
\end{quotation}

Measuring a variety of anatomical distances, including head lengths and heights of a range of "not
wholly fully-grown" males, Galton first provides medians and semi-interquartile ranges
as a measure of error. Plotting the semi-interquartile ranges (Q-units) of two
variables against each other and calculating the slope of the line allowed
Galton to devise a new, unitless measure of co-relatedness, which he termed
r. This has since been lauded as a prime example of a mathematical
discovery made based on observed data. This method is imprecise, as it requires
intuition of a line-of-best-fit based on a hand drawn plot, but is credited as
the first full conceptual definition of a measure of correlation; it should be noted
that Galton did not conceive of negative correlation at this point. Galton's work
did not take place in isolation, and the preceding 60 years featured 
famous names, such as Gauss and Darwin, dancing around the idea of correlation
without recognising its importance; Lee Rodgers & Nicewander (1988)
\cite{lee_1988} provide an excellent overview of this period. 

Building on Galton's groundbreaking work, his younger, more mathematically-minded
student, Karl Pearson, developed the Pearson Product-Moment correlation in 1895
\cite{lee_1988} based on initial formulae by Bravais (1844) \cite{bravais_1844}.
This measure has been remarkably persistent, remaining unchanged
for well over a century. In fact, many other measures of correlation, such
as Spearman's $\rho$, the point-biserial correlation, and the $\phi$ coefficient are
actually special cases of Pearson's *r* applied to different types of data
\cite{henrysson_1971}; such is the dominance of the measure. Equation 2.1 defines
Pearson's *r*:

\begin{equation}
  r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}}
\end{equation}

In this equation, $\bar{x}$ and $\bar{y}$ are the means of each variable. $x_i$
and $x_i$ are the individual values of each variable in question. In the numerator,
the sum of the products of the difference between each value of $x$ and $y$ and their
means are found; this value represents the degree of deviation of all points from the regression line.
Then, in the denominator, this value is divided by the magnitude of the sum of the product of said
deviations; squaring and finding the square root of these values provides the
unsigned magnitude. In statistical language, Pearson's *r* finds the covariance of
two variables, then divides this values by the product of both variables' standard
deviations. Completing this calculation provides a measure of the overall
distance between the observed values and a fitted least squares regression line, and
provides a single value of *r* that describes how strongly related two variables 
are.

# Visualising Correlation {#visualising-correlation}

Of course, while mathematically sound, a single value provides no information about
the distribution of variables from which it was derived. To do this, the data must 
be examined visually. Here arises a parallel; in much the same way as Francis Galton
used a proto-scatterplot to formulate his definition of correlation, so must
data visualisation be used to tell the story behind a value of Pearson's *r*.
The need for visualisation is most viscerally illustrated by Anscombe's quartet
\cite{anscombe_1973}, which is recreated in @fig-anscombe using a dataset from the `datasets` 
core package \cite{r_core} in R.

```{r}
#| label: fig-anscombe
#| include: true
#| fig-cap: Anscombe's quartet \cite{anscombe_1973}. Each scatterplot depics datasets with identical means ($x$ = 9, $y$ = 7.5), regression coefficients ($y$ on $x$ = 0.5), standard errors (0.118), and Pearson's *r* values (≈ 0.816).
#| fig-asp: 0.25

anscombe_plot <- function (x_var, y_var) {
  datasets::anscombe %>%
    as_tibble() %>%
    ggplot(aes(!!sym(x_var), !!sym(y_var))) +
    geom_point(size = 1) +
    geom_smooth(method = "lm", se = F, fullrange = T, size = 0.65) +
    theme_ggdist() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(hjust = 0.5)) +
    xlim(3,20) +
    ylim(3,13)
}

ggarrange(anscombe_plot("x1", "y1"),
          anscombe_plot("x2", "y2"),
          anscombe_plot("x3", "y3"),
          anscombe_plot("x4", "y4"),
          ncol = 4)
```

Anscombe's quartet \cite{anscombe_1973} describes four simple datasets that are
identical with regards to a range of statistical measures. They feature the same
number of observations, the same means, regression coefficients, regression line
equations, sums of squares, estimated standard errors, and correlation coefficients.
A simple examination of these statistics would lead to the conclusion that
the datasets are almost identical; in reality, there are significant differences
between them that can only be seen via visualisation.

In this thesis, the primary concern is correlation and the ways in which people
interpret it from scatterplots. The remainder of this section examines the history
and current landscape of correlation visualisations; starting with Galton's 
scatterplot precursors, I then go on to discuss the development of the familiar
modern scatterplot through the examination of a number of impressive use cases.
Following that, I review the current landscape of correlation visualisation, including
the more recent use of other, non-scatterplot graphs.

## History {#history-corr-viz}

As mentioned in Section \ref{relatedness-and-correlation}, Francis Galton based 
his initial formulation of the correlation coefficient on hand-drawn plots
of the semi-interquartile ranges of two variables. @fig-galton-hand-plot contains
a negative scan of Galton's original plot, along with a modern recreation using 
``ggplot2``.

```{r}
#| label: fig-galton-hand-plot
#| include: true
#| fig-cap: Francis Galton's original plot comparing the semi-interquartile ranges of stature (height) and cubit (forearm length). The plot has been recreated using `ggplot2` on the right.
#| fig-asp: 0.5

set.seed(3932)

n <- 10

data <- data.frame(
  stature = runif(n, -1, 1),
  type = rep(c("x", "o"), times = "5")
) %>%
  mutate(cubit = stature)

p1 <-  ggplot(data, aes(stature, cubit)) +
          geom_jitter(data = subset(data, type == "x"), shape = 4, width = 0.4) +
          geom_jitter(data = subset(data, type == "o"), shape = 1, width = 0.4) +
          geom_abline(intercept = -0.5, slope = 1, linetype = "dashed") +
          geom_abline(intercept = 0.5, slope = 1, linetype = "dashed") +
          # horizontal grid lines
          geom_hline(yintercept = 1, colour = "grey") +
          geom_hline(yintercept = 0, colour = "grey") +
          geom_hline(yintercept = -1, colour = "grey") +
          # vertical grid lines
          geom_vline(xintercept = 1, colour = "grey") +
          geom_vline(xintercept = 0, colour = "grey") +
          geom_vline(xintercept = -1, colour = "grey") +
          # carrying on
          xlim(-1.5,1.5) +
          ylim(-1.5,1.5) +
          geom_smooth(method = "lm", se = F, fullrange = T) +
          scale_y_continuous(
            name = "",
            sec.axis = dup_axis(name = "")) +
          theme_minimal() +
          theme(
            panel.grid = element_blank(),                    # Remove default grid
            panel.border = element_rect(color = "black", fill = NA),  # Add solid box
            axis.ticks = element_line(color = "black"),
            axis.text = element_text(color = "black"),
            axis.title = element_blank(),
            plot.title = element_text(hjust = 0.5, size = 7)
          ) +
          labs(title = "STATURE AND CUBIT") +
          annotate("text", x = -0.25, y = 0.45, angle = 45, label = "Line of Q1 values", size = 4) +
          annotate("text", x = 0.35, y = -0.35, angle = 45, label = "Line of Q3 values", size = 4)


img <- readPNG("../supplied_graphics/galton_scan.png")

g <- rasterGrob(img, interpolate = TRUE)

p2 <- ggdraw() + draw_grob(g)

plot_grid(p2, p1, ncol = 2)
```

Despite the importance of graphics like these for Galton's discovery of
regression, statistical correlation, and the relationship between these and the
bivariate density function \cite{friendly_2005}, a more true example of a scatterplot
can be found in much earlier work on the orbits of twin stars by John F. W. 
Herschel \cite{herschel_1833}. Unfortunately, this scatterplot was never printed
in Herschel's 1833 manuscript, however can be inferred thanks to a detailed 
description of both the figure and the logic behind it. In short, Herschel wished
to figure out the orbits of binary star systems by using (often imprecise) 
astronomical measurements of certain angles and distances made over a long period
of time. It is the imprecision in measurement which necessitated data visualisation,
as precise measurements would would allow common astronomical principles to 
provide precise solutions. First specifying the axes, angles of position ($y$)
and date of observation ($x$), and grid lines, Herschel then describes plotting points and drawing,
by hand, a line-of-best-fit. A particularly enlightening quote, with original
emphasis, is reproduced below:

\begin{quotation}
    ``Our next step, then, must be to draw, by the mere judgement of the eye, and with
a free but careful hand, not \textit{through}, but \textit{among} them, a curve presenting as few
and slight departures from them as possible, consistently with this character of 
large and graceful sinuosity, which must be preserved at all hazards...''
\end{quotation}

From this smoothed-by-eye line, Herschel was able to calculate the parameters 
that determined the rotation of the $\gamma$Virginis system. Herschel beat out
Galton by more than 50 years to claim the first scatterplot, in a remarkable
feat of using graphing to solve an astronomical problem. Just a few decades after
Galton had discovered the concept of correlation, yet another astronomical example
of a scatterplot can be found in the Hertzsprung-Russell diagram, created independently
by both Ejnar Hertzprung and Henry Norris Russell in 1911-1913 \cite{montmerle_2011}.
This type of scatterplot, which still sees use in modern astronomy, plots stellar
luminosity against colour (temperature). I have plotted an HR diagram in 
@fig-HR-plot using the HYG database [^1].

[^1]: https://www.astronexus.com/projects/hyg

```{r}
#| label: fig-HR-plot
#| include: true
#| fig-cap: Hertzsprung-Russell diagram of colour index against stellar luminosity. The code for this plot was taken from John Russell's (no relation) blog \cite{russell_2025}.

HRG_data <- read_csv("../data/other_data/hygdata_v41.csv") %>%
  filter(dist < 10000)

options(scipen = 999)

HRG_data %>%
  ggplot(aes(ci, lum, colour = ci)) +
  geom_point(size = 0.02, show.legend = F) +
  scale_y_log10() +
  scale_x_continuous(limits=c(-.5,2.25)) +
  scale_color_gradient2(low= "blue", mid="white", high="red",midpoint=0.75) +
  theme_minimal() +
  theme(panel.background=element_rect(fill="black"),
        panel.grid=element_blank(),
        plot.title = element_text(size = 8),
        axis.title = element_text(size = 8),
        axis.text = element_text(size = 6.5)) +
  labs(title="Hertzsprung-Russell Diagram",
       y="Luminosity (in comparison to Sun)",
       x="Color Index (blue magnitude - visual magnitude)")
```

The clear band that can be seen in @fig-HR-plot from top-left to bottom
right are "main sequence" stars. It was only by visualisation that astronomers
were able to determine that there were laws that govern the formation and evolution
of stars. Spence and Garrison \cite{spence_1993} conducted a detailed analysis
of the history and development of HR diagrams, and conclude that they represent
a "shining example of the power of graphic display". Again, it was the ability
of the data visualisation to facilitate pattern recognition in its human viewers
that was so crucial to its (continuing) success.

While this section has not been an exhaustive list of every scatterplot and 
scatterplot-alike that has prominently featured in scientific publishing since
Herschel's initial description and Galton's initial formulation of correlation,
I hope that I have conveyed the importance of this visualisation type. As we will
see in Section \ref{present-landscape-corr-viz}, the landscape of correlation
visualisation is now much broader, however the humble scatterplot still remains
a crucial part of the visualiser's toolbox. From its origins as a way of inferring
astronomical relationships, to its use in the discovery of correlation, the 
standard scatterplot remains largely unchanged to this day; this thesis charts
the development of a new type of scatterplot that draws on key elements of human
perception to increase its utility in correlation visualisation, however it is 
important to pay homage to the history of the visualisation as an ever-present
lab mate to those willing and able to use graphing to solve scientific problems.

## Present Landscape {#present-landscape-corr-viz}

When I began this project in the autumn of 2021, I believed that scatterplots were
the first and last word when it came to visualising correlation. I soon discovered
that this was not the case; as scientists often do, a whole host of other 
visualisation types had been designed or adapted in an effort to better visualise correlation.
Parallel coordinates plots (PCPs) \cite{heinrich_2013}, which are popular in the 
InfoVis community, are most often used for multidimensional data. Applying them to
bivariate data results in plots that perform (almost) as well as scatterplots
with regards to visualising correlation between two variables \cite{kay_2015}.
@fig-other-corr-viz illustrates parallel coordinates plots, along with 3 other
types of correlation visualisation, with an *r* value of 0.6.

```{r}
#| label: fig-other-corr-viz
#| include: true
#| fig-cap: "From left to right: Parallel Coordinates Plot (PCP) built with the `GGally` package \\cite{ggally}; Stacked area plot; radial stacked bar plot (doughnut plot); radar plot. Each plot uses the same dataset with an *r* value of 0.6."
#| fig-asp: 1

# define custom theme

theme_other_viz <- function() {
  theme_ggdist() %+replace%
    theme(axis.text = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          #plot.margin = margin(1, 1, 1, 1, unit = "pt"),
          legend.position = "none",
          plot.title = element_text(size = 7, hjust = 0.5)
      
    )
}

# PCP

PCP_plot <- slopes %>% 
  mutate(ID = seq_len(nrow(slopes))) %>%
  ggparcoord(columns = 1:2,
             groupColumn = NULL,
             showPoints = FALSE,
             scale = "uniminmax",
             alphaLines = 0.4) +
  theme_other_viz() +
  theme(aspect.ratio = 1) +
  coord_cartesian(expand = FALSE, clip = "on") +
  labs(title = "Parallel Coordinates Plot") +
  xlim("V1","V2") +
  ylim(0.1,
       0.9)


# stacked area

stacked_area_plot <- slopes %>%
  mutate(ID = seq_len(nrow(slopes))) %>%
  pivot_longer(cols = c("V1", "V2"),
               values_to = "value", names_to = "group") %>%
  mutate(value = value + 2.79) %>%
  ggplot(aes(x = ID, y = value, fill = group)) +
  geom_area() +
  theme_other_viz() +
  theme(aspect.ratio = 1) +
  labs(title = "Stacked Area Plot")
  

# donut

radial_stack <- slopes %>%
  mutate(ID = seq_len(nrow(slopes))) %>%
  pivot_longer(cols = c("V1", "V2"),
               values_to = "value", names_to = "group") %>%
    mutate(value = value + 2.79) %>%
  ggplot(aes(x = ID, y = value, fill = group)) +
    geom_col() +
    theme_other_viz() +
  coord_polar(theta = "y") +
  labs(title = "Radial Stacked Bar Plot")

# radar

radar_plot <- slopes %>%
  mutate(ID = seq_len(nrow(slopes))) %>%
  pivot_longer(cols = c("V1", "V2"),
               values_to = "value", names_to = "group") %>%
  mutate(value = value + 2.79) %>%
  ggplot(aes(x = ID, y = value, colour = group)) +
  geom_line(size = 0.1) +
  theme_other_viz() +
  coord_polar() +
  labs(title = "Radar Plot")

plot_grid(PCP_plot, stacked_area_plot, radial_stack, radar_plot)
```

Despite the existence of plots such as these, scatterplots remain by far the most
popular way of visualising correlation. With the exception of the charts featured
in @fig-other-corr-viz, many other ways of visualising correlation are density-flavoured
remixes of traditional scatterplots, such as Hexbin plots and Kernel Density Estimate
plots (see @fig-remixes).

```{r}
#| label: fig-remixes
#| include: true
#| fig-cap: Hexbin (left) and Kernel Density Estimation (KDE, right) plots. These are binned, density-flavoured versions of standard scatterplots.
#| fig-asp: 0.5

# create larger dataset for hexbin and KDE plots

set.seed(1234)

my_sample_size = 12800

my_desired_r = 0.6

mean_variable_1 = 0
sd_variable_1 = 1

mean_variable_2 = 0
sd_variable_2 = 1

mu <- c(mean_variable_1, mean_variable_2) 

myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)

mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 

corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))

corr_model <- lm(V2 ~ V1, data = corr_data)

my_residuals <- abs(residuals(corr_model))

large_data <- round(cbind(corr_data, my_residuals), 2)

hex_plot <- large_data %>% 
  ggplot(aes(V1, V2)) +
  geom_hex() +
  scale_fill_viridis_c(option = "plasma") +
  theme_other_viz()

KDE_plot <- large_data %>% 
  ggplot(aes(V1, V2)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") + 
  scale_fill_viridis_c(option = "plasma") + 
  theme_other_viz()

ggarrange(hex_plot, KDE_plot, 
          ncol = 2)
```

While there may be many, often more visually appealing ways of visualising
correlation, traditional scatterplots offer a number of distinct advantages.

## Scatterplots {#scatterplots-corr-viz}

In a large scale study, Harrison et al. \cite{harrison_2014} tested a range of correlation
visualisation types, including scatterplots and those depicted in @fig-other-corr-viz,
to see if precision in correlation estimation could be modelled by Weber's law 
\cite{rensink_2010}. A more detailed analysis of the psychophysical laws that
may determine interactions with correlation visualisations is presented in
Section \ref{corr-percept-related-work}. In short, the perception of differences
in correlation has a linear relationship to the objective differences in correlation.
In this study, itself a replication of a previous work investigating Weber's
law for the modelling of perceived correlation \cite{rensink_2010}, participants
were asked to make discriminative judgements between side-by-side correlation
visualisations. An adaptive staircase procedure was employed to infer
just-noticeable differences (JNDs) for correlation perception with nine correlation
visualisation types. Both positive and negative correlations were tested. Fitting
linear models to the JND data revealed that correlation discrimination in the nine
different visualisations tested (scatterplots, PCPs, stacked area charts, stacked 
line charts, stacked bar charts, donut charts, radar charts, line graphs, and 
ordered line graphs) could be modelled using Weber's law. This study also
provides a ranking of the tested visualisation types with regards to participant's
correlation discrimination performance; overall, traditional scatterplots
outperformed all other visualisation types with positively correlated data, and were
tied with PCPs for negatively correlated data.

Rensink (2014) measured correlation discriminability for scatterplots presented
at 100, 400, or 1600 milliseconds. This study found that performance was almost
identical for scatterplots presented for either 400 or 1600 milliseconds, and that
there was only a small deterioration in performance for those presented for 
100 milliseconds. The lack of performance improvement for longer presentation times
both facilitates the rapid collection of large amounts of data and speaks to
the intuitive, perceptual nature of correlation perception in scatterplots.

Finally, scatterplots, in addition to outperforming other visualisation types
and featuring rapid interpretation, are also ubiquitous. In 1983, scatterplots
were estimated to account for between 70% and 80% of data visualisations in 
scientific publications \cite{tufte_1983}. With the advances in computers and 
graphing techniques seen since then, this proportion is certainly lower today. 
Regardless, scatterplots, their derivatives, and their remixes can be seen not
only in scientific and technical publication, but also in the news and mass media.
This combination of advantages makes them particularly suitable for scientific study,
and was part of the reason I chose to pursue this project.

Despite this host of advantages, the estimation of correlation suffers from a 
bias; it is routinely underestimated by viewers. To understand this bias with
a view to correcting, I first explore the perception of correlation more generally,
before briefly discussing the more cognitive aspects of the measure, which are
especially relevant for the experiment described in \chap{chap:belief_change}.
I then deal with the problem itself; the underestimation of correlation in
positively correlated scatterplots, before finishing this chapter by discussing
data visualisation and statistical literacy and stating the research objectives
and contributions of this thesis.

# Correlation Perception {#corr-percept-related-work}

Understanding the perception of correlation represents a complex problem. Despite 
decades of research into correlation, scatterplots, and the perception of information
from visualisations, there is no definitive solution to this problem. Circumstantial
evidence exists, however, that points to correlation perception being driven,
from a top-down point of view, by the shape of the probability distribution
represented by the scatterplot.

Firstly, increasing the $x$ and $y$ scales of a scatterplot such that the size of
the point cloud increases is associated with an increase in viewers' judgements
of bivariate association, despite the objective *r* value remaining the same
\cite{cleveland_1982}. This finding suggests that it is the area of the point cloud
that viewers may use to judge correlation. This area is easier to visualise in 
hexbin or KDE plots (see @fig-remixes). With enough data, the point cloud of
a scatterplot will tend towards a straight line, decreasing the area of the point
cloud in the same way. @fig-point-cloud-trends illustrates this.

```{r}
#| label: fig-point-cloud-trends
#| include: true
#| fig-cap: Scatterplot with Pearson's \\textit{r} = 0.95 (left) and Pearson's \\textit{r} = 0.4 (right).

slopes_0.4 <- prepare_slopes(0.4)

slopes_0.95 <- prepare_slopes(0.95)

ggarrange(plot_example_function(slopes_0.95, "High Correlation: Pearson's *r* = 0.95", 1, 0.2, 7),
          plot_example_function(slopes_0.4, "Low Correlation: Pearson's *r* = 0.4", 1, 0.2, 7),
          ncol = 2)
```



# Correlation Cognition {#corr-cognition}

# Underestimation: What's Really Going On? {#underestimation-whats-going-on}

# Underestimation: Potential Consequences {#underestimation-consequences}

# Data Visualisation Literacy {#graph-literacy-related-work}

# Objectives and Contributions {#objectives-contributions}
