@article{anwyl_2020,
  title = {Gorilla in Our Midst: {{An}} Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  urldate = {2024-10-03},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing---an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test--retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  keywords = {Artificial Intelligence,Attentional control,Browser timing,Online methods,Online research,Remote testing,Timing accuracy},
  file = {C:\Users\mbch4gs2\Zotero\storage\CI9CIJY4\Anwyl-Irvine et al. - 2020 - Gorilla in our midst An online behavioral experim.pdf}
}

@article{arechar_2018,
  title = {Conducting Interactive Experiments Online},
  author = {Arechar, Antonio A. and G{\"a}chter, Simon and Molleman, Lucas},
  year = {2018},
  month = mar,
  journal = {Experimental Economics},
  volume = {21},
  number = {1},
  pages = {99--131},
  issn = {1573-6938},
  doi = {10.1007/s10683-017-9527-2},
  urldate = {2024-10-04},
  abstract = {Online labor markets provide new opportunities for behavioral research, but conducting economic experiments online raises important methodological challenges. This particularly holds for interactive designs. In this paper, we provide a methodological discussion of the similarities and differences between interactive experiments conducted in the laboratory and online. To this end, we conduct a repeated public goods experiment with and without punishment using samples from the laboratory and the online platform Amazon Mechanical Turk. We chose to replicate this experiment because it is long and logistically complex. It therefore provides a good case study for discussing the methodological and practical challenges of online interactive experimentation. We find that basic behavioral patterns of cooperation and punishment in the laboratory are replicable online. The most important challenge of online interactive experiments is participant dropout. We discuss measures for reducing dropout and show that, for our case study, dropouts are exogenous to the experiment. We conclude that data quality for interactive experiments via the Internet is adequate and reliable, making online interactive experimentation a potentially valuable complement to laboratory studies.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,Behavioral research,C71,C88,C90,D71,Experimental methodology,Internet experiments,Public goods game,Punishment},
  file = {C:\Users\mbch4gs2\Zotero\storage\JYT8JH6G\Arechar et al. - 2018 - Conducting interactive experiments online.pdf}
}

@article{ayris_2018,
  title = {{{LIBER Open Science Roadmap}}},
  author = {Ayris, Paul and Bernal, Isabel and Cavalli, Valentino and Dorch, Bertil and Frey, Jeannette and Hallik, Martin and {Hormia-Poutanen}, Kistiina and {Labastida i Juan}, Ignasi and MacColl, John and Ponsati Obiols, Agn{\`e}s and Sacchi, Simone and Scholze, Frank and Schmidt, Birgit and Smit, Anja and Sofronijevic, Adam and Stojanovski, Jadranka and Svoboda, Martin and Tsakonas, Giannis and {van Otegem}, Matthijs and Verheusen, Astrid and Vilks, Andris and Widmark, Wilhelm and Horstmann, Wolfram},
  year = {2018},
  month = jul,
  publisher = {Ligue des biblioth{\`e}ques europ{\'e}ennes de recherche},
  doi = {10.20350/digitalCSIC/15061},
  urldate = {2023-09-13},
  abstract = {Embracing Open Science is critical if we are to make science more collaborative, reproducible, transparent and impactful. Open Science undoubtedly has the power to positively influence society, but its implementation is not yet universal.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2018-07-09T11:07:00Z}
}

@article{barr_2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  journal = {Journal of memory and language},
  volume = {68},
  number = {3},
  pages = {10.1016/j.jml.2012.11.001},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.11.001},
  urldate = {2022-08-23},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  pmcid = {PMC3881361},
  pmid = {24403724},
  file = {C:\Users\mbch4gs2\Zotero\storage\S7MDHRLZ\Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf}
}

@article{bridgesTimingMegastudyComparing2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  urldate = {2024-10-03},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime{\textregistered}, NBS Presentation{\textregistered}, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\DTLJS2L6\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\IKCSU4QQ\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf}
}

@article{brown_2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2024-10-08},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\L39C4ZEA\Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling in R.pdf}
}

@manual{buildmer,
  type = {Manual},
  title = {Buildmer: {{Stepwise}} Elimination and Term Reordering for Mixed-Effects Regression},
  author = {Voeten, Cesko C.},
  year = {2023}
}

@misc{chara_2021,
  title = {We Recently Went Viral on {{TikTok}} - Here's What We Learned},
  author = {Charalambides, Nick},
  year = {2021},
  month = aug,
  journal = {Prolific},
  urldate = {2024-10-04},
  howpublished = {https://www.prolific.com/resources/we-recently-went-viral-on-tiktok-heres-what-we-learned},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\UEAFCRQ5\we-recently-went-viral-on-tiktok-heres-what-we-learned.html}
}

@article{douglas_2023,
  title = {Data Quality in Online Human-Subjects Research: {{Comparisons}} between {{MTurk}}, {{Prolific}}, {{CloudResearch}}, {{Qualtrics}}, and {{SONA}}},
  shorttitle = {Data Quality in Online Human-Subjects Research},
  author = {Douglas, Benjamin D. and Ewell, Patrick J. and Brauer, Markus},
  year = {2023},
  month = mar,
  journal = {PLOS ONE},
  volume = {18},
  number = {3},
  pages = {e0279720},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0279720},
  urldate = {2024-10-04},
  abstract = {With the proliferation of online data collection in human-subjects research, concerns have been raised over the presence of inattentive survey participants and non-human respondents (bots). We compared the quality of the data collected through five commonly used platforms. Data quality was indicated by the percentage of participants who meaningfully respond to the researcher's question (high quality) versus those who only contribute noise (low quality). We found that compared to MTurk, Qualtrics, or an undergraduate student sample (i.e., SONA), participants on Prolific and CloudResearch were more likely to pass various attention checks, provide meaningful answers, follow instructions, remember previously presented information, have a unique IP address and geolocation, and work slowly enough to be able to read all the items. We divided the samples into high- and low-quality respondents and computed the cost we paid per high-quality respondent. Prolific (\$1.90) and CloudResearch (\$2.00) were cheaper than MTurk (\$4.36) and Qualtrics (\$8.17). SONA cost \$0.00, yet took the longest to collect the data.},
  langid = {english},
  keywords = {Attention,Ethnicities,Payment,Personality,Personality traits,Surveys,Undergraduates,United States},
  file = {C:\Users\mbch4gs2\Zotero\storage\E8Q8FA4D\Douglas et al. - 2023 - Data quality in online human-subjects research Comparisons between MTurk, Prolific, CloudResearch,.pdf}
}

@article{effectsize,
  title = {{{e}}ffectsize: {{Estimation}} of Effect Size Indices and Standardized Parameters},
  author = {{Ben-Shachar}, Mattan S. and L{\"u}decke, Daniel and Makowski, Dominique},
  year = {2020},
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {56},
  pages = {2815},
  publisher = {The Open Journal},
  doi = {10.21105/joss.02815}
}

@manual{ematools,
  type = {Manual},
  title = {{{EMAtools}}: {{Data}} Management Tools for Real-Time Monitoring/Ecological Momentary Assessment Data},
  author = {Kleiman, Evan},
  year = {2021}
}

@manual{emmeans,
  type = {Manual},
  title = {Emmeans: {{Estimated}} Marginal Means, Aka Least-Squares Means},
  author = {Lenth, Russell V.},
  year = {2024}
}

@misc{eprime_2020,
  title = {E-{{Prime}}},
  year = {2020},
  howpublished = {Psychology Software Tools}
}

@book{ggplot,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {Springer-Verlag New York},
  isbn = {978-3-319-24277-4}
}

@article{hirao_2021,
  title = {Reliability of {{Online Surveys}} in {{Investigating Perceptions}} and {{Impressions}} of {{Faces}}},
  author = {Hirao, Naoyasu and Koizumi, Koyo and Ikeda, Hanako and Ohira, Hideki},
  year = {2021},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {12},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.733405},
  urldate = {2024-10-04},
  abstract = {{$<$}p{$>$}Online experimental methods are used in psychological studies investigating the perceptions and impressions of facial photographs, even without substantial evidence supporting their reliability and validity. Although, the quality of visual stimuli is more difficult to control remotely, the methods might allow us to obtain a large amount of data. Then the statistical analysis of a larger volume of data may reduce errors and suggest significant difference in the stimuli. Therefore, we analyzed the reliability and validity of online surveys in investigating the perceptions (shine, red, and dark) and impressions (attractiveness, trustworthy, and so on) of facial photographs created from averaged faces with skin tones modified using computer graphics (CG). In this study, we conducted online (Online1) and laboratory experiments with well-controlled conditions (Control). For each experiment, 50 participants (men and women in Japan, age: 20--59years) completed the same questionnaire regarding their impressions of the same 28 CG facial photographs. The results showed significant correlations between the two experiments for all 19 items in the questionnaire. SD in the Online1 compared to the Control from the stimuli and individual differences were 56--84 and 88--104\% in each questionnaire items, respectively. Moreover, the rates of mismatching perceptual evaluations to the corresponding physical features demonstrated in the photographs were 4.9--9.7\% on average in an additional online survey of another 2,000 participants (Online2). These results suggest that online surveys can be applied to experiments to investigate impressions from CG facial photographs instead of general laboratory experiment by obtaining an appropriate number of participants to offset larger statistical errors that may result from the increased noise in the data from conducting the experiment online.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Face,Impression,Online,Reliability - reproducibility of results,Survey},
  file = {C:\Users\mbch4gs2\Zotero\storage\P7HRDX7X\Hirao et al. - 2021 - Reliability of Online Surveys in Investigating Per.pdf}
}

@manual{irr,
  type = {Manual},
  title = {Irr: {{Various}} Coefficients of Interrater Reliability and Agreement},
  author = {Gamer, Matthias and Lemon, Jim and {\textexclamdown}puspendra.pusp22@gmail.com{\textquestiondown}, Ian Fellows Puspendra Singh},
  year = {2019}
}

@article{liddell_2018,
  title = {Analyzing Ordinal Data with Metric Models: {{What}} Could Possibly Go Wrong?},
  shorttitle = {Analyzing Ordinal Data with Metric Models},
  author = {Liddell, Torrin M. and Kruschke, John K.},
  year = {2018},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  volume = {79},
  pages = {328--348},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2018.08.009},
  urldate = {2024-09-03},
  abstract = {We surveyed all articles in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology: General (JEP:G) that mentioned the term ``Likert,'' and found that 100\% of the articles that analyzed ordinal data did so using a metric model. We present novel evidence that analyzing ordinal data as if they were metric can systematically lead to errors. We demonstrate false alarms (i.e., detecting an effect where none exists, Type I errors) and failures to detect effects (i.e., loss of power, Type II errors). We demonstrate systematic inversions of effects, for which treating ordinal data as metric indicates the opposite ordering of means than the true ordering of means. We show the same problems --- false alarms, misses, and inversions --- for interactions in factorial designs and for trend analyses in regression. We demonstrate that averaging across multiple ordinal measurements does not solve or even ameliorate these problems. A central contribution is a graphical explanation of how and when the misrepresentations occur. Moreover, we point out that there is no sure-fire way to detect these problems by treating the ordinal values as metric, and instead we advocate use of ordered-probit models (or similar) because they will better describe the data. Finally, although frequentist approaches to some ordered-probit models are available, we use Bayesian methods because of their flexibility in specifying models and their richness and accuracy in providing parameter estimates. An R script is provided for running an analysis that compares ordered-probit and metric models.},
  keywords = {Bayesian analysis,Likert,Ordered-probit,Ordinal data},
  file = {C:\Users\mbch4gs2\Zotero\storage\ILW7PTGH\Liddell and Kruschke - 2018 - Analyzing ordinal data with metric models What co.pdf}
}

@article{mathot_2012,
  title = {{{OpenSesame}}: {{An}} Open-Source, Graphical Experiment Builder for the Social Sciences},
  shorttitle = {{{OpenSesame}}},
  author = {Math{\^o}t, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
  year = {2012},
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {314--324},
  issn = {1554-351X},
  doi = {10.3758/s13428-011-0168-7},
  urldate = {2024-10-03},
  abstract = {In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
  pmcid = {PMC3356517},
  pmid = {22083660},
  file = {C:\Users\mbch4gs2\Zotero\storage\RLJFCGWK\Math√¥t et al. - 2012 - OpenSesame An open-source, graphical experiment b.pdf}
}

@manual{ordinal,
  type = {Manual},
  title = {Ordinal---Regression Models for Ordinal Data},
  author = {Christensen, Rune H. B.},
  year = {2023}
}

@article{peer_2021,
  title = {Data Quality of Platforms and Panels for Online Behavioral Research},
  author = {Peer, Eyal and Rothschild, David and Gordon, Andrew and Evernden, Zak and Damer, Ekaterina},
  year = {2021},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {4},
  pages = {1643--1662},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01694-3},
  urldate = {2022-07-05},
  abstract = {We examine key aspects of data quality for online behavioral research between selected platforms (Amazon Mechanical Turk, CloudResearch, and Prolific) and panels (Qualtrics and Dynata). To identify the key aspects of data quality, we first engaged with the behavioral research community to discover which aspects are most critical to researchers and found that these include attention, comprehension, honesty, and reliability. We then explored differences in these data quality aspects in two studies (N\,{\textasciitilde}\,4000), with or without data quality filters (approval ratings). We found considerable differences between the sites, especially in comprehension, attention, and dishonesty. In Study 1 (without filters), we found that only Prolific provided high data quality on all measures. In Study 2 (with filters), we found high data quality among CloudResearch and Prolific. MTurk showed alarmingly low data quality even with data quality filters. We also found that while reputation (approval rating) did not predict data quality, frequency and purpose of usage did, especially on MTurk: the lowest data quality came from MTurk participants who report using the site as their main source of income but spend few hours on it per week. We provide a framework for future investigation into the ever-changing nature of data quality in online research, and how the evolving set of platforms and panels performs on these key aspects.},
  langid = {english},
  keywords = {Amazon mechanical turk,Attention,Comprehension,Data quality,Honesty,Online research,Prolific,Reliability},
  file = {C:\Users\mbch4gs2\Zotero\storage\YY2VD7W8\Peer et al. - 2021 - Data quality of platforms and panels for online be.pdf}
}

@article{peirce_2019,
  title = {{{PsychoPy2}}: {{Experiments}} in Behavior Made Easy},
  shorttitle = {{{PsychoPy2}}},
  author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {195--203},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01193-y},
  abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
  langid = {english},
  pmcid = {PMC6420413},
  pmid = {30734206},
  keywords = {Behavioral Research,Experiment,Humans,Open science,Open-source,Psychology,Reaction time,Reproducibility of Results,Software,Timing,User-Computer Interface},
  file = {C:\Users\mbch4gs2\Zotero\storage\93ITF788\Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf}
}

@article{prisse_2022,
  title = {Lab vs Online Experiments: {{No}} Differences},
  shorttitle = {Lab vs Online Experiments},
  author = {Priss{\'e}, Benjamin and Jorrat, Diego},
  year = {2022},
  month = oct,
  journal = {Journal of Behavioral and Experimental Economics},
  volume = {100},
  pages = {101910},
  issn = {2214-8043},
  doi = {10.1016/j.socec.2022.101910},
  urldate = {2024-10-04},
  abstract = {We ran an experiment to study whether not controlling, or the lack of control, of the experimental environment has an effect on experimental results. Subjects were recruited following standard procedures and randomly assigned to complete the experiment online or in the laboratory. The experimental design is otherwise identical between conditions. The results suggest that there are no differences between conditions, except for a larger percentage of online subjects who donate nothing in the Dictator Game.},
  keywords = {CTB,Experiments,Time preferences},
  file = {C:\Users\mbch4gs2\Zotero\storage\LU9C4H4S\S2214804322000842.html}
}

@misc{prolific_2024,
  title = {Prolific.Co},
  year = {2024},
  howpublished = {Prolific}
}

@manual{rcore,
  type = {Manual},
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2024},
  address = {Vienna, Austria},
  institution = {R Foundation for Statistical Computing}
}

@article{schielzeth_2020,
  title = {Robustness of Linear Mixed-Effects Models to Violations of Distributional Assumptions},
  author = {Schielzeth, Holger and Dingemanse, Niels J. and Nakagawa, Shinichi and Westneat, David F. and Allegue, Hassen and Teplitsky, C{\'e}line and R{\'e}ale, Denis and Dochtermann, Ned A. and Garamszegi, L{\'a}szl{\'o} Zsolt and {Araya-Ajoy}, Yimen G.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {9},
  pages = {1141--1152},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13434},
  urldate = {2023-11-29},
  abstract = {Linear mixed-effects models are powerful tools for analysing complex datasets with repeated or clustered observations, a common data structure in ecology and evolution. Mixed-effects models involve complex fitting procedures and make several assumptions, in particular about the distribution of residual and random effects. Violations of these assumptions are common in real datasets, yet it is not always clear how much these violations matter to accurate and unbiased estimation. Here we address the consequences of violations in distributional assumptions and the impact of missing random effect components on model estimates. In particular, we evaluate the effects of skewed, bimodal and heteroscedastic random effect and residual variances, of missing random effect terms and of correlated fixed effect predictors. We focus on bias and prediction error on estimates of fixed and random effects. Model estimates were usually robust to violations of assumptions, with the exception of slight upward biases in estimates of random effect variance if the generating distribution was bimodal but was modelled by Gaussian error distributions. Further, estimates for (random effect) components that violated distributional assumptions became less precise but remained unbiased. However, this particular problem did not affect other parameters of the model. The same pattern was found for strongly correlated fixed effects, which led to imprecise, but unbiased estimates, with uncertainty estimates reflecting imprecision. Unmodelled sources of random effect variance had predictable effects on variance component estimates. The pattern is best viewed as a cascade of hierarchical grouping factors. Variances trickle down the hierarchy such that missing higher-level random effect variances pool at lower levels and missing lower-level and crossed random effect variances manifest as residual variance. Overall, our results show remarkable robustness of mixed-effects models that should allow researchers to use mixed-effects models even if the distributional assumptions are objectively violated. However, this does not free researchers from careful evaluation of the model. Estimates that are based on data that show clear violations of key assumptions should be treated with caution because individual datasets might give highly imprecise estimates, even if they will be unbiased on average across datasets.},
  langid = {english},
  keywords = {biostatistics,correlated predictors,distributional assumptions,linear mixed-effects models,missing random effects,statistical quantification of individual differences (SQuID)},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\3ETINHH6\\Schielzeth et al. - 2020 - Robustness of linear mixed-effects models to viola.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\2D4QYXJB\\2041-210X.html}
}

@article{strain_2023,
  title = {The {{Effects}} of {{Contrast}} on {{Correlation Perception}} in {{Scatterplots}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = aug,
  journal = {International Journal of Human-Computer Studies},
  volume = {176},
  pages = {103040},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2023.103040},
  urldate = {2023-04-11},
  abstract = {Scatterplots are common data visualizations that can be used to communicate a range of ideas, the most intensively studied being the correlation between two variables. Despite their ubiquity, people typically do not perceive correlations between variables accurately from scatterplots, tending to underestimate the strength of the relationship displayed. Here we describe a two-experiment study in which we adjust the visual contrast of scatterplot points, and demonstrate a systematic approach to altering the bias. We find evidence that lowering the total visual contrast in a plot leads to increased bias in correlation estimates and show that decreasing the salience of points as a function of their distance from the regression line, by lowering their contrast, can facilitate more accurate correlation perception. We discuss the implications of these findings for visualization design, and provide a framework for online, reproducible, and large-sample-size (N = 150 per experiment) testing of the design parameters of data visualizations.},
  langid = {english},
  keywords = {Correlation perception,Crowdsourced,Data visualization,Scatterplot},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TGMNQWN3\\Strain et al. - 2023 - The Effects of Contrast on Correlation Perception .pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\DBRLAN7I\\S1071581923000496.html}
}

@inproceedings{strain_2023b,
  title = {Adjusting {{Point Size}} to {{Facilitate More Accurate Correlation Perception}} in {{Scatterplots}}},
  booktitle = {2023 {{IEEE Vis X Vision}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/VisXVision60716.2023.00006},
  urldate = {2024-02-15},
  abstract = {Viewers consistently underestimate correlation in positively correlated scatterplots. We use a novel data point size manipulation to correct for this bias. In a high-powered and fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot as a function of its distance from the regression line is able to correct for a systematic perceptual bias long present in the literature. We recommend the implementation of our technique when designing scatterplots that aim to communicate positive correlations.},
  isbn = {9798350329841},
  langid = {english},
  keywords = {Correlation,Empirical studies in HCI,Empirical studies in visualization,Human computer interaction,Human-centered computing,Human-centered computing-Human computer interaction (HCI),Systematics,Visualization},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TZCCRWMX\\Strain et al. - 2023 - Adjusting Point Size to Facilitate More Accurate C.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\AYEELKVC\\10350485.html}
}

@inproceedings{strain_2024,
  title = {Effects of {{Point Size}} and {{Opacity Adjustments}} in {{Scatterplots}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul A. and Jay, Caroline},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642127},
  urldate = {2024-05-29},
  abstract = {Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive fashion. We show that there is a great deal of scope for using visual features to change viewers' perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.},
  isbn = {9798400703300},
  keywords = {correlation,crowdsourced,perception,scatterplot},
  file = {C:\Users\mbch4gs2\Zotero\storage\IGSEXHSG\Strain et al. - 2024 - Effects of Point Size and Opacity Adjustments in S.pdf}
}
