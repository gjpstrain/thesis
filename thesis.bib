@article{anwyl_2020,
  title = {Gorilla in Our Midst: {{An}} Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  urldate = {2024-10-03},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing---an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test--retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  keywords = {Artificial Intelligence,Attentional control,Browser timing,Online methods,Online research,Remote testing,Timing accuracy},
  file = {C:\Users\mbch4gs2\Zotero\storage\CI9CIJY4\Anwyl-Irvine et al. - 2020 - Gorilla in our midst An online behavioral experim.pdf}
}

@article{arechar_2018,
  title = {Conducting Interactive Experiments Online},
  author = {Arechar, Antonio A. and G{\"a}chter, Simon and Molleman, Lucas},
  year = {2018},
  month = mar,
  journal = {Experimental Economics},
  volume = {21},
  number = {1},
  pages = {99--131},
  issn = {1573-6938},
  doi = {10.1007/s10683-017-9527-2},
  urldate = {2024-10-04},
  abstract = {Online labor markets provide new opportunities for behavioral research, but conducting economic experiments online raises important methodological challenges. This particularly holds for interactive designs. In this paper, we provide a methodological discussion of the similarities and differences between interactive experiments conducted in the laboratory and online. To this end, we conduct a repeated public goods experiment with and without punishment using samples from the laboratory and the online platform Amazon Mechanical Turk. We chose to replicate this experiment because it is long and logistically complex. It therefore provides a good case study for discussing the methodological and practical challenges of online interactive experimentation. We find that basic behavioral patterns of cooperation and punishment in the laboratory are replicable online. The most important challenge of online interactive experiments is participant dropout. We discuss measures for reducing dropout and show that, for our case study, dropouts are exogenous to the experiment. We conclude that data quality for interactive experiments via the Internet is adequate and reliable, making online interactive experimentation a potentially valuable complement to laboratory studies.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,Behavioral research,C71,C88,C90,D71,Experimental methodology,Internet experiments,Public goods game,Punishment},
  file = {C:\Users\mbch4gs2\Zotero\storage\JYT8JH6G\Arechar et al. - 2018 - Conducting interactive experiments online.pdf}
}

@article{ayris_2018,
  title = {{{LIBER Open Science Roadmap}}},
  author = {Ayris, Paul and Bernal, Isabel and Cavalli, Valentino and Dorch, Bertil and Frey, Jeannette and Hallik, Martin and {Hormia-Poutanen}, Kistiina and {Labastida i Juan}, Ignasi and MacColl, John and Ponsati Obiols, Agn{\`e}s and Sacchi, Simone and Scholze, Frank and Schmidt, Birgit and Smit, Anja and Sofronijevic, Adam and Stojanovski, Jadranka and Svoboda, Martin and Tsakonas, Giannis and {van Otegem}, Matthijs and Verheusen, Astrid and Vilks, Andris and Widmark, Wilhelm and Horstmann, Wolfram},
  year = {2018},
  month = jul,
  publisher = {Ligue des biblioth{\`e}ques europ{\'e}ennes de recherche},
  doi = {10.20350/digitalCSIC/15061},
  urldate = {2023-09-13},
  abstract = {Embracing Open Science is critical if we are to make science more collaborative, reproducible, transparent and impactful. Open Science undoubtedly has the power to positively influence society, but its implementation is not yet universal.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2018-07-09T11:07:00Z}
}

@article{bridges_2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  urldate = {2024-10-03},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime{\textregistered}, NBS Presentation{\textregistered}, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\DTLJS2L6\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\IKCSU4QQ\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf}
}

@misc{chara_2021,
  title = {We Recently Went Viral on {{TikTok}} - Here's What We Learned},
  author = {Charalambides, Nick},
  year = {2021},
  month = aug,
  journal = {Prolific},
  urldate = {2024-10-04},
  howpublished = {https://www.prolific.com/resources/we-recently-went-viral-on-tiktok-heres-what-we-learned},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\UEAFCRQ5\we-recently-went-viral-on-tiktok-heres-what-we-learned.html}
}

@article{douglas_2023,
  title = {Data Quality in Online Human-Subjects Research: {{Comparisons}} between {{MTurk}}, {{Prolific}}, {{CloudResearch}}, {{Qualtrics}}, and {{SONA}}},
  shorttitle = {Data Quality in Online Human-Subjects Research},
  author = {Douglas, Benjamin D. and Ewell, Patrick J. and Brauer, Markus},
  year = {2023},
  month = mar,
  journal = {PLOS ONE},
  volume = {18},
  number = {3},
  pages = {e0279720},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0279720},
  urldate = {2024-10-04},
  abstract = {With the proliferation of online data collection in human-subjects research, concerns have been raised over the presence of inattentive survey participants and non-human respondents (bots). We compared the quality of the data collected through five commonly used platforms. Data quality was indicated by the percentage of participants who meaningfully respond to the researcher's question (high quality) versus those who only contribute noise (low quality). We found that compared to MTurk, Qualtrics, or an undergraduate student sample (i.e., SONA), participants on Prolific and CloudResearch were more likely to pass various attention checks, provide meaningful answers, follow instructions, remember previously presented information, have a unique IP address and geolocation, and work slowly enough to be able to read all the items. We divided the samples into high- and low-quality respondents and computed the cost we paid per high-quality respondent. Prolific (\$1.90) and CloudResearch (\$2.00) were cheaper than MTurk (\$4.36) and Qualtrics (\$8.17). SONA cost \$0.00, yet took the longest to collect the data.},
  langid = {english},
  keywords = {Attention,Ethnicities,Payment,Personality,Personality traits,Surveys,Undergraduates,United States},
  file = {C:\Users\mbch4gs2\Zotero\storage\E8Q8FA4D\Douglas et al. - 2023 - Data quality in online human-subjects research Comparisons between MTurk, Prolific, CloudResearch,.pdf}
}

@misc{eprime_2020,
  title = {E-{{Prime}}},
  year = {2020},
  howpublished = {Psychology Software Tools}
}

@article{hirao_2021,
  title = {Reliability of {{Online Surveys}} in {{Investigating Perceptions}} and {{Impressions}} of {{Faces}}},
  author = {Hirao, Naoyasu and Koizumi, Koyo and Ikeda, Hanako and Ohira, Hideki},
  year = {2021},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {12},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.733405},
  urldate = {2024-10-04},
  abstract = {{$<$}p{$>$}Online experimental methods are used in psychological studies investigating the perceptions and impressions of facial photographs, even without substantial evidence supporting their reliability and validity. Although, the quality of visual stimuli is more difficult to control remotely, the methods might allow us to obtain a large amount of data. Then the statistical analysis of a larger volume of data may reduce errors and suggest significant difference in the stimuli. Therefore, we analyzed the reliability and validity of online surveys in investigating the perceptions (shine, red, and dark) and impressions (attractiveness, trustworthy, and so on) of facial photographs created from averaged faces with skin tones modified using computer graphics (CG). In this study, we conducted online (Online1) and laboratory experiments with well-controlled conditions (Control). For each experiment, 50 participants (men and women in Japan, age: 20--59years) completed the same questionnaire regarding their impressions of the same 28 CG facial photographs. The results showed significant correlations between the two experiments for all 19 items in the questionnaire. SD in the Online1 compared to the Control from the stimuli and individual differences were 56--84 and 88--104\% in each questionnaire items, respectively. Moreover, the rates of mismatching perceptual evaluations to the corresponding physical features demonstrated in the photographs were 4.9--9.7\% on average in an additional online survey of another 2,000 participants (Online2). These results suggest that online surveys can be applied to experiments to investigate impressions from CG facial photographs instead of general laboratory experiment by obtaining an appropriate number of participants to offset larger statistical errors that may result from the increased noise in the data from conducting the experiment online.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Face,Impression,Online,Reliability - reproducibility of results,Survey},
  file = {C:\Users\mbch4gs2\Zotero\storage\P7HRDX7X\Hirao et al. - 2021 - Reliability of Online Surveys in Investigating Per.pdf}
}

@article{mathot_2012,
  title = {{{OpenSesame}}: {{An}} Open-Source, Graphical Experiment Builder for the Social Sciences},
  shorttitle = {{{OpenSesame}}},
  author = {Math{\^o}t, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
  year = {2012},
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {314--324},
  issn = {1554-351X},
  doi = {10.3758/s13428-011-0168-7},
  urldate = {2024-10-03},
  abstract = {In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
  pmcid = {PMC3356517},
  pmid = {22083660},
  file = {C:\Users\mbch4gs2\Zotero\storage\RLJFCGWK\Math√¥t et al. - 2012 - OpenSesame An open-source, graphical experiment b.pdf}
}

@article{peer_2021,
  title = {Data Quality of Platforms and Panels for Online Behavioral Research},
  author = {Peer, Eyal and Rothschild, David and Gordon, Andrew and Evernden, Zak and Damer, Ekaterina},
  year = {2021},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {4},
  pages = {1643--1662},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01694-3},
  urldate = {2022-07-05},
  abstract = {We examine key aspects of data quality for online behavioral research between selected platforms (Amazon Mechanical Turk, CloudResearch, and Prolific) and panels (Qualtrics and Dynata). To identify the key aspects of data quality, we first engaged with the behavioral research community to discover which aspects are most critical to researchers and found that these include attention, comprehension, honesty, and reliability. We then explored differences in these data quality aspects in two studies (N\,{\textasciitilde}\,4000), with or without data quality filters (approval ratings). We found considerable differences between the sites, especially in comprehension, attention, and dishonesty. In Study 1 (without filters), we found that only Prolific provided high data quality on all measures. In Study 2 (with filters), we found high data quality among CloudResearch and Prolific. MTurk showed alarmingly low data quality even with data quality filters. We also found that while reputation (approval rating) did not predict data quality, frequency and purpose of usage did, especially on MTurk: the lowest data quality came from MTurk participants who report using the site as their main source of income but spend few hours on it per week. We provide a framework for future investigation into the ever-changing nature of data quality in online research, and how the evolving set of platforms and panels performs on these key aspects.},
  langid = {english},
  keywords = {Amazon mechanical turk,Attention,Comprehension,Data quality,Honesty,Online research,Prolific,Reliability},
  file = {C:\Users\mbch4gs2\Zotero\storage\YY2VD7W8\Peer et al. - 2021 - Data quality of platforms and panels for online be.pdf}
}

@article{peirce_2019,
  title = {{{PsychoPy2}}: {{Experiments}} in Behavior Made Easy},
  shorttitle = {{{PsychoPy2}}},
  author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {195--203},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01193-y},
  abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
  langid = {english},
  pmcid = {PMC6420413},
  pmid = {30734206},
  keywords = {Behavioral Research,Experiment,Humans,Open science,Open-source,Psychology,Reaction time,Reproducibility of Results,Software,Timing,User-Computer Interface},
  file = {C:\Users\mbch4gs2\Zotero\storage\93ITF788\Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf}
}

@article{prisse_2022,
  title = {Lab vs Online Experiments: {{No}} Differences},
  shorttitle = {Lab vs Online Experiments},
  author = {Priss{\'e}, Benjamin and Jorrat, Diego},
  year = {2022},
  month = oct,
  journal = {Journal of Behavioral and Experimental Economics},
  volume = {100},
  pages = {101910},
  issn = {2214-8043},
  doi = {10.1016/j.socec.2022.101910},
  urldate = {2024-10-04},
  abstract = {We ran an experiment to study whether not controlling, or the lack of control, of the experimental environment has an effect on experimental results. Subjects were recruited following standard procedures and randomly assigned to complete the experiment online or in the laboratory. The experimental design is otherwise identical between conditions. The results suggest that there are no differences between conditions, except for a larger percentage of online subjects who donate nothing in the Dictator Game.},
  keywords = {CTB,Experiments,Time preferences},
  file = {C:\Users\mbch4gs2\Zotero\storage\LU9C4H4S\S2214804322000842.html}
}

@misc{prolific_2024,
  title = {Prolific.Co},
  year = {2024},
  howpublished = {Prolific}
}

@article{schielzeth_2020,
  title = {Robustness of Linear Mixed-Effects Models to Violations of Distributional Assumptions},
  author = {Schielzeth, Holger and Dingemanse, Niels J. and Nakagawa, Shinichi and Westneat, David F. and Allegue, Hassen and Teplitsky, C{\'e}line and R{\'e}ale, Denis and Dochtermann, Ned A. and Garamszegi, L{\'a}szl{\'o} Zsolt and {Araya-Ajoy}, Yimen G.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {9},
  pages = {1141--1152},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13434},
  urldate = {2023-11-29},
  abstract = {Linear mixed-effects models are powerful tools for analysing complex datasets with repeated or clustered observations, a common data structure in ecology and evolution. Mixed-effects models involve complex fitting procedures and make several assumptions, in particular about the distribution of residual and random effects. Violations of these assumptions are common in real datasets, yet it is not always clear how much these violations matter to accurate and unbiased estimation. Here we address the consequences of violations in distributional assumptions and the impact of missing random effect components on model estimates. In particular, we evaluate the effects of skewed, bimodal and heteroscedastic random effect and residual variances, of missing random effect terms and of correlated fixed effect predictors. We focus on bias and prediction error on estimates of fixed and random effects. Model estimates were usually robust to violations of assumptions, with the exception of slight upward biases in estimates of random effect variance if the generating distribution was bimodal but was modelled by Gaussian error distributions. Further, estimates for (random effect) components that violated distributional assumptions became less precise but remained unbiased. However, this particular problem did not affect other parameters of the model. The same pattern was found for strongly correlated fixed effects, which led to imprecise, but unbiased estimates, with uncertainty estimates reflecting imprecision. Unmodelled sources of random effect variance had predictable effects on variance component estimates. The pattern is best viewed as a cascade of hierarchical grouping factors. Variances trickle down the hierarchy such that missing higher-level random effect variances pool at lower levels and missing lower-level and crossed random effect variances manifest as residual variance. Overall, our results show remarkable robustness of mixed-effects models that should allow researchers to use mixed-effects models even if the distributional assumptions are objectively violated. However, this does not free researchers from careful evaluation of the model. Estimates that are based on data that show clear violations of key assumptions should be treated with caution because individual datasets might give highly imprecise estimates, even if they will be unbiased on average across datasets.},
  langid = {english},
  keywords = {biostatistics,correlated predictors,distributional assumptions,linear mixed-effects models,missing random effects,statistical quantification of individual differences (SQuID)},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\3ETINHH6\\Schielzeth et al. - 2020 - Robustness of linear mixed-effects models to viola.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\2D4QYXJB\\2041-210X.html}
}

@article{strain_2023,
  title = {The {{Effects}} of {{Contrast}} on {{Correlation Perception}} in {{Scatterplots}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = aug,
  journal = {International Journal of Human-Computer Studies},
  volume = {176},
  pages = {103040},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2023.103040},
  urldate = {2023-04-11},
  abstract = {Scatterplots are common data visualizations that can be used to communicate a range of ideas, the most intensively studied being the correlation between two variables. Despite their ubiquity, people typically do not perceive correlations between variables accurately from scatterplots, tending to underestimate the strength of the relationship displayed. Here we describe a two-experiment study in which we adjust the visual contrast of scatterplot points, and demonstrate a systematic approach to altering the bias. We find evidence that lowering the total visual contrast in a plot leads to increased bias in correlation estimates and show that decreasing the salience of points as a function of their distance from the regression line, by lowering their contrast, can facilitate more accurate correlation perception. We discuss the implications of these findings for visualization design, and provide a framework for online, reproducible, and large-sample-size (N = 150 per experiment) testing of the design parameters of data visualizations.},
  langid = {english},
  keywords = {Correlation perception,Crowdsourced,Data visualization,Scatterplot},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TGMNQWN3\\Strain et al. - 2023 - The Effects of Contrast on Correlation Perception .pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\DBRLAN7I\\S1071581923000496.html}
}

@inproceedings{strain_2023b,
  title = {Adjusting {{Point Size}} to {{Facilitate More Accurate Correlation Perception}} in {{Scatterplots}}},
  booktitle = {2023 {{IEEE Vis X Vision}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/VisXVision60716.2023.00006},
  urldate = {2024-02-15},
  abstract = {Viewers consistently underestimate correlation in positively correlated scatterplots. We use a novel data point size manipulation to correct for this bias. In a high-powered and fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot as a function of its distance from the regression line is able to correct for a systematic perceptual bias long present in the literature. We recommend the implementation of our technique when designing scatterplots that aim to communicate positive correlations.},
  isbn = {9798350329841},
  langid = {english},
  keywords = {Correlation,Empirical studies in HCI,Empirical studies in visualization,Human computer interaction,Human-centered computing,Human-centered computing-Human computer interaction (HCI),Systematics,Visualization},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TZCCRWMX\\Strain et al. - 2023 - Adjusting Point Size to Facilitate More Accurate C.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\AYEELKVC\\10350485.html}
}

@inproceedings{strain_2024,
  title = {Effects of {{Point Size}} and {{Opacity Adjustments}} in {{Scatterplots}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul A. and Jay, Caroline},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642127},
  urldate = {2024-05-29},
  abstract = {Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive fashion. We show that there is a great deal of scope for using visual features to change viewers' perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.},
  isbn = {9798400703300},
  keywords = {correlation,crowdsourced,perception,scatterplot},
  file = {C:\Users\mbch4gs2\Zotero\storage\IGSEXHSG\Strain et al. - 2024 - Effects of Point Size and Opacity Adjustments in S.pdf}
}
