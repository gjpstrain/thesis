@manual{allaire_2024,
  type = {Manual},
  title = {Quarto: {{R}} Interface to 'quarto' Markdown Publishing System},
  author = {Allaire, {\relax JJ} and Dervieux, Christophe},
  year = {2024}
}

@article{anwyl_2020,
  title = {Gorilla in Our Midst: {{An}} Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  urldate = {2024-10-03},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing---an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test--retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  keywords = {Artificial Intelligence,Attentional control,Browser timing,Online methods,Online research,Remote testing,Timing accuracy},
  file = {C:\Users\mbch4gs2\Zotero\storage\CI9CIJY4\Anwyl-Irvine et al. - 2020 - Gorilla in our midst An online behavioral experim.pdf}
}

@article{arechar_2018,
  title = {Conducting Interactive Experiments Online},
  author = {Arechar, Antonio A. and G{\"a}chter, Simon and Molleman, Lucas},
  year = {2018},
  month = mar,
  journal = {Experimental Economics},
  volume = {21},
  number = {1},
  pages = {99--131},
  issn = {1573-6938},
  doi = {10.1007/s10683-017-9527-2},
  urldate = {2024-10-04},
  abstract = {Online labor markets provide new opportunities for behavioral research, but conducting economic experiments online raises important methodological challenges. This particularly holds for interactive designs. In this paper, we provide a methodological discussion of the similarities and differences between interactive experiments conducted in the laboratory and online. To this end, we conduct a repeated public goods experiment with and without punishment using samples from the laboratory and the online platform Amazon Mechanical Turk. We chose to replicate this experiment because it is long and logistically complex. It therefore provides a good case study for discussing the methodological and practical challenges of online interactive experimentation. We find that basic behavioral patterns of cooperation and punishment in the laboratory are replicable online. The most important challenge of online interactive experiments is participant dropout. We discuss measures for reducing dropout and show that, for our case study, dropouts are exogenous to the experiment. We conclude that data quality for interactive experiments via the Internet is adequate and reliable, making online interactive experimentation a potentially valuable complement to laboratory studies.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,Behavioral research,C71,C88,C90,D71,Experimental methodology,Internet experiments,Public goods game,Punishment},
  file = {C:\Users\mbch4gs2\Zotero\storage\JYT8JH6G\Arechar et al. - 2018 - Conducting interactive experiments online.pdf}
}

@article{ayris_2018,
  title = {{{LIBER Open Science Roadmap}}},
  author = {Ayris, Paul and Bernal, Isabel and Cavalli, Valentino and Dorch, Bertil and Frey, Jeannette and Hallik, Martin and {Hormia-Poutanen}, Kistiina and {Labastida i Juan}, Ignasi and MacColl, John and Ponsati Obiols, Agn{\`e}s and Sacchi, Simone and Scholze, Frank and Schmidt, Birgit and Smit, Anja and Sofronijevic, Adam and Stojanovski, Jadranka and Svoboda, Martin and Tsakonas, Giannis and {van Otegem}, Matthijs and Verheusen, Astrid and Vilks, Andris and Widmark, Wilhelm and Horstmann, Wolfram},
  year = {2018},
  month = jul,
  publisher = {Ligue des biblioth{\`e}ques europ{\'e}ennes de recherche},
  doi = {10.20350/digitalCSIC/15061},
  urldate = {2023-09-13},
  abstract = {Embracing Open Science is critical if we are to make science more collaborative, reproducible, transparent and impactful. Open Science undoubtedly has the power to positively influence society, but its implementation is not yet universal.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2018-07-09T11:07:00Z}
}

@article{barr_2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  journal = {Journal of memory and language},
  volume = {68},
  number = {3},
  pages = {10.1016/j.jml.2012.11.001},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.11.001},
  urldate = {2022-08-23},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  pmcid = {PMC3881361},
  pmid = {24403724},
  file = {C:\Users\mbch4gs2\Zotero\storage\S7MDHRLZ\Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf}
}

@misc{bates_2018,
  title = {Parsimonious {{Mixed Models}}},
  author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
  year = {2018},
  journal = {arXiv.org},
  urldate = {2024-10-09},
  abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
  howpublished = {https://arxiv.org/abs/1506.04967v2},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\3DPP68UG\Bates et al. - 2015 - Parsimonious Mixed Models.pdf}
}

@article{ben-shacharEffectsizeEstimationEffect2020,
  title = {{{e}}ffectsize: {{Estimation}} of Effect Size Indices and Standardized Parameters},
  author = {{Ben-Shachar}, Mattan S. and L{\"u}decke, Daniel and Makowski, Dominique},
  year = {2020},
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {56},
  pages = {2815},
  publisher = {The Open Journal},
  doi = {10.21105/joss.02815}
}

@article{boettiger_2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  year = {2015},
  month = jan,
  journal = {SIGOPS Oper. Syst. Rev.},
  volume = {49},
  number = {1},
  pages = {71--79},
  issn = {0163-5980},
  doi = {10.1145/2723872.2723882},
  urldate = {2024-10-09},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a 'DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  file = {C:\Users\mbch4gs2\Zotero\storage\S24E64X2\Boettiger - 2015 - An introduction to Docker for reproducible research.pdf}
}

@article{bridges_2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  urldate = {2024-10-03},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime{\textregistered}, NBS Presentation{\textregistered}, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\DTLJS2L6\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\IKCSU4QQ\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf}
}

@article{brown_2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2024-10-08},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\L39C4ZEA\Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling in R.pdf}
}

@manual{buildmer,
  type = {Manual},
  title = {Buildmer: {{Stepwise}} Elimination and Term Reordering for Mixed-Effects Regression},
  author = {Voeten, Cesko C.},
  year = {2023}
}

@misc{chara_2021,
  title = {We Recently Went Viral on {{TikTok}} - Here's What We Learned},
  author = {Charalambides, Nick},
  year = {2021},
  month = aug,
  journal = {Prolific},
  urldate = {2024-10-04},
  howpublished = {https://www.prolific.com/resources/we-recently-went-viral-on-tiktok-heres-what-we-learned},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\UEAFCRQ5\we-recently-went-viral-on-tiktok-heres-what-we-learned.html}
}

@article{collberg_2016,
  title = {Repeatability in Computer Systems Research},
  author = {Collberg, Christian and Proebsting, Todd A.},
  year = {2016},
  month = feb,
  journal = {Communications of the ACM},
  volume = {59},
  number = {3},
  pages = {62--69},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2812803},
  urldate = {2024-10-09},
  abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
  langid = {english}
}

@article{douglas_2023,
  title = {Data Quality in Online Human-Subjects Research: {{Comparisons}} between {{MTurk}}, {{Prolific}}, {{CloudResearch}}, {{Qualtrics}}, and {{SONA}}},
  shorttitle = {Data Quality in Online Human-Subjects Research},
  author = {Douglas, Benjamin D. and Ewell, Patrick J. and Brauer, Markus},
  year = {2023},
  month = mar,
  journal = {PLOS ONE},
  volume = {18},
  number = {3},
  pages = {e0279720},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0279720},
  urldate = {2024-10-04},
  abstract = {With the proliferation of online data collection in human-subjects research, concerns have been raised over the presence of inattentive survey participants and non-human respondents (bots). We compared the quality of the data collected through five commonly used platforms. Data quality was indicated by the percentage of participants who meaningfully respond to the researcher's question (high quality) versus those who only contribute noise (low quality). We found that compared to MTurk, Qualtrics, or an undergraduate student sample (i.e., SONA), participants on Prolific and CloudResearch were more likely to pass various attention checks, provide meaningful answers, follow instructions, remember previously presented information, have a unique IP address and geolocation, and work slowly enough to be able to read all the items. We divided the samples into high- and low-quality respondents and computed the cost we paid per high-quality respondent. Prolific (\$1.90) and CloudResearch (\$2.00) were cheaper than MTurk (\$4.36) and Qualtrics (\$8.17). SONA cost \$0.00, yet took the longest to collect the data.},
  langid = {english},
  keywords = {Attention,Ethnicities,Payment,Personality,Personality traits,Surveys,Undergraduates,United States},
  file = {C:\Users\mbch4gs2\Zotero\storage\E8Q8FA4D\Douglas et al. - 2023 - Data quality in online human-subjects research Comparisons between MTurk, Prolific, CloudResearch,.pdf}
}

@manual{ematools,
  type = {Manual},
  title = {{{EMAtools}}: {{Data}} Management Tools for Real-Time Monitoring/Ecological Momentary Assessment Data},
  author = {Kleiman, Evan},
  year = {2021}
}

@manual{emmeans,
  type = {Manual},
  title = {Emmeans: {{Estimated}} Marginal Means, Aka Least-Squares Means},
  author = {Lenth, Russell V.},
  year = {2024}
}

@misc{eprime_2020,
  title = {E-{{Prime}}},
  year = {2020},
  howpublished = {Psychology Software Tools}
}

@book{hadley_gg2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {Springer-Verlag New York},
  urldate = {2022-09-14},
  isbn = {978-3-319-24277-4},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\3IA3HC9Z\978-3-319-24277-4.html}
}

@article{hirao_2021,
  title = {Reliability of {{Online Surveys}} in {{Investigating Perceptions}} and {{Impressions}} of {{Faces}}},
  author = {Hirao, Naoyasu and Koizumi, Koyo and Ikeda, Hanako and Ohira, Hideki},
  year = {2021},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {12},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.733405},
  urldate = {2024-10-04},
  abstract = {{$<$}p{$>$}Online experimental methods are used in psychological studies investigating the perceptions and impressions of facial photographs, even without substantial evidence supporting their reliability and validity. Although, the quality of visual stimuli is more difficult to control remotely, the methods might allow us to obtain a large amount of data. Then the statistical analysis of a larger volume of data may reduce errors and suggest significant difference in the stimuli. Therefore, we analyzed the reliability and validity of online surveys in investigating the perceptions (shine, red, and dark) and impressions (attractiveness, trustworthy, and so on) of facial photographs created from averaged faces with skin tones modified using computer graphics (CG). In this study, we conducted online (Online1) and laboratory experiments with well-controlled conditions (Control). For each experiment, 50 participants (men and women in Japan, age: 20--59years) completed the same questionnaire regarding their impressions of the same 28 CG facial photographs. The results showed significant correlations between the two experiments for all 19 items in the questionnaire. SD in the Online1 compared to the Control from the stimuli and individual differences were 56--84 and 88--104\% in each questionnaire items, respectively. Moreover, the rates of mismatching perceptual evaluations to the corresponding physical features demonstrated in the photographs were 4.9--9.7\% on average in an additional online survey of another 2,000 participants (Online2). These results suggest that online surveys can be applied to experiments to investigate impressions from CG facial photographs instead of general laboratory experiment by obtaining an appropriate number of participants to offset larger statistical errors that may result from the increased noise in the data from conducting the experiment online.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Face,Impression,Online,Reliability - reproducibility of results,Survey},
  file = {C:\Users\mbch4gs2\Zotero\storage\P7HRDX7X\Hirao et al. - 2021 - Reliability of Online Surveys in Investigating Per.pdf}
}

@article{holmes_2021,
  title = {Reproducible Manuscript Preparation with {{RMarkdown}} Application to {{JMSACL}} and Other {{Elsevier Journals}}},
  author = {Holmes, Daniel T. and Mobini, Mahdi and McCudden, Christopher R.},
  year = {2021},
  month = nov,
  journal = {Journal of Mass Spectrometry and Advances in the Clinical Lab},
  volume = {22},
  pages = {8--16},
  issn = {2667145X},
  doi = {10.1016/j.jmsacl.2021.09.002},
  urldate = {2024-10-09},
  abstract = {Introduction: With the rising complexity of modern multimarker analytical techniques and notable scientific publication retractions required for erroneous statistical analysis, there is increasing awareness of the importance of research transparency and reproducibility. The development of mature open-source tools for literate programming in multiple langauge paradigms has made fully-reproducible authorship possible. Objectives: We describe the procedure for manuscript preparation using RMarkdown and the R statistical programming language with application to JMSACL or any other Elsevier journal. Methods: An instructional manuscript has been prepared in the RMarkdown markup language with stepwise directions on preparing sections, subsections, lists, tables, figures and reference management in an entirely reproducible format. Results: From RMarkdown code, a submission-ready PDF is generated and JMSACL-compatible LaTeX code is generated. These can be uploaded to the Editorial Manager. Conclusion: A completely reproducible manuscript preparation pipeline using the R and RMarkdown is described.},
  langid = {english},
  file = {C:\Users\mbch4gs2\Zotero\storage\PEQSK4WG\Holmes et al. - 2021 - Reproducible manuscript preparation with RMarkdown application to JMSACL and other Elsevier Journals.pdf}
}

@manual{irr,
  type = {Manual},
  title = {Irr: {{Various}} Coefficients of Interrater Reliability and Agreement},
  author = {Gamer, Matthias and Lemon, Jim and {\textexclamdown}puspendra.pusp22@gmail.com{\textquestiondown}, Ian Fellows Puspendra Singh},
  year = {2019}
}

@article{leisch_2002,
  title = {Sweave, Part {{I}}: {{Mixing R}} and {{LaTeX}}},
  shorttitle = {Sweave, Part {{I}}},
  author = {Leisch, Friedrich},
  year = {2002},
  month = dec,
  journal = {R News},
  volume = {2},
  number = {3},
  pages = {28--31},
  issn = {1609-3631},
  urldate = {2024-10-09},
  abstract = {"Sweave, part I: Mixing R and LaTeX" published in R News.}
}

@article{liddell_2018,
  title = {Analyzing Ordinal Data with Metric Models: {{What}} Could Possibly Go Wrong?},
  shorttitle = {Analyzing Ordinal Data with Metric Models},
  author = {Liddell, Torrin M. and Kruschke, John K.},
  year = {2018},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  volume = {79},
  pages = {328--348},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2018.08.009},
  urldate = {2024-09-03},
  abstract = {We surveyed all articles in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology: General (JEP:G) that mentioned the term ``Likert,'' and found that 100\% of the articles that analyzed ordinal data did so using a metric model. We present novel evidence that analyzing ordinal data as if they were metric can systematically lead to errors. We demonstrate false alarms (i.e., detecting an effect where none exists, Type I errors) and failures to detect effects (i.e., loss of power, Type II errors). We demonstrate systematic inversions of effects, for which treating ordinal data as metric indicates the opposite ordering of means than the true ordering of means. We show the same problems --- false alarms, misses, and inversions --- for interactions in factorial designs and for trend analyses in regression. We demonstrate that averaging across multiple ordinal measurements does not solve or even ameliorate these problems. A central contribution is a graphical explanation of how and when the misrepresentations occur. Moreover, we point out that there is no sure-fire way to detect these problems by treating the ordinal values as metric, and instead we advocate use of ordered-probit models (or similar) because they will better describe the data. Finally, although frequentist approaches to some ordered-probit models are available, we use Bayesian methods because of their flexibility in specifying models and their richness and accuracy in providing parameter estimates. An R script is provided for running an analysis that compares ordered-probit and metric models.},
  keywords = {Bayesian analysis,Likert,Ordered-probit,Ordinal data},
  file = {C:\Users\mbch4gs2\Zotero\storage\ILW7PTGH\Liddell and Kruschke - 2018 - Analyzing ordinal data with metric models What co.pdf}
}

@article{mathot_2012,
  title = {{{OpenSesame}}: {{An}} Open-Source, Graphical Experiment Builder for the Social Sciences},
  shorttitle = {{{OpenSesame}}},
  author = {Math{\^o}t, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
  year = {2012},
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {314--324},
  issn = {1554-351X},
  doi = {10.3758/s13428-011-0168-7},
  urldate = {2024-10-03},
  abstract = {In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
  pmcid = {PMC3356517},
  pmid = {22083660},
  file = {C:\Users\mbch4gs2\Zotero\storage\RLJFCGWK\Math√¥t et al. - 2012 - OpenSesame An open-source, graphical experiment b.pdf}
}

@article{merkel_2014,
  title = {Docker: Lightweight {{Linux}} Containers for Consistent Development and Deployment},
  shorttitle = {Docker},
  author = {Merkel, Dirk},
  year = {2014},
  month = mar,
  journal = {Linux J.},
  volume = {2014},
  number = {239},
  pages = {2:2},
  issn = {1075-3583},
  abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.}
}

@article{meteyard_2020,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2024-10-09},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods -- a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\D465EMBS\\Meteyard and Davies - 2020 - Best practice guidance for linear mixed-effects models in psychological science.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\TDDPQS6U\\S0749596X20300061.html}
}

@article{nakagawa_2013,
  title = {A General and Simple Method for Obtaining {{R2}} from Generalized Linear Mixed-Effects Models},
  author = {Nakagawa, Shinichi and Schielzeth, Holger},
  year = {2013},
  journal = {Methods in Ecology and Evolution},
  volume = {4},
  number = {2},
  pages = {133--142},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210x.2012.00261.x},
  urldate = {2023-09-07},
  abstract = {The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models. The presentation of `variance explained' (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest. One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation). Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems. This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances.},
  langid = {english},
  keywords = {coefficient of determination,goodness-of-fit,heritability,information criteria,intra-class correlation,linear models,model fit,repeatability,variance explained},
  file = {C:\Users\mbch4gs2\Zotero\storage\B82C6FKK\j.2041-210x.2012.00261.html}
}

@manual{ordinal,
  type = {Manual},
  title = {Ordinal---Regression Models for Ordinal Data},
  author = {Christensen, Rune H. B.},
  year = {2023}
}

@misc{pandoc,
  title = {Pandoc},
  author = {MacFarlane, John},
  urldate = {2024-10-09},
  howpublished = {https://pandoc.org/index.html},
  file = {C:\Users\mbch4gs2\Zotero\storage\J4KGFDGI\index.html}
}

@article{peer_2021,
  title = {Data Quality of Platforms and Panels for Online Behavioral Research},
  author = {Peer, Eyal and Rothschild, David and Gordon, Andrew and Evernden, Zak and Damer, Ekaterina},
  year = {2021},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {4},
  pages = {1643--1662},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01694-3},
  urldate = {2022-07-05},
  abstract = {We examine key aspects of data quality for online behavioral research between selected platforms (Amazon Mechanical Turk, CloudResearch, and Prolific) and panels (Qualtrics and Dynata). To identify the key aspects of data quality, we first engaged with the behavioral research community to discover which aspects are most critical to researchers and found that these include attention, comprehension, honesty, and reliability. We then explored differences in these data quality aspects in two studies (N\,{\textasciitilde}\,4000), with or without data quality filters (approval ratings). We found considerable differences between the sites, especially in comprehension, attention, and dishonesty. In Study 1 (without filters), we found that only Prolific provided high data quality on all measures. In Study 2 (with filters), we found high data quality among CloudResearch and Prolific. MTurk showed alarmingly low data quality even with data quality filters. We also found that while reputation (approval rating) did not predict data quality, frequency and purpose of usage did, especially on MTurk: the lowest data quality came from MTurk participants who report using the site as their main source of income but spend few hours on it per week. We provide a framework for future investigation into the ever-changing nature of data quality in online research, and how the evolving set of platforms and panels performs on these key aspects.},
  langid = {english},
  keywords = {Amazon mechanical turk,Attention,Comprehension,Data quality,Honesty,Online research,Prolific,Reliability},
  file = {C:\Users\mbch4gs2\Zotero\storage\YY2VD7W8\Peer et al. - 2021 - Data quality of platforms and panels for online be.pdf}
}

@article{peirce_2019,
  title = {{{PsychoPy2}}: {{Experiments}} in Behavior Made Easy},
  shorttitle = {{{PsychoPy2}}},
  author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {195--203},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01193-y},
  abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
  langid = {english},
  pmcid = {PMC6420413},
  pmid = {30734206},
  keywords = {Behavioral Research,Experiment,Humans,Open science,Open-source,Psychology,Reaction time,Reproducibility of Results,Software,Timing,User-Computer Interface},
  file = {C:\Users\mbch4gs2\Zotero\storage\93ITF788\Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf}
}

@article{peng_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1213847},
  urldate = {2024-10-09},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  file = {C:\Users\mbch4gs2\Zotero\storage\UDVSLPAC\Peng - 2011 - Reproducible Research in Computational Science.pdf}
}

@article{piccolo_2016,
  title = {Tools and Techniques for Computational Reproducibility},
  author = {Piccolo, Stephen R and Frampton, Michael B},
  year = {2016},
  month = dec,
  journal = {GigaScience},
  volume = {5},
  number = {1},
  pages = {s13742-016-0135-4},
  issn = {2047-217X},
  doi = {10.1186/s13742-016-0135-4},
  urldate = {2024-10-09},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed---and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\77UXH3QG\\Piccolo and Frampton - 2016 - Tools and techniques for computational reproducibility.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\4LCTUVIA\\2720991.html}
}

@article{prisse_2022,
  title = {Lab vs Online Experiments: {{No}} Differences},
  shorttitle = {Lab vs Online Experiments},
  author = {Priss{\'e}, Benjamin and Jorrat, Diego},
  year = {2022},
  month = oct,
  journal = {Journal of Behavioral and Experimental Economics},
  volume = {100},
  pages = {101910},
  issn = {2214-8043},
  doi = {10.1016/j.socec.2022.101910},
  urldate = {2024-10-04},
  abstract = {We ran an experiment to study whether not controlling, or the lack of control, of the experimental environment has an effect on experimental results. Subjects were recruited following standard procedures and randomly assigned to complete the experiment online or in the laboratory. The experimental design is otherwise identical between conditions. The results suggest that there are no differences between conditions, except for a larger percentage of online subjects who donate nothing in the Dictator Game.},
  keywords = {CTB,Experiments,Time preferences},
  file = {C:\Users\mbch4gs2\Zotero\storage\LU9C4H4S\S2214804322000842.html}
}

@misc{prolific_2024,
  title = {Prolific.Co},
  year = {2024},
  howpublished = {Prolific}
}

@manual{r2glmm,
  type = {Manual},
  title = {R2glmm: {{Computes R}} Squared for Mixed (Multilevel) Models},
  author = {Jaeger, Byron},
  year = {2017}
}

@manual{rcore,
  type = {Manual},
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2024},
  address = {Vienna, Austria},
  institution = {R Foundation for Statistical Computing}
}

@article{samuel_2024,
  title = {Computational Reproducibility of {{Jupyter}} Notebooks from Biomedical Publications},
  author = {Samuel, Sheeba and Mietchen, Daniel},
  year = {2024},
  month = jan,
  journal = {GigaScience},
  volume = {13},
  pages = {giad113},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giad113},
  urldate = {2024-10-09},
  abstract = {Jupyter notebooks facilitate the bundling of executable code with its documentation and output in one interactive environment, and they represent a popular mechanism to document and share computational workflows, including for research publications. The reproducibility of computational aspects of research is a key component of scientific reproducibility but has not yet been assessed at scale for Jupyter notebooks associated with biomedical publications.We address computational reproducibility at 2 levels: (i) using fully automated workflows, we analyzed the computational reproducibility of Jupyter notebooks associated with publications indexed in the biomedical literature repository PubMed Central. We identified such notebooks by mining the article's full text, trying to locate them on GitHub, and attempting to rerun them in an environment as close to the original as possible. We documented reproduction success and exceptions and explored relationships between notebook reproducibility and variables related to the notebooks or publications. (ii) This study represents a reproducibility attempt in and of itself, using essentially the same methodology twice on PubMed Central over the course of 2 years, during which the corpus of Jupyter notebooks from articles indexed in PubMed Central has grown in a highly dynamic fashion.Out of 27,271 Jupyter notebooks from 2,660 GitHub repositories associated with 3,467 publications, 22,578 notebooks were written in Python, including 15,817 that had their dependencies declared in standard requirement files and that we attempted to rerun automatically. For 10,388 of these, all declared dependencies could be installed successfully, and we reran them to assess reproducibility. Of these, 1,203 notebooks ran through without any errors, including 879 that produced results identical to those reported in the original notebook and 324 for which our results differed from the originally reported ones. Running the other notebooks resulted in exceptions.We zoom in on common problems and practices, highlight trends, and discuss potential improvements to Jupyter-related workflows associated with biomedical publications.},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\ED7ZQDPR\\Samuel and Mietchen - 2024 - Computational reproducibility of Jupyter notebooks from biomedical publications.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\BD84WRIK\\7516267.html}
}

@article{schielzeth_2020,
  title = {Robustness of Linear Mixed-Effects Models to Violations of Distributional Assumptions},
  author = {Schielzeth, Holger and Dingemanse, Niels J. and Nakagawa, Shinichi and Westneat, David F. and Allegue, Hassen and Teplitsky, C{\'e}line and R{\'e}ale, Denis and Dochtermann, Ned A. and Garamszegi, L{\'a}szl{\'o} Zsolt and {Araya-Ajoy}, Yimen G.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {9},
  pages = {1141--1152},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13434},
  urldate = {2023-11-29},
  abstract = {Linear mixed-effects models are powerful tools for analysing complex datasets with repeated or clustered observations, a common data structure in ecology and evolution. Mixed-effects models involve complex fitting procedures and make several assumptions, in particular about the distribution of residual and random effects. Violations of these assumptions are common in real datasets, yet it is not always clear how much these violations matter to accurate and unbiased estimation. Here we address the consequences of violations in distributional assumptions and the impact of missing random effect components on model estimates. In particular, we evaluate the effects of skewed, bimodal and heteroscedastic random effect and residual variances, of missing random effect terms and of correlated fixed effect predictors. We focus on bias and prediction error on estimates of fixed and random effects. Model estimates were usually robust to violations of assumptions, with the exception of slight upward biases in estimates of random effect variance if the generating distribution was bimodal but was modelled by Gaussian error distributions. Further, estimates for (random effect) components that violated distributional assumptions became less precise but remained unbiased. However, this particular problem did not affect other parameters of the model. The same pattern was found for strongly correlated fixed effects, which led to imprecise, but unbiased estimates, with uncertainty estimates reflecting imprecision. Unmodelled sources of random effect variance had predictable effects on variance component estimates. The pattern is best viewed as a cascade of hierarchical grouping factors. Variances trickle down the hierarchy such that missing higher-level random effect variances pool at lower levels and missing lower-level and crossed random effect variances manifest as residual variance. Overall, our results show remarkable robustness of mixed-effects models that should allow researchers to use mixed-effects models even if the distributional assumptions are objectively violated. However, this does not free researchers from careful evaluation of the model. Estimates that are based on data that show clear violations of key assumptions should be treated with caution because individual datasets might give highly imprecise estimates, even if they will be unbiased on average across datasets.},
  langid = {english},
  keywords = {biostatistics,correlated predictors,distributional assumptions,linear mixed-effects models,missing random effects,statistical quantification of individual differences (SQuID)},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\3ETINHH6\\Schielzeth et al. - 2020 - Robustness of linear mixed-effects models to viola.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\2D4QYXJB\\2041-210X.html}
}

@article{strain_2023,
  title = {The {{Effects}} of {{Contrast}} on {{Correlation Perception}} in {{Scatterplots}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = aug,
  journal = {International Journal of Human-Computer Studies},
  volume = {176},
  pages = {103040},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2023.103040},
  urldate = {2023-04-11},
  abstract = {Scatterplots are common data visualizations that can be used to communicate a range of ideas, the most intensively studied being the correlation between two variables. Despite their ubiquity, people typically do not perceive correlations between variables accurately from scatterplots, tending to underestimate the strength of the relationship displayed. Here we describe a two-experiment study in which we adjust the visual contrast of scatterplot points, and demonstrate a systematic approach to altering the bias. We find evidence that lowering the total visual contrast in a plot leads to increased bias in correlation estimates and show that decreasing the salience of points as a function of their distance from the regression line, by lowering their contrast, can facilitate more accurate correlation perception. We discuss the implications of these findings for visualization design, and provide a framework for online, reproducible, and large-sample-size (N = 150 per experiment) testing of the design parameters of data visualizations.},
  langid = {english},
  keywords = {Correlation perception,Crowdsourced,Data visualization,Scatterplot},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TGMNQWN3\\Strain et al. - 2023 - The Effects of Contrast on Correlation Perception .pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\DBRLAN7I\\S1071581923000496.html}
}

@inproceedings{strain_2023b,
  title = {Adjusting {{Point Size}} to {{Facilitate More Accurate Correlation Perception}} in {{Scatterplots}}},
  booktitle = {2023 {{IEEE Vis X Vision}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/VisXVision60716.2023.00006},
  urldate = {2024-02-15},
  abstract = {Viewers consistently underestimate correlation in positively correlated scatterplots. We use a novel data point size manipulation to correct for this bias. In a high-powered and fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot as a function of its distance from the regression line is able to correct for a systematic perceptual bias long present in the literature. We recommend the implementation of our technique when designing scatterplots that aim to communicate positive correlations.},
  isbn = {9798350329841},
  langid = {english},
  keywords = {Correlation,Empirical studies in HCI,Empirical studies in visualization,Human computer interaction,Human-centered computing,Human-centered computing-Human computer interaction (HCI),Systematics,Visualization},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TZCCRWMX\\Strain et al. - 2023 - Adjusting Point Size to Facilitate More Accurate C.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\AYEELKVC\\10350485.html}
}

@inproceedings{strain_2024,
  title = {Effects of {{Point Size}} and {{Opacity Adjustments}} in {{Scatterplots}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul A. and Jay, Caroline},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642127},
  urldate = {2024-05-29},
  abstract = {Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive fashion. We show that there is a great deal of scope for using visual features to change viewers' perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.},
  isbn = {9798400703300},
  keywords = {correlation,crowdsourced,perception,scatterplot},
  file = {C:\Users\mbch4gs2\Zotero\storage\IGSEXHSG\Strain et al. - 2024 - Effects of Point Size and Opacity Adjustments in S.pdf}
}

@article{trisovic_2022,
  title = {A Large-Scale Study on Research Code Quality and Execution},
  author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Merc{\`e}},
  year = {2022},
  month = feb,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {60},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01143-6},
  urldate = {2024-10-09},
  abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals' collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Information technology,Research data,Software},
  file = {C:\Users\mbch4gs2\Zotero\storage\YAWPBCQQ\Trisovic et al. - 2022 - A large-scale study on research code quality and execution.pdf}
}

@book{xie_2015,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{xie_2020,
  title = {R Markdown Cookbook},
  author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
  year = {2020},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida},
  isbn = {978-0-367-56383-7}
}
