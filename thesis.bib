@article{anwyl_2020,
  title = {Gorilla in Our Midst: {{An}} Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  urldate = {2024-10-03},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing---an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test--retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  keywords = {Artificial Intelligence,Attentional control,Browser timing,Online methods,Online research,Remote testing,Timing accuracy},
  file = {C:\Users\mbch4gs2\Zotero\storage\CI9CIJY4\Anwyl-Irvine et al. - 2020 - Gorilla in our midst An online behavioral experim.pdf}
}

@article{ayris_2018,
  title = {{{LIBER Open Science Roadmap}}},
  author = {Ayris, Paul and Bernal, Isabel and Cavalli, Valentino and Dorch, Bertil and Frey, Jeannette and Hallik, Martin and {Hormia-Poutanen}, Kistiina and {Labastida i Juan}, Ignasi and MacColl, John and Ponsati Obiols, Agn{\`e}s and Sacchi, Simone and Scholze, Frank and Schmidt, Birgit and Smit, Anja and Sofronijevic, Adam and Stojanovski, Jadranka and Svoboda, Martin and Tsakonas, Giannis and {van Otegem}, Matthijs and Verheusen, Astrid and Vilks, Andris and Widmark, Wilhelm and Horstmann, Wolfram},
  year = {2018},
  month = jul,
  publisher = {Ligue des biblioth{\`e}ques europ{\'e}ennes de recherche},
  doi = {10.20350/digitalCSIC/15061},
  urldate = {2023-09-13},
  abstract = {Embracing Open Science is critical if we are to make science more collaborative, reproducible, transparent and impactful. Open Science undoubtedly has the power to positively influence society, but its implementation is not yet universal.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2018-07-09T11:07:00Z}
}

@article{bridges_2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  urldate = {2024-10-03},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime{\textregistered}, NBS Presentation{\textregistered}, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\DTLJS2L6\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\IKCSU4QQ\\Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf}
}

@misc{eprime_2020,
  title = {E-{{Prime}}},
  year = {2020},
  howpublished = {Psychology Software Tools}
}

@article{mathot_2012,
  title = {{{OpenSesame}}: {{An}} Open-Source, Graphical Experiment Builder for the Social Sciences},
  shorttitle = {{{OpenSesame}}},
  author = {Math{\^o}t, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
  year = {2012},
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {314--324},
  issn = {1554-351X},
  doi = {10.3758/s13428-011-0168-7},
  urldate = {2024-10-03},
  abstract = {In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
  pmcid = {PMC3356517},
  pmid = {22083660},
  file = {C:\Users\mbch4gs2\Zotero\storage\RLJFCGWK\Math√¥t et al. - 2012 - OpenSesame An open-source, graphical experiment b.pdf}
}

@article{pierce_2019,
  title = {{{PsychoPy2}}: {{Experiments}} in Behavior Made Easy},
  shorttitle = {{{PsychoPy2}}},
  author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {195--203},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01193-y},
  abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
  langid = {english},
  pmcid = {PMC6420413},
  pmid = {30734206},
  keywords = {Behavioral Research,Experiment,Humans,Open science,Open-source,Psychology,Reaction time,Reproducibility of Results,Software,Timing,User-Computer Interface},
  file = {C:\Users\mbch4gs2\Zotero\storage\93ITF788\Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf}
}

@article{strain_2023,
  title = {The {{Effects}} of {{Contrast}} on {{Correlation Perception}} in {{Scatterplots}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = aug,
  journal = {International Journal of Human-Computer Studies},
  volume = {176},
  pages = {103040},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2023.103040},
  urldate = {2023-04-11},
  abstract = {Scatterplots are common data visualizations that can be used to communicate a range of ideas, the most intensively studied being the correlation between two variables. Despite their ubiquity, people typically do not perceive correlations between variables accurately from scatterplots, tending to underestimate the strength of the relationship displayed. Here we describe a two-experiment study in which we adjust the visual contrast of scatterplot points, and demonstrate a systematic approach to altering the bias. We find evidence that lowering the total visual contrast in a plot leads to increased bias in correlation estimates and show that decreasing the salience of points as a function of their distance from the regression line, by lowering their contrast, can facilitate more accurate correlation perception. We discuss the implications of these findings for visualization design, and provide a framework for online, reproducible, and large-sample-size (N = 150 per experiment) testing of the design parameters of data visualizations.},
  langid = {english},
  keywords = {Correlation perception,Crowdsourced,Data visualization,Scatterplot},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TGMNQWN3\\Strain et al. - 2023 - The Effects of Contrast on Correlation Perception .pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\DBRLAN7I\\S1071581923000496.html}
}

@inproceedings{strain_2023b,
  title = {Adjusting {{Point Size}} to {{Facilitate More Accurate Correlation Perception}} in {{Scatterplots}}},
  booktitle = {2023 {{IEEE Vis X Vision}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul and Jay, Caroline},
  year = {2023},
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/VisXVision60716.2023.00006},
  urldate = {2024-02-15},
  abstract = {Viewers consistently underestimate correlation in positively correlated scatterplots. We use a novel data point size manipulation to correct for this bias. In a high-powered and fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot as a function of its distance from the regression line is able to correct for a systematic perceptual bias long present in the literature. We recommend the implementation of our technique when designing scatterplots that aim to communicate positive correlations.},
  isbn = {9798350329841},
  langid = {english},
  keywords = {Correlation,Empirical studies in HCI,Empirical studies in visualization,Human computer interaction,Human-centered computing,Human-centered computing-Human computer interaction (HCI),Systematics,Visualization},
  file = {C\:\\Users\\mbch4gs2\\Zotero\\storage\\TZCCRWMX\\Strain et al. - 2023 - Adjusting Point Size to Facilitate More Accurate C.pdf;C\:\\Users\\mbch4gs2\\Zotero\\storage\\AYEELKVC\\10350485.html}
}

@inproceedings{strain_2024,
  title = {Effects of {{Point Size}} and {{Opacity Adjustments}} in {{Scatterplots}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul A. and Jay, Caroline},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642127},
  urldate = {2024-05-29},
  abstract = {Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive fashion. We show that there is a great deal of scope for using visual features to change viewers' perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.},
  isbn = {9798400703300},
  keywords = {correlation,crowdsourced,perception,scatterplot},
  file = {C:\Users\mbch4gs2\Zotero\storage\IGSEXHSG\Strain et al. - 2024 - Effects of Point Size and Opacity Adjustments in S.pdf}
}
